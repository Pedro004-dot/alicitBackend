#!/usr/bin/env python3
"""
ðŸ”¥ TESTE COMPARATIVO COM DADOS REAIS: llama3.2 vs qwen2.5:7b
Conecta ao Supabase e testa validaÃ§Ã£o de matches com licitaÃ§Ãµes e empresas REAIS
"""

import time
import json
import logging
import requests
import os
import psycopg2
from datetime import datetime
from typing import Dict, List, Any, Optional
from psycopg2.extras import DictCursor

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RealDataOllamaTest:
    def __init__(self):
        self.ollama_base_url = "http://localhost:11434"
        self.models = ["llama3.2:latest", "qwen2.5:7b"]
        
        # ConfiguraÃ§Ã£o do banco de dados (usando as mesmas configuraÃ§Ãµes do projeto)
        self.db_config = {
            'host': 'aws-0-sa-east-1.pooler.supabase.com',
            'port': 5432,
            'database': 'postgres',
            'user': 'postgres.hdlowzlkwrboqfzjewom',
            'password': os.getenv('SUPABASE_DB_PASSWORD', 'Oqj7QlCCJvEL9J3O')  # Use env var se disponÃ­vel
        }
    
    def get_db_connection(self):
        """Conectar ao banco de dados"""
        return psycopg2.connect(**self.db_config)
    
    def fetch_real_licitacoes(self, limit: int = 5) -> List[Dict[str, Any]]:
        """Buscar licitaÃ§Ãµes reais com itens do Supabase"""
        conn = self.get_db_connection()
        try:
            with conn.cursor(cursor_factory=DictCursor) as cursor:
                cursor.execute("""
                    SELECT 
                        l.pncp_id,
                        l.objeto_compra,
                        array_agg(DISTINCT li.descricao) as itens
                    FROM licitacoes l
                    LEFT JOIN licitacao_itens li ON l.id = li.licitacao_id
                    WHERE l.objeto_compra IS NOT NULL 
                        AND l.objeto_compra != ''
                        AND li.descricao IS NOT NULL
                    GROUP BY l.id, l.pncp_id, l.objeto_compra
                    HAVING COUNT(li.id) > 0
                    ORDER BY l.created_at DESC
                    LIMIT %s
                """, (limit,))
                
                licitacoes = []
                for row in cursor.fetchall():
                    # Limpar itens None ou vazios
                    itens_limpos = [item for item in row['itens'] if item and item.strip()]
                    if itens_limpos:  # SÃ³ incluir se tiver itens vÃ¡lidos
                        licitacoes.append({
                            'pncp_id': row['pncp_id'],
                            'objeto_compra': row['objeto_compra'],
                            'itens': itens_limpos[:3]  # Limitar a 3 itens para manter prompt gerenciÃ¡vel
                        })
                
                return licitacoes
        finally:
            conn.close()
    
    def fetch_real_empresas(self, limit: int = 10) -> List[Dict[str, Any]]:
        """Buscar empresas reais com produtos do Supabase"""
        conn = self.get_db_connection()
        try:
            with conn.cursor(cursor_factory=DictCursor) as cursor:
                cursor.execute("""
                    SELECT 
                        nome_fantasia,
                        descricao_servicos_produtos,
                        produtos
                    FROM empresas 
                    WHERE produtos IS NOT NULL 
                        AND produtos != '[]'::jsonb
                        AND descricao_servicos_produtos IS NOT NULL
                        AND descricao_servicos_produtos != ''
                        AND jsonb_array_length(produtos) > 0
                    ORDER BY nome_fantasia
                    LIMIT %s
                """, (limit,))
                
                empresas = []
                for row in cursor.fetchall():
                    # Parse dos produtos JSON
                    produtos = row['produtos'] if isinstance(row['produtos'], list) else []
                    if produtos:  # SÃ³ incluir se tiver produtos
                        empresas.append({
                            'nome': row['nome_fantasia'],
                            'descricao': row['descricao_servicos_produtos'],
                            'produtos': produtos[:5]  # Limitar a 5 produtos para prompt
                        })
                
                return empresas
        finally:
            conn.close()
    
    def build_optimized_prompt_llama32_v2(self, empresa_nome: str, empresa_descricao: str, 
                                          empresa_produtos: List[str], licitacao_objeto: str, 
                                          licitacao_itens: List[str], similarity_score: float) -> str:
        """
        Prompt MELHORADO para llama3.2 - Mais equilibrado, menos rigoroso
        """
        produtos_texto = "\n".join([f"â€¢ {p}" for p in empresa_produtos]) if empresa_produtos else "NÃ£o especificados"
        itens_texto = "\n".join([f"â€¢ {i[:100]}..." if len(i) > 100 else f"â€¢ {i}" for i in licitacao_itens]) if licitacao_itens else "NÃ£o especificados"
        
        return f"""ANÃLISE DE COMPATIBILIDADE EMPRESARIAL

**EMPRESA: {empresa_nome}**
Atividades: {empresa_descricao}
Produtos/ServiÃ§os oferecidos:
{produtos_texto}

**LICITAÃ‡ÃƒO:**
Objetivo: {licitacao_objeto[:200]}...
Itens necessÃ¡rios:
{itens_texto}

**CRITÃ‰RIOS DE AVALIAÃ‡ÃƒO:**
1. A empresa tem produtos/serviÃ§os relacionados aos itens da licitaÃ§Ã£o?
2. A Ã¡rea de atuaÃ§Ã£o Ã© compatÃ­vel com o objetivo da licitaÃ§Ã£o?
3. Ã‰ razoÃ¡vel que esta empresa possa atender esta demanda?

**INSTRUÃ‡Ã•ES:**
- APROVE se houver compatibilidade clara entre os produtos da empresa e os itens da licitaÃ§Ã£o
- APROVE se a empresa atua na mesma Ã¡rea geral (ex: escritÃ³rio vs materiais de escritÃ³rio)
- REJEITE apenas se for claramente incompatÃ­vel (ex: empresa de alimentos vs equipamentos mÃ©dicos)
- Em caso de dÃºvida, seja moderadamente favorÃ¡vel Ã  aprovaÃ§Ã£o

Responda APENAS em JSON vÃ¡lido:
{{"compativel": true/false, "confianca": 0.XX, "justificativa": "explicaÃ§Ã£o breve"}}"""

    def build_balanced_prompt_qwen(self, empresa_nome: str, empresa_descricao: str, 
                                   empresa_produtos: List[str], licitacao_objeto: str, 
                                   licitacao_itens: List[str], similarity_score: float) -> str:
        """
        Prompt BALANCEADO para qwen2.5:7b - MantÃ©m qualidade mas mais rigoroso
        """
        produtos_texto = "\n".join([f"â€¢ {p}" for p in empresa_produtos]) if empresa_produtos else "NÃ£o especificados"
        itens_texto = "\n".join([f"â€¢ {i[:100]}..." if len(i) > 100 else f"â€¢ {i}" for i in licitacao_itens]) if licitacao_itens else "NÃ£o especificados"
        
        return f"""ANÃLISE RIGOROSA DE COMPATIBILIDADE COMERCIAL

### DADOS DA EMPRESA
**Nome:** {empresa_nome}
**DescriÃ§Ã£o:** {empresa_descricao}
**Produtos/ServiÃ§os:**
{produtos_texto}

### DADOS DA LICITAÃ‡ÃƒO
**Objeto:** {licitacao_objeto[:250]}...
**Itens EspecÃ­ficos:**
{itens_texto}

**Score SemÃ¢ntico:** {similarity_score:.1%}

### CRITÃ‰RIOS DE AVALIAÃ‡ÃƒO RIGOROSA:
1. **Compatibilidade Direta**: Os produtos da empresa atendem ESPECIFICAMENTE aos itens licitados?
2. **ExperiÃªncia Setorial**: A empresa demonstra experiÃªncia na Ã¡rea requerida?
3. **Viabilidade TÃ©cnica**: A empresa tem capacidade real de entregar o que Ã© solicitado?
4. **CoerÃªncia Comercial**: Faz sentido comercial esta empresa participar desta licitaÃ§Ã£o?

### DECISÃƒO:
- âœ… APROVAR: Somente se houver correspondÃªncia clara e direta
- ðŸš« REJEITAR: Se houver qualquer dÃºvida significativa sobre a capacidade

Responda APENAS com JSON vÃ¡lido:
{{"compativel": true/false, "confianca": 0.XX, "justificativa": "anÃ¡lise detalhada"}}"""

    def create_realistic_test_scenarios(self) -> List[Dict[str, Any]]:
        """Criar cenÃ¡rios de teste com dados reais do Supabase"""
        print("ðŸ” Buscando dados reais do Supabase...")
        
        try:
            licitacoes = self.fetch_real_licitacoes(5)
            empresas = self.fetch_real_empresas(10)
            
            print(f"ðŸ“‹ {len(licitacoes)} licitaÃ§Ãµes carregadas")
            print(f"ðŸ¢ {len(empresas)} empresas carregadas")
            
            # Criar cenÃ¡rios manuais baseados nos dados reais
            scenarios = []
            
            # CenÃ¡rio 1: Match Ã³bvio - Materiais de escritÃ³rio
            materiais_escritorio = next((e for e in empresas if 'escritorio' in e['nome'].lower()), None)
            licitacao_equipamentos = next((l for l in licitacoes if any(
                'equipamento' in item.lower() or 'material' in item.lower() 
                for item in l['itens']
            )), None)
            
            if materiais_escritorio and licitacao_equipamentos:
                scenarios.append({
                    "nome": "Match Realista - Equipamentos/Materiais",
                    "empresa": materiais_escritorio,
                    "licitacao": licitacao_equipamentos,
                    "expected": True,
                    "justificativa": "Empresa de materiais pode fornecer equipamentos correlatos"
                })
            
            # CenÃ¡rio 2: Match impossÃ­vel - Software vs Agro
            empresa_tech = next((e for e in empresas if any(
                tech in e['descricao'].lower() for tech in ['software', 'tecnologia', 'ia']
            )), None)
            licitacao_agro = next((l for l in licitacoes if any(
                agro in l['objeto_compra'].lower() for agro in ['agr', 'fertilizante', 'campo']
            )), None)
            
            if empresa_tech:
                # Usar qualquer licitaÃ§Ã£o nÃ£o-tech como contraste
                licitacao_contraste = next((l for l in licitacoes if not any(
                    tech in l['objeto_compra'].lower() for tech in ['software', 'sistema', 'tecnologia']
                )), None)
                
                if licitacao_contraste:
                    scenarios.append({
                        "nome": "Match ImpossÃ­vel - Tech vs FÃ­sico",
                        "empresa": empresa_tech,
                        "licitacao": licitacao_contraste,
                        "expected": False,
                        "justificativa": "Empresa de tecnologia nÃ£o fornece produtos fÃ­sicos nÃ£o-tech"
                    })
            
            # CenÃ¡rio 3: Match questionÃ¡vel - AgroFrare
            agrofware = next((e for e in empresas if 'agro' in e['nome'].lower()), None)
            if agrofware:
                licitacao_nao_agro = next((l for l in licitacoes if not any(
                    agro in l['objeto_compra'].lower() for agro in ['agr', 'fertilizante', 'campo', 'rural']
                )), None)
                
                if licitacao_nao_agro:
                    scenarios.append({
                        "nome": "Match QuestionÃ¡vel - Agro vs Outros",
                        "empresa": agrofware,
                        "licitacao": licitacao_nao_agro,
                        "expected": False,
                        "justificativa": "Empresa agro especializada nÃ£o atende demandas gerais"
                    })
            
            # CenÃ¡rio 4: Usar dados diversos
            for i, (empresa, licitacao) in enumerate(zip(empresas[:2], licitacoes[:2])):
                # Tentar determinar se Ã© um match razoÃ¡vel baseado em palavras-chave
                empresa_keywords = set(empresa['descricao'].lower().split() + 
                                     [p.lower() for p in empresa['produtos']])
                licitacao_keywords = set(licitacao['objeto_compra'].lower().split() + 
                                       [item.lower() for item in licitacao['itens']])
                
                # InterseÃ§Ã£o simples para determinar expectativa
                overlap = len(empresa_keywords.intersection(licitacao_keywords))
                expected = overlap > 2  # Se houver mais de 2 palavras em comum
                
                scenarios.append({
                    "nome": f"CenÃ¡rio Real {i+1} - {empresa['nome']} vs LicitaÃ§Ã£o",
                    "empresa": empresa,
                    "licitacao": licitacao,
                    "expected": expected,
                    "justificativa": f"Baseado em anÃ¡lise de palavras-chave (overlap: {overlap})"
                })
            
            return scenarios
            
        except Exception as e:
            logger.error(f"âŒ Erro ao buscar dados do Supabase: {e}")
            return []

    def call_ollama_model(self, model: str, prompt: str, timeout: int = 90) -> Dict[str, Any]:
        """Chamar modelo especÃ­fico no Ollama com timeout aumentado"""
        try:
            start_time = time.time()
            
            response = requests.post(
                f"{self.ollama_base_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.2,  # Ligeiramente mais criativo
                        "top_p": 0.9,
                        "num_predict": 300   # Respostas mais curtas
                    }
                },
                timeout=timeout
            )
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            if response.status_code == 200:
                result = response.json()
                raw_response = result.get("response", "").strip()
                
                # Parse mais robusto do JSON
                try:
                    # MÃºltiplas tentativas de extrair JSON
                    json_attempts = [
                        raw_response,  # Resposta completa
                        raw_response[raw_response.find('{'):raw_response.rfind('}')+1],  # Entre chaves
                        raw_response[raw_response.find('{"'):raw_response.rfind('"}')+2]  # JSON quoted
                    ]
                    
                    parsed_result = None
                    for attempt in json_attempts:
                        try:
                            if attempt and '{' in attempt and '}' in attempt:
                                parsed_result = json.loads(attempt)
                                break
                        except:
                            continue
                    
                    if parsed_result:
                        return {
                            "success": True,
                            "processing_time": processing_time,
                            "raw_response": raw_response,
                            "parsed_result": parsed_result,
                            "compativel": parsed_result.get("compativel", False),
                            "confianca": float(parsed_result.get("confianca", 0.0)),
                            "justificativa": parsed_result.get("justificativa", "")
                        }
                    else:
                        raise ValueError("NÃ£o foi possÃ­vel extrair JSON vÃ¡lido")
                        
                except Exception as e:
                    return {
                        "success": False,
                        "processing_time": processing_time,
                        "error": f"Erro ao parsear JSON: {e}",
                        "raw_response": raw_response
                    }
            else:
                return {
                    "success": False,
                    "processing_time": processing_time,
                    "error": f"HTTP {response.status_code}: {response.text}"
                }
                
        except Exception as e:
            return {
                "success": False,
                "processing_time": 0,
                "error": str(e)
            }

    def run_real_data_test(self):
        """Executar teste com dados reais do Supabase"""
        
        print("ðŸ”¥ INICIANDO TESTE COM DADOS REAIS DO SUPABASE")
        print("=" * 70)
        
        # Verificar conectividade
        try:
            conn = self.get_db_connection()
            conn.close()
            print("âœ… ConexÃ£o com Supabase estabelecida")
        except Exception as e:
            logger.error(f"âŒ Erro de conexÃ£o com Supabase: {e}")
            return
        
        # Verificar Ollama
        try:
            response = requests.get(f"{self.ollama_base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                available_models = [model['name'] for model in response.json().get('models', [])]
                print(f"ðŸ¦™ Ollama ativo. Modelos: {available_models}")
                
                for model in self.models:
                    if model not in available_models:
                        logger.error(f"âŒ Modelo {model} nÃ£o disponÃ­vel!")
                        return
            else:
                logger.error("âŒ Ollama nÃ£o estÃ¡ respondendo")
                return
        except Exception as e:
            logger.error(f"âŒ Erro ao conectar com Ollama: {e}")
            return
        
        # Criar cenÃ¡rios de teste
        test_scenarios = self.create_realistic_test_scenarios()
        if not test_scenarios:
            logger.error("âŒ NÃ£o foi possÃ­vel criar cenÃ¡rios de teste")
            return
        
        print(f"ðŸ“‹ {len(test_scenarios)} cenÃ¡rios de teste criados")
        
        results = {}
        
        for model in self.models:
            print(f"\nðŸ¤– TESTANDO MODELO: {model}")
            print("-" * 50)
            
            results[model] = {
                "total_time": 0,
                "successful_tests": 0,
                "failed_tests": 0,
                "correct_decisions": 0,
                "test_details": []
            }
            
            for i, scenario in enumerate(test_scenarios, 1):
                print(f"\n[{i}/{len(test_scenarios)}] {scenario['nome']}")
                print(f"   ðŸ¢ Empresa: {scenario['empresa']['nome']}")
                print(f"   ðŸ“‹ LicitaÃ§Ã£o: {scenario['licitacao']['pncp_id']}")
                
                # Escolher prompt baseado no modelo
                if model == "llama3.2:latest":
                    prompt = self.build_optimized_prompt_llama32_v2(
                        scenario['empresa']['nome'],
                        scenario['empresa']['descricao'],
                        scenario['empresa']['produtos'],
                        scenario['licitacao']['objeto_compra'],
                        scenario['licitacao']['itens'],
                        0.75
                    )
                else:
                    prompt = self.build_balanced_prompt_qwen(
                        scenario['empresa']['nome'],
                        scenario['empresa']['descricao'],
                        scenario['empresa']['produtos'],
                        scenario['licitacao']['objeto_compra'],
                        scenario['licitacao']['itens'],
                        0.75
                    )
                
                result = self.call_ollama_model(model, prompt)
                
                if result["success"]:
                    decision = result["compativel"]
                    confidence = result["confianca"]
                    processing_time = result["processing_time"]
                    justificativa = result["justificativa"]
                    
                    is_correct = decision == scenario["expected"]
                    
                    print(f"   â±ï¸  Tempo: {processing_time:.1f}s")
                    print(f"   ðŸŽ¯ DecisÃ£o: {'âœ… APROVOU' if decision else 'ðŸš« REJEITOU'}")
                    print(f"   ðŸ“Š ConfianÃ§a: {confidence:.0%}")
                    print(f"   âœ… Correto: {'SIM' if is_correct else 'NÃƒO'}")
                    print(f"   ðŸ’­ Justificativa: {justificativa[:100]}...")
                    
                    results[model]["total_time"] += processing_time
                    results[model]["successful_tests"] += 1
                    if is_correct:
                        results[model]["correct_decisions"] += 1
                    
                    results[model]["test_details"].append({
                        "scenario_name": scenario["nome"],
                        "empresa_nome": scenario['empresa']['nome'],
                        "licitacao_id": scenario['licitacao']['pncp_id'],
                        "expected": scenario["expected"],
                        "decision": decision,
                        "confidence": confidence,
                        "processing_time": processing_time,
                        "is_correct": is_correct,
                        "justificativa": justificativa,
                        "scenario_justificativa": scenario["justificativa"]
                    })
                else:
                    print(f"   âŒ ERRO: {result['error']}")
                    results[model]["failed_tests"] += 1
        
        # AnÃ¡lise final
        self.print_real_data_results(results)
        self.save_real_data_results(results)
        
        return results

    def print_real_data_results(self, results: Dict):
        """Imprimir anÃ¡lise dos resultados com dados reais"""
        print("\n" + "=" * 70)
        print("ðŸ“Š ANÃLISE COMPARATIVA - DADOS REAIS DO SUPABASE")
        print("=" * 70)
        
        for model in self.models:
            data = results[model]
            if data["successful_tests"] > 0:
                avg_time = data["total_time"] / data["successful_tests"]
                accuracy = (data["correct_decisions"] / data["successful_tests"]) * 100
                
                print(f"\nðŸ¤– {model.upper()}:")
                print(f"   â±ï¸  Tempo mÃ©dio: {avg_time:.1f}s")
                print(f"   ðŸŽ¯ AcurÃ¡cia: {accuracy:.1f}% ({data['correct_decisions']}/{data['successful_tests']})")
                print(f"   âœ… Sucessos: {data['successful_tests']}")
                print(f"   âŒ Falhas: {data['failed_tests']}")
        
        # ComparaÃ§Ã£o e recomendaÃ§Ã£o
        if len(self.models) == 2 and all(results[m]["successful_tests"] > 0 for m in self.models):
            model1, model2 = self.models
            data1, data2 = results[model1], results[model2]
            
            avg_time1 = data1["total_time"] / data1["successful_tests"]
            avg_time2 = data2["total_time"] / data2["successful_tests"]
            
            accuracy1 = (data1["correct_decisions"] / data1["successful_tests"]) * 100
            accuracy2 = (data2["correct_decisions"] / data2["successful_tests"]) * 100
            
            print(f"\nðŸ† COMPARAÃ‡ÃƒO FINAL:")
            print(f"   âš¡ Velocidade: {model1} {avg_time1:.1f}s vs {model2} {avg_time2:.1f}s")
            print(f"   ðŸŽ¯ PrecisÃ£o: {model1} {accuracy1:.1f}% vs {model2} {accuracy2:.1f}%")
            
            # Calcular "score" combinado (velocidade + precisÃ£o)
            speed_score1 = 100 / avg_time1 if avg_time1 > 0 else 0
            speed_score2 = 100 / avg_time2 if avg_time2 > 0 else 0
            
            combined_score1 = (accuracy1 * 0.7) + (speed_score1 * 0.3)  # 70% precisÃ£o, 30% velocidade
            combined_score2 = (accuracy2 * 0.7) + (speed_score2 * 0.3)
            
            print(f"\nðŸ’¡ RECOMENDAÃ‡ÃƒO:")
            if combined_score1 > combined_score2:
                print(f"   ðŸ† Use {model1} - Melhor equilÃ­brio geral")
                if accuracy1 > accuracy2:
                    print(f"   âœ… Mais preciso E mais rÃ¡pido")
                else:
                    print(f"   âš¡ Velocidade compensa a menor precisÃ£o")
            else:
                print(f"   ðŸ† Use {model2} - Melhor equilÃ­brio geral")
                if accuracy2 > accuracy1:
                    print(f"   âœ… Maior precisÃ£o justifica tempo extra")
                else:
                    print(f"   âš¡ Velocidade superior")

    def save_real_data_results(self, results: Dict):
        """Salvar resultados detalhados"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"real_data_test_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nðŸ’¾ Resultados salvos em: {filename}")

def main():
    """FunÃ§Ã£o principal"""
    tester = RealDataOllamaTest()
    tester.run_real_data_test()

if __name__ == "__main__":
    main() 