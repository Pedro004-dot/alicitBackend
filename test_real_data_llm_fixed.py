#!/usr/bin/env python3
"""
üî¨ TESTE EXPANDIDO DE VALIDA√á√ÉO LLM COM DADOS REAIS DO SUPABASE
=================================================================

Este script testa os validadores LLM (OLLAMA local) com dados reais do banco Supabase.
Agora com prompts otimizados e cen√°rios de teste mais abrangentes.

Prompts Otimizados:
- QWEN2.5: Foco na velocidade mantendo qualidade
- LLAMA3.2: Mais reflex√£o e crit√©rio rigoroso

Funcionalidades:
‚úÖ Busca licita√ß√µes e empresas reais do Supabase  
‚úÖ Testa validadores OLLAMA (qwen2.5:7b e llama3.2)
‚úÖ An√°lise de produtos vs itens detalhada
‚úÖ Prompts otimizados para cada modelo
‚úÖ Cen√°rios de teste variados (muito espec√≠ficos, gen√©ricos, incompat√≠veis)
‚úÖ M√©tricas de performance (tempo + acur√°cia)
"""

import os
import sys
import json
import time
import logging
import asyncio
import psycopg2
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# üîß CONFIGURA√á√ÉO DO AMBIENTE
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
load_dotenv('config.env')

# ‚ö†Ô∏è  IMPORT DIRETO PARA EVITAR PROBLEMAS DE DEPEND√äNCIA
import aiohttp
import json as json_lib
from dataclasses import dataclass

@dataclass
class ValidationResult:
    """Resultado da valida√ß√£o LLM"""
    approved: bool
    reasoning: str
    confidence: float = 0.0

class SimpleOllamaValidator:
    """üî¨ Validador OLLAMA simplificado para testes"""
    
    def __init__(self, model_name: str = "qwen2.5:7b"):
        self.model_name = model_name
        self.base_url = "http://localhost:11434"
    
    def _build_optimized_prompt(self, empresa_nome: str, empresa_descricao: str, 
                              licitacao_objeto: str, similarity_score: float,
                              licitacao_itens: list = None, empresa_produtos: list = None) -> str:
        """üöÄ Prompt otimizado espec√≠fico para cada modelo"""
        
        # Valida√ß√£o robusta dos par√¢metros
        if licitacao_itens is None:
            licitacao_itens = []
        if empresa_produtos is None:
            empresa_produtos = []
        
        # Converter para string se necess√°rio
        if isinstance(licitacao_itens, str):
            try:
                licitacao_itens = json_lib.loads(licitacao_itens)
            except:
                licitacao_itens = []
        
        if isinstance(empresa_produtos, str):
            try:
                empresa_produtos = json_lib.loads(empresa_produtos)
            except:
                empresa_produtos = []
        
        # üéØ PROMPT ESPEC√çFICO POR MODELO
        if "qwen" in self.model_name.lower():
            return self._qwen_prompt(empresa_nome, empresa_descricao, licitacao_objeto, 
                                   similarity_score, licitacao_itens, empresa_produtos)
        elif "llama" in self.model_name.lower():
            return self._llama_prompt(empresa_nome, empresa_descricao, licitacao_objeto,
                                    similarity_score, licitacao_itens, empresa_produtos)
        else:
            return self._default_prompt(empresa_nome, empresa_descricao, licitacao_objeto,
                                      similarity_score, licitacao_itens, empresa_produtos)
    
    def _qwen_prompt(self, empresa_nome: str, empresa_descricao: str, licitacao_objeto: str,
                     similarity_score: float, licitacao_itens: list, empresa_produtos: list) -> str:
        """‚ö° PROMPT OTIMIZADO PARA QWEN2.5 - FOCO NA VELOCIDADE"""
        
        itens_str = ""
        if licitacao_itens:
            itens_resumo = []
            for item in licitacao_itens[:3]:  # M√°ximo 3 itens para velocidade
                if isinstance(item, dict):
                    desc = item.get('descricao', '')
                    itens_resumo.append(f"‚Ä¢ {desc}")
            if itens_resumo:
                itens_str = f"\nItens principais:\n" + "\n".join(itens_resumo)
        
        produtos_str = ""
        if empresa_produtos:
            produtos_resumo = empresa_produtos[:5]  # M√°ximo 5 produtos
            produtos_str = f"\nProdutos/servi√ßos: {', '.join(produtos_resumo)}"
        
        return f"""üéØ AN√ÅLISE R√ÅPIDA DE COMPATIBILIDADE

EMPRESA: {empresa_nome}
Descri√ß√£o: {empresa_descricao}{produtos_str}

LICITA√á√ÉO: {licitacao_objeto}{itens_str}

Score de similaridade: {similarity_score:.2f}

TAREFA: Avaliar se a empresa pode fornecer o que a licita√ß√£o solicita.

CRIT√âRIOS DIRETOS:
‚úÖ APROVAR se: produtos/servi√ßos da empresa atendem diretamente aos itens
‚ùå REJEITAR se: incompatibilidade clara ou falta de especificidade

RESPOSTA (JSON):
{{"approved": true/false, "reasoning": "explica√ß√£o objetiva em 1-2 frases"}}"""
    
    def _llama_prompt(self, empresa_nome: str, empresa_descricao: str, licitacao_objeto: str,
                      similarity_score: float, licitacao_itens: list, empresa_produtos: list) -> str:
        """üß† PROMPT OTIMIZADO PARA LLAMA3.2 - FOCO NA REFLEX√ÉO E CRIT√âRIO"""
        
        itens_str = ""
        if licitacao_itens:
            itens_detalhados = []
            for item in licitacao_itens:
                if isinstance(item, dict):
                    desc = item.get('descricao', '')
                    qtd = item.get('quantidade', '')
                    unidade = item.get('unidade_medida', '')
                    itens_detalhados.append(f"‚Ä¢ {desc} ({qtd} {unidade})")
            if itens_detalhados:
                itens_str = f"\n\nItens detalhados da licita√ß√£o:\n" + "\n".join(itens_detalhados)
        
        produtos_str = ""
        if empresa_produtos:
            produtos_str = f"\n\nProdutos/servi√ßos oferecidos pela empresa:\n‚Ä¢ " + "\n‚Ä¢ ".join(empresa_produtos)
        
        return f"""üîç AN√ÅLISE CRITERIOSA DE COMPATIBILIDADE PARA LICITA√á√ÉO

DADOS DA EMPRESA:
Nome: {empresa_nome}
Descri√ß√£o: {empresa_descricao}{produtos_str}

DADOS DA LICITA√á√ÉO:
Objeto: {licitacao_objeto}{itens_str}

Score de similaridade inicial: {similarity_score:.2f}

üß† PROCESSO DE AN√ÅLISE REFLEXIVA:

1Ô∏è‚É£ PRIMEIRO: Analise se os produtos/servi√ßos da empresa t√™m RELA√á√ÉO DIRETA com os itens solicitados.

2Ô∏è‚É£ SEGUNDO: Considere se a empresa tem CAPACIDADE T√âCNICA para fornecer especificamente o que √© pedido.

3Ô∏è‚É£ TERCEIRO: Avalie se existe COMPATIBILIDADE SETORIAL real (n√£o apenas palavras gen√©ricas).

4Ô∏è‚É£ QUARTO: QUESTIONE-SE: "Essa empresa realmente pode cumprir este contrato espec√≠fico?"

5Ô∏è‚É£ DECIS√ÉO FINAL: 
- APROVADO apenas se houver compatibilidade clara e espec√≠fica
- REJEITADO se houver d√∫vidas ou incompatibilidade

üö® ATEN√á√ÉO: Seja RIGOROSO. Evite aprova√ß√µes por similaridade superficial.

RESPOSTA OBRIGAT√ìRIA (JSON):
{{"approved": true/false, "reasoning": "explica√ß√£o detalhada do racioc√≠nio em 2-3 frases"}}"""
    
    def _default_prompt(self, empresa_nome: str, empresa_descricao: str, licitacao_objeto: str,
                       similarity_score: float, licitacao_itens: list, empresa_produtos: list) -> str:
        """üìù Prompt padr√£o para outros modelos"""
        return f"""Analise a compatibilidade entre a empresa e a licita√ß√£o.

EMPRESA: {empresa_nome}
DESCRI√á√ÉO: {empresa_descricao}
PRODUTOS: {empresa_produtos}

LICITA√á√ÉO: {licitacao_objeto}
ITENS: {licitacao_itens}

Responda apenas em JSON: {{"approved": true/false, "reasoning": "explica√ß√£o"}}"""
    
    async def validate_match(self, empresa_nome: str, empresa_descricao: str,
                           licitacao_objeto: str, similarity_score: float,
                           licitacao_itens: list = None, empresa_produtos: list = None) -> ValidationResult:
        """üîç Validar match usando OLLAMA"""
        
        prompt = self._build_optimized_prompt(
            empresa_nome, empresa_descricao, licitacao_objeto,
            similarity_score, licitacao_itens, empresa_produtos
        )
        
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "top_p": 0.9,
                        "num_predict": 200
                    }
                }
                
                async with session.post(
                    f"{self.base_url}/api/generate",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    
                    if response.status != 200:
                        return ValidationResult(False, f"Erro HTTP {response.status}", 0.0)
                    
                    result = await response.json()
                    response_text = result.get('response', '').strip()
                    
                    # Tentar extrair JSON da resposta
                    try:
                        # Buscar JSON na resposta
                        start_idx = response_text.find('{')
                        end_idx = response_text.rfind('}') + 1
                        
                        if start_idx >= 0 and end_idx > start_idx:
                            json_str = response_text[start_idx:end_idx]
                            parsed = json_lib.loads(json_str)
                            
                            approved = parsed.get('approved', False)
                            reasoning = parsed.get('reasoning', 'Sem explica√ß√£o fornecida')
                            
                            return ValidationResult(approved, reasoning, 0.8)
                        else:
                            # Fallback: tentar interpretar texto
                            approved = any(word in response_text.lower() for word in ['true', 'aprovado', 'sim', 'yes'])
                            return ValidationResult(approved, response_text[:200], 0.5)
                            
                    except json_lib.JSONDecodeError:
                        # Fallback para an√°lise de texto
                        approved = any(word in response_text.lower() for word in ['true', 'aprovado', 'sim'])
                        return ValidationResult(approved, response_text[:200], 0.3)
                        
        except Exception as e:
            return ValidationResult(False, f"Erro na valida√ß√£o: {str(e)}", 0.0)

class EnhancedRealDataLLMTester:
    """üî¨ Testador avan√ßado com dados reais do Supabase"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        self.validators = {}
        
        # üéØ MODELOS A TESTAR
        self.models_to_test = [
            'qwen2.5:7b',  # Mais r√°pido com prompts otimizados
            'llama3.2'     # Mais criterioso com reflex√£o
        ]
        
        self.results = {
            'test_start_time': time.time(),
            'models_tested': {},
            'summary': {}
        }
        
    def _setup_logging(self) -> logging.Logger:
        """Configurar logging detalhado"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('test_real_data_llm.log')
            ]
        )
        return logging.getLogger(__name__)

    def get_db_connection(self):
        """üîó Conectar ao Supabase usando DATABASE_URL"""
        try:
            database_url = os.getenv('DATABASE_URL')
            if not database_url:
                raise ValueError("DATABASE_URL n√£o encontrada no ambiente")
            
            self.logger.info(f"üîó Conectando ao banco: {database_url[:30]}...")
            conn = psycopg2.connect(database_url)
            self.logger.info("‚úÖ Conex√£o estabelecida com sucesso")
            return conn
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro na conex√£o: {e}")
            raise
    
    def fetch_real_licitacoes(self, limit: int = 10) -> List[Dict[str, Any]]:
        """üèõÔ∏è Buscar licita√ß√µes reais com mais variedade"""
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            # üî• BUSCA SIMPLIFICADA - Apenas dados essenciais
            query = """
                SELECT 
                    pncp_id,
                    objeto_compra
                FROM licitacoes 
                WHERE objeto_compra IS NOT NULL 
                    AND LENGTH(objeto_compra) > 50
                    AND (
                        objeto_compra ILIKE '%equipamento%' OR
                        objeto_compra ILIKE '%software%' OR
                        objeto_compra ILIKE '%medicamento%' OR
                        objeto_compra ILIKE '%combust√≠vel%' OR
                        objeto_compra ILIKE '%limpeza%' OR
                        objeto_compra ILIKE '%material%' OR
                        objeto_compra ILIKE '%servi√ßo%' OR
                        objeto_compra ILIKE '%√°gua%' OR
                        objeto_compra ILIKE '%escrit√≥rio%' OR
                        objeto_compra ILIKE '%ve√≠culo%'
                    )
                ORDER BY RANDOM()
                LIMIT %s;
            """
            
            cursor.execute(query, (limit,))
            results = cursor.fetchall()
            
            licitacoes = []
            for row in results:
                licitacao_base = {
                    'pncp_id': row[0],
                    'objeto_compra': row[1],
                    'situacao_compra_nome': 'Em andamento',  # Valor padr√£o
                    'itens': []
                }
                
                # üîç BUSCAR ITENS SEPARADAMENTE PARA ESTA LICITA√á√ÉO
                itens_query = """
                    SELECT 
                        li.numero_item,
                        li.descricao,
                        li.quantidade,
                        li.valor_unitario_estimado,
                        li.unidade_medida
                    FROM licitacao_itens li
                    INNER JOIN licitacoes l ON l.id = li.licitacao_id
                    WHERE l.pncp_id = %s
                    ORDER BY li.numero_item
                    LIMIT 5;
                """
                
                cursor.execute(itens_query, (row[0],))
                itens_results = cursor.fetchall()
                
                itens = []
                for item_row in itens_results:
                    item = {
                        'numero_item': item_row[0],
                        'descricao': item_row[1],
                        'quantidade': item_row[2],
                        'valor_unitario_estimado': item_row[3],
                        'unidade_medida': item_row[4]
                    }
                    itens.append(item)
                
                licitacao_base['itens'] = itens
                licitacoes.append(licitacao_base)
            
            cursor.close()
            conn.close()
            
            self.logger.info(f"üìã Buscadas {len(licitacoes)} licita√ß√µes do Supabase")
            return licitacoes
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao buscar licita√ß√µes: {e}")
            return []

    def fetch_real_empresas(self, limit: int = 8) -> List[Dict[str, Any]]:
        """üè¢ Buscar empresas reais com mais diversidade"""
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            # üî• BUSCA EXPANDIDA - Empresas de setores bem diversos
            query = """
                SELECT 
                    nome_fantasia,
                    descricao_servicos_produtos,
                    produtos
                FROM empresas 
                WHERE descricao_servicos_produtos IS NOT NULL 
                    AND LENGTH(descricao_servicos_produtos) > 20
                    AND produtos IS NOT NULL 
                    AND jsonb_array_length(produtos) > 0
                ORDER BY RANDOM()
                LIMIT %s;
            """
            
            cursor.execute(query, (limit,))
            results = cursor.fetchall()
            
            empresas = []
            for row in results:
                empresa = {
                    'nome_fantasia': row[0],
                    'descricao_servicos_produtos': row[1],
                    'produtos': row[2] if row[2] else []
                }
                empresas.append(empresa)
            
            cursor.close()
            conn.close()
            
            self.logger.info(f"üè¢ Buscadas {len(empresas)} empresas do Supabase")
            return empresas
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao buscar empresas: {e}")
            return []

    def create_test_scenarios(self, licitacoes: List[Dict], empresas: List[Dict]) -> List[Dict]:
        """üéØ Criar cen√°rios de teste mais inteligentes e variados"""
        scenarios = []
        scenario_id = 1
        
        # üìä AN√ÅLISE INTELIGENTE PARA CRIAR CEN√ÅRIOS DIVERSOS
        self.logger.info("üß† Analisando dados para criar cen√°rios de teste...")
        
        for licitacao in licitacoes:
            for empresa in empresas:
                
                # üéØ CATEGORIZA√á√ÉO INTELIGENTE
                objeto_lower = licitacao['objeto_compra'].lower()
                empresa_desc_lower = empresa['descricao_servicos_produtos'].lower()
                produtos_texto = ' '.join(empresa['produtos']).lower() if empresa['produtos'] else ''
                
                # üîç DETEC√á√ÉO DE COMPATIBILIDADE
                match_type = self._detect_match_type(objeto_lower, empresa_desc_lower, produtos_texto)
                expected_result = self._determine_expected_result(match_type, objeto_lower, empresa_desc_lower, produtos_texto)
                
                scenario = {
                    'id': scenario_id,
                    'match_type': match_type,
                    'expected_result': expected_result,
                    'confidence': self._calculate_confidence(match_type),
                    'empresa_nome': empresa['nome_fantasia'],
                    'empresa_descricao': empresa['descricao_servicos_produtos'],
                    'empresa_produtos': empresa['produtos'],
                    'licitacao_objeto': licitacao['objeto_compra'],
                    'licitacao_itens': licitacao['itens'],
                    'similarity_score': 0.85,  # Score alto para for√ßar valida√ß√£o LLM
                    'description': self._generate_scenario_description(match_type, empresa['nome_fantasia'], licitacao['objeto_compra'])
                }
                
                scenarios.append(scenario)
                scenario_id += 1
        
        self.logger.info(f"üéØ Criados {len(scenarios)} cen√°rios de teste")
        return scenarios

    def _detect_match_type(self, objeto: str, empresa_desc: str, produtos: str) -> str:
        """üîç Detectar tipo de compatibilidade"""
        
        # üéØ ALTA COMPATIBILIDADE
        high_match_keywords = {
            'medicamento': ['medicamento', 'farmac', 'sa√∫de', 'hospital'],
            'software': ['software', 'sistema', 'tecnologia', 'inform√°tica', 'ti'],
            'equipamento': ['equipamento', 'm√°quina', 'ferramenta', 'aparelho'],
            'combust√≠vel': ['combust√≠vel', 'gasolina', 'diesel', 'posto'],
            'limpeza': ['limpeza', 'higiene', 'detergente', 'produto de limpeza'],
            'escrit√≥rio': ['escrit√≥rio', 'papelaria', 'material escolar'],
            've√≠culo': ['ve√≠culo', 'carro', 'caminh√£o', 'transporte'],
            '√°gua': ['√°gua', 'mineral', 'bebida']
        }
        
        for categoria, keywords in high_match_keywords.items():
            if any(keyword in objeto for keyword in keywords):
                if any(keyword in empresa_desc or keyword in produtos for keyword in keywords):
                    return 'highly_compatible'
        
        # üéØ COMPATIBILIDADE GEN√âRICA
        generic_keywords = ['material', 'produto', 'servi√ßo', 'fornecimento', 'aquisi√ß√£o']
        if any(keyword in objeto for keyword in generic_keywords):
            return 'generic_compatible'
        
        # üéØ INCOMPAT√çVEL
        return 'incompatible'

    def _determine_expected_result(self, match_type: str, objeto: str, empresa_desc: str, produtos: str) -> str:
        """üéØ Determinar resultado esperado"""
        if match_type == 'highly_compatible':
            return 'APROVADO'
        elif match_type == 'generic_compatible':
            # An√°lise mais detalhada para gen√©ricos
            if any(word in empresa_desc for word in ['diversos', 'geral', 'variados']):
                return 'APROVADO'
            return 'REJEITADO'
        else:
            return 'REJEITADO'

    def _calculate_confidence(self, match_type: str) -> float:
        """üìä Calcular confian√ßa na predi√ß√£o"""
        confidence_map = {
            'highly_compatible': 0.9,
            'generic_compatible': 0.6,
            'incompatible': 0.95
        }
        return confidence_map.get(match_type, 0.5)

    def _generate_scenario_description(self, match_type: str, empresa: str, licitacao: str) -> str:
        """üìù Gerar descri√ß√£o do cen√°rio"""
        descriptions = {
            'highly_compatible': f"‚úÖ MATCH ESPERADO: {empresa} vs {licitacao[:50]}...",
            'generic_compatible': f"‚ùì GEN√âRICO: {empresa} vs {licitacao[:50]}...",
            'incompatible': f"‚ùå INCOMPAT√çVEL: {empresa} vs {licitacao[:50]}..."
        }
        return descriptions.get(match_type, f"Teste: {empresa} vs {licitacao[:50]}...")

    async def test_model_performance(self, model_name: str, scenarios: List[Dict]) -> Dict:
        """üöÄ Testar performance de um modelo espec√≠fico"""
        self.logger.info(f"\nüöÄ TESTANDO MODELO: {model_name}")
        self.logger.info("=" * 60)
        
        try:
            # üèóÔ∏è INICIALIZAR VALIDADOR SIMPLIFICADO
            validator = SimpleOllamaValidator(model_name)
            self.logger.info(f"üîß Modelo configurado: {model_name}")
            
            results = {
                'model_name': model_name,
                'total_tests': len(scenarios),
                'correct_predictions': 0,
                'incorrect_predictions': 0,
                'total_time': 0.0,
                'average_time': 0.0,
                'test_details': [],
                'performance_summary': {}
            }
            
            # üéØ EXECUTAR TESTES
            for i, scenario in enumerate(scenarios):
                self.logger.info(f"\nüìù TESTE {i+1}/{len(scenarios)}: {scenario['description']}")
                
                start_time = time.time()
                
                try:
                    # üîç EXECUTAR VALIDA√á√ÉO
                    resultado = await validator.validate_match(
                        empresa_nome=scenario['empresa_nome'],
                        empresa_descricao=scenario['empresa_descricao'],
                        licitacao_objeto=scenario['licitacao_objeto'],
                        similarity_score=scenario['similarity_score'],
                        licitacao_itens=scenario['licitacao_itens'],
                        empresa_produtos=scenario['empresa_produtos']
                    )
                    
                    end_time = time.time()
                    test_time = end_time - start_time
                    
                    # üìä ANALISAR RESULTADO
                    is_correct = resultado.approved == (scenario['expected_result'] == 'APROVADO')
                    
                    if is_correct:
                        results['correct_predictions'] += 1
                        status = "‚úÖ CORRETO"
                    else:
                        results['incorrect_predictions'] += 1
                        status = "‚ùå INCORRETO"
                    
                    # üìã REGISTRAR DETALHES
                    test_detail = {
                        'scenario_id': scenario['id'],
                        'expected': scenario['expected_result'],
                        'actual': 'APROVADO' if resultado.approved else 'REJEITADO',
                        'correct': is_correct,
                        'time_seconds': round(test_time, 2),
                        'confidence': scenario['confidence'],
                        'reasoning': resultado.reasoning[:200] + "..." if len(resultado.reasoning) > 200 else resultado.reasoning
                    }
                    
                    results['test_details'].append(test_detail)
                    results['total_time'] += test_time
                    
                    # üìä LOG DO RESULTADO
                    self.logger.info(f"   {status} | Tempo: {test_time:.2f}s | Esperado: {scenario['expected_result']} | Obtido: {'APROVADO' if resultado.approved else 'REJEITADO'}")
                    self.logger.info(f"   üí≠ Racioc√≠nio: {resultado.reasoning[:100]}...")
                    
                except Exception as e:
                    self.logger.error(f"   ‚ùå Erro no teste: {e}")
                    test_detail = {
                        'scenario_id': scenario['id'],
                        'expected': scenario['expected_result'],
                        'actual': 'ERRO',
                        'correct': False,
                        'time_seconds': 0,
                        'error': str(e)
                    }
                    results['test_details'].append(test_detail)
                    results['incorrect_predictions'] += 1
            
            # üìä CALCULAR M√âTRICAS FINAIS
            results['average_time'] = results['total_time'] / len(scenarios) if scenarios else 0
            results['accuracy'] = results['correct_predictions'] / len(scenarios) if scenarios else 0
            
            # üìà RESUMO DE PERFORMANCE
            results['performance_summary'] = {
                'accuracy_percentage': round(results['accuracy'] * 100, 1),
                'total_time_minutes': round(results['total_time'] / 60, 2),
                'avg_time_per_test': round(results['average_time'], 2),
                'tests_per_minute': round(60 / results['average_time'], 1) if results['average_time'] > 0 else 0
            }
            
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro geral no teste do modelo {model_name}: {e}")
            return {'error': str(e)}

    def print_comprehensive_report(self, all_results: Dict):
        """üìä Relat√≥rio detalhado de todos os testes"""
        print("\n" + "=" * 80)
        print("üéØ RELAT√ìRIO DETALHADO DE TESTES LLM COM DADOS REAIS")
        print("=" * 80)
        
        print(f"\n‚è∞ Tempo total dos testes: {time.time() - self.results['test_start_time']:.1f}s")
        print(f"üìã Total de cen√°rios testados: {len(all_results.get(list(all_results.keys())[0], {}).get('test_details', []))}")
        
        # üìä COMPARA√á√ÉO ENTRE MODELOS
        print(f"\n{'='*50}")
        print("üìä COMPARA√á√ÉO DE PERFORMANCE ENTRE MODELOS")
        print(f"{'='*50}")
        
        comparison_data = []
        for model_name, results in all_results.items():
            if 'performance_summary' in results:
                summary = results['performance_summary']
                comparison_data.append({
                    'model': model_name,
                    'accuracy': summary['accuracy_percentage'],
                    'avg_time': summary['avg_time_per_test'],
                    'tests_per_min': summary['tests_per_minute']
                })
        
        # üèÜ RANKING POR ACUR√ÅCIA
        comparison_data.sort(key=lambda x: x['accuracy'], reverse=True)
        
        print(f"\nüèÜ RANKING POR ACUR√ÅCIA:")
        print(f"{'Modelo':<15} {'Acur√°cia':<10} {'Tempo/Teste':<12} {'Testes/Min':<12}")
        print("-" * 55)
        
        for i, data in enumerate(comparison_data):
            medal = "ü•á" if i == 0 else "ü•à" if i == 1 else "ü•â" if i == 2 else "  "
            print(f"{medal} {data['model']:<13} {data['accuracy']:<9.1f}% {data['avg_time']:<11.2f}s {data['tests_per_min']:<11.1f}")
        
        # ‚ö° RANKING POR VELOCIDADE
        comparison_data.sort(key=lambda x: x['avg_time'])
        
        print(f"\n‚ö° RANKING POR VELOCIDADE:")
        print(f"{'Modelo':<15} {'Tempo/Teste':<12} {'Testes/Min':<12} {'Acur√°cia':<10}")
        print("-" * 55)
        
        for i, data in enumerate(comparison_data):
            medal = "ü•á" if i == 0 else "ü•à" if i == 1 else "ü•â" if i == 2 else "  "
            print(f"{medal} {data['model']:<13} {data['avg_time']:<11.2f}s {data['tests_per_min']:<11.1f} {data['accuracy']:<9.1f}%")
        
        # üìà DETALHES POR MODELO
        for model_name, results in all_results.items():
            if 'error' in results:
                print(f"\n‚ùå ERRO NO MODELO {model_name}: {results['error']}")
                continue
                
            summary = results['performance_summary']
            
            print(f"\n{'='*60}")
            print(f"üìã DETALHES - {model_name.upper()}")
            print(f"{'='*60}")
            
            print(f"‚úÖ Acertos: {results['correct_predictions']}/{results['total_tests']} ({summary['accuracy_percentage']}%)")
            print(f"‚ùå Erros: {results['incorrect_predictions']}/{results['total_tests']}")
            print(f"‚è±Ô∏è  Tempo total: {summary['total_time_minutes']} min")
            print(f"‚ö° Tempo m√©dio por teste: {summary['avg_time_per_test']}s")
            print(f"üöÄ Testes por minuto: {summary['tests_per_minute']}")
            
            # üîç AN√ÅLISE DE ERROS
            errors = [detail for detail in results['test_details'] if not detail['correct']]
            if errors:
                print(f"\nüîç AN√ÅLISE DE ERROS ({len(errors)} casos):")
                for error in errors[:3]:  # Mostrar apenas os primeiros 3
                    print(f"   ‚Ä¢ Esperado: {error['expected']} | Obtido: {error['actual']}")
                    if 'reasoning' in error:
                        print(f"     Racioc√≠nio: {error['reasoning'][:80]}...")

        # üéØ RECOMENDA√á√ïES FINAIS
        print(f"\n{'='*60}")
        print("üéØ RECOMENDA√á√ïES E CONCLUS√ïES")
        print(f"{'='*60}")
        
        best_accuracy = max((data['accuracy'] for data in comparison_data), default=0)
        fastest_model = min(comparison_data, key=lambda x: x['avg_time'])['model'] if comparison_data else "N/A"
        most_accurate = max(comparison_data, key=lambda x: x['accuracy'])['model'] if comparison_data else "N/A"
        
        print(f"\nüèÜ Modelo mais preciso: {most_accurate} ({best_accuracy:.1f}% acur√°cia)")
        print(f"‚ö° Modelo mais r√°pido: {fastest_model}")
        
        if best_accuracy >= 80:
            print("\n‚úÖ RESULTADO POSITIVO: Pelo menos um modelo atingiu boa acur√°cia (‚â•80%)")
        else:
            print("\n‚ö†Ô∏è  ATEN√á√ÉO: Nenhum modelo atingiu acur√°cia satisfat√≥ria (‚â•80%)")
            print("   Considere ajustar prompts ou testar outros modelos.")
        
        # üí° RECOMENDA√á√ÉO DE USO
        balanced_scores = []
        for data in comparison_data:
            # Score balanceado: acur√°cia ponderada por velocidade
            balanced_score = data['accuracy'] * (60 / data['avg_time']) if data['avg_time'] > 0 else 0
            balanced_scores.append((data['model'], balanced_score, data['accuracy'], data['avg_time']))
        
        best_balanced = max(balanced_scores, key=lambda x: x[1]) if balanced_scores else None
        
        if best_balanced:
            print(f"\nüí° RECOMENDA√á√ÉO PARA PRODU√á√ÉO:")
            print(f"   Modelo: {best_balanced[0]}")
            print(f"   Justificativa: Melhor equil√≠brio entre acur√°cia ({best_balanced[2]:.1f}%) e velocidade ({best_balanced[3]:.1f}s)")

    async def run_comprehensive_tests(self):
        """üéØ Executar bateria completa de testes"""
        try:
            self.logger.info("üöÄ INICIANDO TESTES EXPANDIDOS DE VALIDA√á√ÉO LLM")
            self.logger.info("=" * 80)
            
            # üìä BUSCAR DADOS REAIS
            self.logger.info("üìä Buscando dados reais do Supabase...")
            licitacoes = self.fetch_real_licitacoes(limit=10)  # Mais licita√ß√µes
            empresas = self.fetch_real_empresas(limit=8)       # Mais empresas
            
            if not licitacoes or not empresas:
                self.logger.error("‚ùå N√£o foi poss√≠vel buscar dados suficientes")
                return
            
            # üéØ CRIAR CEN√ÅRIOS DE TESTE
            scenarios = self.create_test_scenarios(licitacoes, empresas)
            
            if not scenarios:
                self.logger.error("‚ùå Nenhum cen√°rio de teste foi criado")
                return
            
            # üìä RESUMO PR√â-TESTE
            self.logger.info(f"üìã Dados coletados:")
            self.logger.info(f"   ‚Ä¢ {len(licitacoes)} licita√ß√µes")
            self.logger.info(f"   ‚Ä¢ {len(empresas)} empresas") 
            self.logger.info(f"   ‚Ä¢ {len(scenarios)} cen√°rios de teste")
            
            # üöÄ TESTAR CADA MODELO
            all_results = {}
            
            for model_name in self.models_to_test:
                try:
                    self.logger.info(f"\nüéØ Iniciando testes com {model_name}...")
                    results = await self.test_model_performance(model_name, scenarios)
                    all_results[model_name] = results
                    
                except Exception as e:
                    self.logger.error(f"‚ùå Erro ao testar {model_name}: {e}")
                    all_results[model_name] = {'error': str(e)}
            
            # üìä RELAT√ìRIO FINAL DETALHADO
            self.print_comprehensive_report(all_results)
            
            # üíæ SALVAR RESULTADOS
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"test_results_expanded_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'timestamp': timestamp,
                    'test_config': {
                        'models_tested': self.models_to_test,
                        'total_scenarios': len(scenarios),
                        'licitacoes_count': len(licitacoes),
                        'empresas_count': len(empresas)
                    },
                    'results': all_results,
                    'scenarios': scenarios
                }, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"\nüíæ Resultados salvos em: {filename}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro geral nos testes: {e}")
            raise

if __name__ == "__main__":
    print("üî¨ TESTE EXPANDIDO DE VALIDA√á√ÉO LLM COM DADOS REAIS")
    print("=" * 60)
    print("üéØ Testando modelos OLLAMA com prompts otimizados")
    print("üìä Usando dados reais do Supabase")
    print("=" * 60)
    
    tester = EnhancedRealDataLLMTester()
    asyncio.run(tester.run_comprehensive_tests()) 