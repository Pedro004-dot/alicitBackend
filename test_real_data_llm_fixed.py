#!/usr/bin/env python3
"""
ğŸ”¬ TESTE EXPANDIDO DE VALIDAÃ‡ÃƒO LLM COM DADOS REAIS DO SUPABASE
=================================================================

Este script testa os validadores LLM (OLLAMA local) com dados reais do banco Supabase.
Agora com prompts otimizados e cenÃ¡rios de teste mais abrangentes.

Prompts Otimizados:
- QWEN2.5: Foco na velocidade mantendo qualidade
- LLAMA3.2: Mais reflexÃ£o e critÃ©rio rigoroso

Funcionalidades:
âœ… Busca licitaÃ§Ãµes e empresas reais do Supabase  
âœ… Testa validadores OLLAMA (qwen2.5:7b e llama3.2)
âœ… AnÃ¡lise de produtos vs itens detalhada
âœ… Prompts otimizados para cada modelo
âœ… CenÃ¡rios de teste variados (muito especÃ­ficos, genÃ©ricos, incompatÃ­veis)
âœ… MÃ©tricas de performance (tempo + acurÃ¡cia)
"""

import os
import sys
import json
import time
import logging
import asyncio
import psycopg2
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# ğŸ”§ CONFIGURAÃ‡ÃƒO DO AMBIENTE
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
load_dotenv('config.env')

# âš ï¸  IMPORT DIRETO PARA EVITAR PROBLEMAS DE DEPENDÃŠNCIA
import aiohttp
import json as json_lib
from dataclasses import dataclass

@dataclass
class ValidationResult:
    """Resultado da validaÃ§Ã£o LLM"""
    approved: bool
    reasoning: str
    confidence: float = 0.0

class SimpleOllamaValidator:
    """ğŸ”¬ Validador OLLAMA simplificado para testes"""
    
    def __init__(self, model_name: str = "qwen2.5:7b"):
        self.model_name = model_name
        self.base_url = "http://localhost:11434"
    
    def _build_optimized_prompt(self, empresa_nome: str, empresa_descricao: str, 
                              licitacao_objeto: str, similarity_score: float,
                              licitacao_itens: list = None, empresa_produtos: list = None) -> str:
        """ğŸš€ Prompt otimizado especÃ­fico para cada modelo"""
        
        # ValidaÃ§Ã£o robusta dos parÃ¢metros
        if licitacao_itens is None:
            licitacao_itens = []
        if empresa_produtos is None:
            empresa_produtos = []
        
        # Converter para string se necessÃ¡rio
        if isinstance(licitacao_itens, str):
            try:
                licitacao_itens = json_lib.loads(licitacao_itens)
            except:
                licitacao_itens = []
        
        if isinstance(empresa_produtos, str):
            try:
                empresa_produtos = json_lib.loads(empresa_produtos)
            except:
                empresa_produtos = []
        
        # ğŸ¯ PROMPT ESPECÃFICO POR MODELO
        if "qwen" in self.model_name.lower():
            return self._qwen_prompt(empresa_nome, empresa_descricao, licitacao_objeto, 
                                   similarity_score, licitacao_itens, empresa_produtos)
        elif "llama" in self.model_name.lower():
            return self._llama_prompt(empresa_nome, empresa_descricao, licitacao_objeto,
                                    similarity_score, licitacao_itens, empresa_produtos)
        else:
            return self._default_prompt(empresa_nome, empresa_descricao, licitacao_objeto,
                                      similarity_score, licitacao_itens, empresa_produtos)
    
    def _qwen_prompt(self, empresa_nome: str, empresa_descricao: str, licitacao_objeto: str,
                     similarity_score: float, licitacao_itens: list, empresa_produtos: list) -> str:
        """âš¡ PROMPT OTIMIZADO PARA QWEN2.5 - FOCO NA VELOCIDADE"""
        
        itens_str = ""
        if licitacao_itens:
            itens_resumo = []
            for item in licitacao_itens[:3]:  # MÃ¡ximo 3 itens para velocidade
                if isinstance(item, dict):
                    desc = item.get('descricao', '')
                    itens_resumo.append(f"â€¢ {desc}")
            if itens_resumo:
                itens_str = f"\nItens principais:\n" + "\n".join(itens_resumo)
        
        produtos_str = ""
        if empresa_produtos:
            produtos_resumo = empresa_produtos[:5]  # MÃ¡ximo 5 produtos
            produtos_str = f"\nProdutos/serviÃ§os: {', '.join(produtos_resumo)}"
        
        return f"""ğŸ¯ ANÃLISE RÃPIDA DE COMPATIBILIDADE

EMPRESA: {empresa_nome}
DescriÃ§Ã£o: {empresa_descricao}{produtos_str}

LICITAÃ‡ÃƒO: {licitacao_objeto}{itens_str}

Score de similaridade: {similarity_score:.2f}

TAREFA: Avaliar se a empresa pode fornecer o que a licitaÃ§Ã£o solicita.

CRITÃ‰RIOS DIRETOS:
âœ… APROVAR se: produtos/serviÃ§os da empresa atendem diretamente aos itens
âŒ REJEITAR se: incompatibilidade clara ou falta de especificidade

RESPOSTA (JSON):
{{"approved": true/false, "reasoning": "explicaÃ§Ã£o objetiva em 1-2 frases"}}"""
    
    def _llama_prompt(self, empresa_nome: str, empresa_descricao: str, licitacao_objeto: str,
                      similarity_score: float, licitacao_itens: list, empresa_produtos: list) -> str:
        """ğŸ§  PROMPT OTIMIZADO PARA LLAMA3.2 - FOCO NA REFLEXÃƒO E CRITÃ‰RIO"""
        
        itens_str = ""
        if licitacao_itens:
            itens_detalhados = []
            for item in licitacao_itens:
                if isinstance(item, dict):
                    desc = item.get('descricao', '')
                    qtd = item.get('quantidade', '')
                    unidade = item.get('unidade_medida', '')
                    itens_detalhados.append(f"â€¢ {desc} ({qtd} {unidade})")
            if itens_detalhados:
                itens_str = f"\n\nItens detalhados da licitaÃ§Ã£o:\n" + "\n".join(itens_detalhados)
        
        produtos_str = ""
        if empresa_produtos:
            produtos_str = f"\n\nProdutos/serviÃ§os oferecidos pela empresa:\nâ€¢ " + "\nâ€¢ ".join(empresa_produtos)
        
        return f"""ğŸ” ANÃLISE CRITERIOSA DE COMPATIBILIDADE PARA LICITAÃ‡ÃƒO

DADOS DA EMPRESA:
Nome: {empresa_nome}
DescriÃ§Ã£o: {empresa_descricao}{produtos_str}

DADOS DA LICITAÃ‡ÃƒO:
Objeto: {licitacao_objeto}{itens_str}

Score de similaridade inicial: {similarity_score:.2f}

ğŸ§  PROCESSO DE ANÃLISE REFLEXIVA:

1ï¸âƒ£ PRIMEIRO: Analise se os produtos/serviÃ§os da empresa tÃªm RELAÃ‡ÃƒO DIRETA com os itens solicitados.

2ï¸âƒ£ SEGUNDO: Considere se a empresa tem CAPACIDADE TÃ‰CNICA para fornecer especificamente o que Ã© pedido.

3ï¸âƒ£ TERCEIRO: Avalie se existe COMPATIBILIDADE SETORIAL real (nÃ£o apenas palavras genÃ©ricas).

4ï¸âƒ£ QUARTO: QUESTIONE-SE: "Essa empresa realmente pode cumprir este contrato especÃ­fico?"

5ï¸âƒ£ DECISÃƒO FINAL: 
- APROVADO apenas se houver compatibilidade clara e especÃ­fica
- REJEITADO se houver dÃºvidas ou incompatibilidade

ğŸš¨ ATENÃ‡ÃƒO: Seja RIGOROSO. Evite aprovaÃ§Ãµes por similaridade superficial.

RESPOSTA OBRIGATÃ“RIA (JSON):
{{"approved": true/false, "reasoning": "explicaÃ§Ã£o detalhada do raciocÃ­nio em 2-3 frases"}}"""
    
    def _default_prompt(self, empresa_nome: str, empresa_descricao: str, licitacao_objeto: str,
                       similarity_score: float, licitacao_itens: list, empresa_produtos: list) -> str:
        """ğŸ“ Prompt padrÃ£o para outros modelos"""
        return f"""Analise a compatibilidade entre a empresa e a licitaÃ§Ã£o.

EMPRESA: {empresa_nome}
DESCRIÃ‡ÃƒO: {empresa_descricao}
PRODUTOS: {empresa_produtos}

LICITAÃ‡ÃƒO: {licitacao_objeto}
ITENS: {licitacao_itens}

Responda apenas em JSON: {{"approved": true/false, "reasoning": "explicaÃ§Ã£o"}}"""
    
    async def validate_match(self, empresa_nome: str, empresa_descricao: str,
                           licitacao_objeto: str, similarity_score: float,
                           licitacao_itens: list = None, empresa_produtos: list = None) -> ValidationResult:
        """ğŸ” Validar match usando OLLAMA"""
        
        prompt = self._build_optimized_prompt(
            empresa_nome, empresa_descricao, licitacao_objeto,
            similarity_score, licitacao_itens, empresa_produtos
        )
        
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "top_p": 0.9,
                        "num_predict": 200
                    }
                }
                
                async with session.post(
                    f"{self.base_url}/api/generate",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    
                    if response.status != 200:
                        return ValidationResult(False, f"Erro HTTP {response.status}", 0.0)
                    
                    result = await response.json()
                    response_text = result.get('response', '').strip()
                    
                    # Tentar extrair JSON da resposta
                    try:
                        # Buscar JSON na resposta
                        start_idx = response_text.find('{')
                        end_idx = response_text.rfind('}') + 1
                        
                        if start_idx >= 0 and end_idx > start_idx:
                            json_str = response_text[start_idx:end_idx]
                            parsed = json_lib.loads(json_str)
                            
                            approved = parsed.get('approved', False)
                            reasoning = parsed.get('reasoning', 'Sem explicaÃ§Ã£o fornecida')
                            
                            return ValidationResult(approved, reasoning, 0.8)
                        else:
                            # Fallback: tentar interpretar texto
                            approved = any(word in response_text.lower() for word in ['true', 'aprovado', 'sim', 'yes'])
                            return ValidationResult(approved, response_text[:200], 0.5)
                            
                    except json_lib.JSONDecodeError:
                        # Fallback para anÃ¡lise de texto
                        approved = any(word in response_text.lower() for word in ['true', 'aprovado', 'sim'])
                        return ValidationResult(approved, response_text[:200], 0.3)
                        
        except Exception as e:
            return ValidationResult(False, f"Erro na validaÃ§Ã£o: {str(e)}", 0.0)

class EnhancedRealDataLLMTester:
    """ğŸ”¬ Testador avanÃ§ado com dados reais do Supabase"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        self.validators = {}
        
        # ğŸ¯ MODELOS A TESTAR
        self.models_to_test = [
            'qwen2.5:7b',  # Mais rÃ¡pido com prompts otimizados
            'llama3.2'     # Mais criterioso com reflexÃ£o
        ]
        
        self.results = {
            'test_start_time': time.time(),
            'models_tested': {},
            'summary': {}
        }
        
    def _setup_logging(self) -> logging.Logger:
        """Configurar logging detalhado"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('test_real_data_llm.log')
            ]
        )
        return logging.getLogger(__name__)

    def get_db_connection(self):
        """ğŸ”— Conectar ao Supabase usando DATABASE_URL"""
        try:
            database_url = os.getenv('DATABASE_URL')
            if not database_url:
                raise ValueError("DATABASE_URL nÃ£o encontrada no ambiente")
            
            self.logger.info(f"ğŸ”— Conectando ao banco: {database_url[:30]}...")
            conn = psycopg2.connect(database_url)
            self.logger.info("âœ… ConexÃ£o estabelecida com sucesso")
            return conn
            
        except Exception as e:
            self.logger.error(f"âŒ Erro na conexÃ£o: {e}")
            raise
    
    def fetch_real_licitacoes(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ğŸ›ï¸ Buscar licitaÃ§Ãµes reais com mais variedade"""
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            # ğŸ”¥ BUSCA SIMPLIFICADA - Apenas dados essenciais
            query = """
                SELECT 
                    pncp_id,
                    objeto_compra
                FROM licitacoes 
                WHERE objeto_compra IS NOT NULL 
                    AND LENGTH(objeto_compra) > 50
                    AND (
                        objeto_compra ILIKE '%equipamento%' OR
                        objeto_compra ILIKE '%software%' OR
                        objeto_compra ILIKE '%medicamento%' OR
                        objeto_compra ILIKE '%combustÃ­vel%' OR
                        objeto_compra ILIKE '%limpeza%' OR
                        objeto_compra ILIKE '%material%' OR
                        objeto_compra ILIKE '%serviÃ§o%' OR
                        objeto_compra ILIKE '%Ã¡gua%' OR
                        objeto_compra ILIKE '%escritÃ³rio%' OR
                        objeto_compra ILIKE '%veÃ­culo%'
                    )
                ORDER BY RANDOM()
                LIMIT %s;
            """
            
            cursor.execute(query, (limit,))
            results = cursor.fetchall()
            
            licitacoes = []
            for row in results:
                licitacao_base = {
                    'pncp_id': row[0],
                    'objeto_compra': row[1],
                    'situacao_compra_nome': 'Em andamento',  # Valor padrÃ£o
                    'itens': []
                }
                
                # ğŸ” BUSCAR ITENS SEPARADAMENTE PARA ESTA LICITAÃ‡ÃƒO
                itens_query = """
                    SELECT 
                        li.numero_item,
                        li.descricao,
                        li.quantidade,
                        li.valor_unitario_estimado,
                        li.unidade_medida
                    FROM licitacao_itens li
                    INNER JOIN licitacoes l ON l.id = li.licitacao_id
                    WHERE l.pncp_id = %s
                    ORDER BY li.numero_item
                    LIMIT 5;
                """
                
                cursor.execute(itens_query, (row[0],))
                itens_results = cursor.fetchall()
                
                itens = []
                for item_row in itens_results:
                    item = {
                        'numero_item': item_row[0],
                        'descricao': item_row[1],
                        'quantidade': item_row[2],
                        'valor_unitario_estimado': item_row[3],
                        'unidade_medida': item_row[4]
                    }
                    itens.append(item)
                
                licitacao_base['itens'] = itens
                licitacoes.append(licitacao_base)
            
            cursor.close()
            conn.close()
            
            self.logger.info(f"ğŸ“‹ Buscadas {len(licitacoes)} licitaÃ§Ãµes do Supabase")
            return licitacoes
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao buscar licitaÃ§Ãµes: {e}")
            return []

    def fetch_real_empresas(self, limit: int = 8) -> List[Dict[str, Any]]:
        """ğŸ¢ Buscar empresas reais com mais diversidade"""
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            # ğŸ”¥ BUSCA EXPANDIDA - Empresas de setores bem diversos
            query = """
                SELECT 
                    nome_fantasia,
                    descricao_servicos_produtos,
                    produtos
                FROM empresas 
                WHERE descricao_servicos_produtos IS NOT NULL 
                    AND LENGTH(descricao_servicos_produtos) > 20
                    AND produtos IS NOT NULL 
                    AND jsonb_array_length(produtos) > 0
                ORDER BY RANDOM()
                LIMIT %s;
            """
            
            cursor.execute(query, (limit,))
            results = cursor.fetchall()
            
            empresas = []
            for row in results:
                empresa = {
                    'nome_fantasia': row[0],
                    'descricao_servicos_produtos': row[1],
                    'produtos': row[2] if row[2] else []
                }
                empresas.append(empresa)
            
            cursor.close()
            conn.close()
            
            self.logger.info(f"ğŸ¢ Buscadas {len(empresas)} empresas do Supabase")
            return empresas
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao buscar empresas: {e}")
            return []

    def create_test_scenarios(self, licitacoes: List[Dict], empresas: List[Dict]) -> List[Dict]:
        """ğŸ¯ Criar cenÃ¡rios de teste mais inteligentes e variados"""
        scenarios = []
        scenario_id = 1
        
        # ğŸ“Š ANÃLISE INTELIGENTE PARA CRIAR CENÃRIOS DIVERSOS
        self.logger.info("ğŸ§  Analisando dados para criar cenÃ¡rios de teste...")
        
        for licitacao in licitacoes:
            for empresa in empresas:
                
                # ğŸ¯ CATEGORIZAÃ‡ÃƒO INTELIGENTE
                objeto_lower = licitacao['objeto_compra'].lower()
                empresa_desc_lower = empresa['descricao_servicos_produtos'].lower()
                produtos_texto = ' '.join(empresa['produtos']).lower() if empresa['produtos'] else ''
                
                # ğŸ” DETECÃ‡ÃƒO DE COMPATIBILIDADE
                match_type = self._detect_match_type(objeto_lower, empresa_desc_lower, produtos_texto)
                expected_result = self._determine_expected_result(match_type, objeto_lower, empresa_desc_lower, produtos_texto)
                
                scenario = {
                    'id': scenario_id,
                    'match_type': match_type,
                    'expected_result': expected_result,
                    'confidence': self._calculate_confidence(match_type),
                    'empresa_nome': empresa['nome_fantasia'],
                    'empresa_descricao': empresa['descricao_servicos_produtos'],
                    'empresa_produtos': empresa['produtos'],
                    'licitacao_objeto': licitacao['objeto_compra'],
                    'licitacao_itens': licitacao['itens'],
                    'similarity_score': 0.85,  # Score alto para forÃ§ar validaÃ§Ã£o LLM
                    'description': self._generate_scenario_description(match_type, empresa['nome_fantasia'], licitacao['objeto_compra'])
                }
                
                scenarios.append(scenario)
                scenario_id += 1
        
        self.logger.info(f"ğŸ¯ Criados {len(scenarios)} cenÃ¡rios de teste")
        return scenarios

    def _detect_match_type(self, objeto: str, empresa_desc: str, produtos: str) -> str:
        """ğŸ” Detectar tipo de compatibilidade"""
        
        # ğŸ¯ ALTA COMPATIBILIDADE
        high_match_keywords = {
            'medicamento': ['medicamento', 'farmac', 'saÃºde', 'hospital'],
            'software': ['software', 'sistema', 'tecnologia', 'informÃ¡tica', 'ti'],
            'equipamento': ['equipamento', 'mÃ¡quina', 'ferramenta', 'aparelho'],
            'combustÃ­vel': ['combustÃ­vel', 'gasolina', 'diesel', 'posto'],
            'limpeza': ['limpeza', 'higiene', 'detergente', 'produto de limpeza'],
            'escritÃ³rio': ['escritÃ³rio', 'papelaria', 'material escolar'],
            'veÃ­culo': ['veÃ­culo', 'carro', 'caminhÃ£o', 'transporte'],
            'Ã¡gua': ['Ã¡gua', 'mineral', 'bebida']
        }
        
        for categoria, keywords in high_match_keywords.items():
            if any(keyword in objeto for keyword in keywords):
                if any(keyword in empresa_desc or keyword in produtos for keyword in keywords):
                    return 'highly_compatible'
        
        # ğŸ¯ COMPATIBILIDADE GENÃ‰RICA
        generic_keywords = ['material', 'produto', 'serviÃ§o', 'fornecimento', 'aquisiÃ§Ã£o']
        if any(keyword in objeto for keyword in generic_keywords):
            return 'generic_compatible'
        
        # ğŸ¯ INCOMPATÃVEL
        return 'incompatible'

    def _determine_expected_result(self, match_type: str, objeto: str, empresa_desc: str, produtos: str) -> str:
        """ğŸ¯ Determinar resultado esperado"""
        if match_type == 'highly_compatible':
            return 'APROVADO'
        elif match_type == 'generic_compatible':
            # AnÃ¡lise mais detalhada para genÃ©ricos
            if any(word in empresa_desc for word in ['diversos', 'geral', 'variados']):
                return 'APROVADO'
            return 'REJEITADO'
        else:
            return 'REJEITADO'

    def _calculate_confidence(self, match_type: str) -> float:
        """ğŸ“Š Calcular confianÃ§a na prediÃ§Ã£o"""
        confidence_map = {
            'highly_compatible': 0.9,
            'generic_compatible': 0.6,
            'incompatible': 0.95
        }
        return confidence_map.get(match_type, 0.5)

    def _generate_scenario_description(self, match_type: str, empresa: str, licitacao: str) -> str:
        """ğŸ“ Gerar descriÃ§Ã£o do cenÃ¡rio"""
        descriptions = {
            'highly_compatible': f"âœ… MATCH ESPERADO: {empresa} vs {licitacao[:50]}...",
            'generic_compatible': f"â“ GENÃ‰RICO: {empresa} vs {licitacao[:50]}...",
            'incompatible': f"âŒ INCOMPATÃVEL: {empresa} vs {licitacao[:50]}..."
        }
        return descriptions.get(match_type, f"Teste: {empresa} vs {licitacao[:50]}...")

    async def test_model_performance(self, model_name: str, scenarios: List[Dict]) -> Dict:
        """ğŸš€ Testar performance de um modelo especÃ­fico"""
        self.logger.info(f"\nğŸš€ TESTANDO MODELO: {model_name}")
        self.logger.info("=" * 60)
        
        try:
            # ğŸ—ï¸ INICIALIZAR VALIDADOR SIMPLIFICADO
            validator = SimpleOllamaValidator(model_name)
            self.logger.info(f"ğŸ”§ Modelo configurado: {model_name}")
            
            results = {
                'model_name': model_name,
                'total_tests': len(scenarios),
                'correct_predictions': 0,
                'incorrect_predictions': 0,
                'total_time': 0.0,
                'average_time': 0.0,
                'test_details': [],
                'performance_summary': {}
            }
            
            # ğŸ¯ EXECUTAR TESTES
            for i, scenario in enumerate(scenarios):
                self.logger.info(f"\nğŸ“ TESTE {i+1}/{len(scenarios)}: {scenario['description']}")
                
                start_time = time.time()
                
                try:
                    # ğŸ” EXECUTAR VALIDAÃ‡ÃƒO
                    resultado = await validator.validate_match(
                        empresa_nome=scenario['empresa_nome'],
                        empresa_descricao=scenario['empresa_descricao'],
                        licitacao_objeto=scenario['licitacao_objeto'],
                        similarity_score=scenario['similarity_score'],
                        licitacao_itens=scenario['licitacao_itens'],
                        empresa_produtos=scenario['empresa_produtos']
                    )
                    
                    end_time = time.time()
                    test_time = end_time - start_time
                    
                    # ğŸ“Š ANALISAR RESULTADO
                    is_correct = resultado.approved == (scenario['expected_result'] == 'APROVADO')
                    
                    if is_correct:
                        results['correct_predictions'] += 1
                        status = "âœ… CORRETO"
                    else:
                        results['incorrect_predictions'] += 1
                        status = "âŒ INCORRETO"
                    
                    # ğŸ“‹ REGISTRAR DETALHES
                    test_detail = {
                        'scenario_id': scenario['id'],
                        'expected': scenario['expected_result'],
                        'actual': 'APROVADO' if resultado.approved else 'REJEITADO',
                        'correct': is_correct,
                        'time_seconds': round(test_time, 2),
                        'confidence': scenario['confidence'],
                        'reasoning': resultado.reasoning[:200] + "..." if len(resultado.reasoning) > 200 else resultado.reasoning
                    }
                    
                    results['test_details'].append(test_detail)
                    results['total_time'] += test_time
                    
                    # ğŸ“Š LOG DO RESULTADO
                    self.logger.info(f"   {status} | Tempo: {test_time:.2f}s | Esperado: {scenario['expected_result']} | Obtido: {'APROVADO' if resultado.approved else 'REJEITADO'}")
                    self.logger.info(f"   ğŸ’­ RaciocÃ­nio: {resultado.reasoning[:100]}...")
                    
                except Exception as e:
                    self.logger.error(f"   âŒ Erro no teste: {e}")
                    test_detail = {
                        'scenario_id': scenario['id'],
                        'expected': scenario['expected_result'],
                        'actual': 'ERRO',
                        'correct': False,
                        'time_seconds': 0,
                        'error': str(e)
                    }
                    results['test_details'].append(test_detail)
                    results['incorrect_predictions'] += 1
            
            # ğŸ“Š CALCULAR MÃ‰TRICAS FINAIS
            results['average_time'] = results['total_time'] / len(scenarios) if scenarios else 0
            results['accuracy'] = results['correct_predictions'] / len(scenarios) if scenarios else 0
            
            # ğŸ“ˆ RESUMO DE PERFORMANCE
            results['performance_summary'] = {
                'accuracy_percentage': round(results['accuracy'] * 100, 1),
                'total_time_minutes': round(results['total_time'] / 60, 2),
                'avg_time_per_test': round(results['average_time'], 2),
                'tests_per_minute': round(60 / results['average_time'], 1) if results['average_time'] > 0 else 0
            }
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ Erro geral no teste do modelo {model_name}: {e}")
            return {'error': str(e)}

    def print_comprehensive_report(self, all_results: Dict):
        """ğŸ“Š RelatÃ³rio detalhado de todos os testes"""
        print("\n" + "=" * 80)
        print("ğŸ¯ RELATÃ“RIO DETALHADO DE TESTES LLM COM DADOS REAIS")
        print("=" * 80)
        
        print(f"\nâ° Tempo total dos testes: {time.time() - self.results['test_start_time']:.1f}s")
        print(f"ğŸ“‹ Total de cenÃ¡rios testados: {len(all_results.get(list(all_results.keys())[0], {}).get('test_details', []))}")
        
        # ğŸ“Š COMPARAÃ‡ÃƒO ENTRE MODELOS
        print(f"\n{'='*50}")
        print("ğŸ“Š COMPARAÃ‡ÃƒO DE PERFORMANCE ENTRE MODELOS")
        print(f"{'='*50}")
        
        comparison_data = []
        for model_name, results in all_results.items():
            if 'performance_summary' in results:
                summary = results['performance_summary']
                comparison_data.append({
                    'model': model_name,
                    'accuracy': summary['accuracy_percentage'],
                    'avg_time': summary['avg_time_per_test'],
                    'tests_per_min': summary['tests_per_minute']
                })
        
        # ğŸ† RANKING POR ACURÃCIA
        comparison_data.sort(key=lambda x: x['accuracy'], reverse=True)
        
        print(f"\nğŸ† RANKING POR ACURÃCIA:")
        print(f"{'Modelo':<15} {'AcurÃ¡cia':<10} {'Tempo/Teste':<12} {'Testes/Min':<12}")
        print("-" * 55)
        
        for i, data in enumerate(comparison_data):
            medal = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
            print(f"{medal} {data['model']:<13} {data['accuracy']:<9.1f}% {data['avg_time']:<11.2f}s {data['tests_per_min']:<11.1f}")
        
        # âš¡ RANKING POR VELOCIDADE
        comparison_data.sort(key=lambda x: x['avg_time'])
        
        print(f"\nâš¡ RANKING POR VELOCIDADE:")
        print(f"{'Modelo':<15} {'Tempo/Teste':<12} {'Testes/Min':<12} {'AcurÃ¡cia':<10}")
        print("-" * 55)
        
        for i, data in enumerate(comparison_data):
            medal = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
            print(f"{medal} {data['model']:<13} {data['avg_time']:<11.2f}s {data['tests_per_min']:<11.1f} {data['accuracy']:<9.1f}%")
        
        # ğŸ“ˆ DETALHES POR MODELO
        for model_name, results in all_results.items():
            if 'error' in results:
                print(f"\nâŒ ERRO NO MODELO {model_name}: {results['error']}")
                continue
                
            summary = results['performance_summary']
            
            print(f"\n{'='*60}")
            print(f"ğŸ“‹ DETALHES - {model_name.upper()}")
            print(f"{'='*60}")
            
            print(f"âœ… Acertos: {results['correct_predictions']}/{results['total_tests']} ({summary['accuracy_percentage']}%)")
            print(f"âŒ Erros: {results['incorrect_predictions']}/{results['total_tests']}")
            print(f"â±ï¸  Tempo total: {summary['total_time_minutes']} min")
            print(f"âš¡ Tempo mÃ©dio por teste: {summary['avg_time_per_test']}s")
            print(f"ğŸš€ Testes por minuto: {summary['tests_per_minute']}")
            
            # ğŸ” ANÃLISE DE ERROS
            errors = [detail for detail in results['test_details'] if not detail['correct']]
            if errors:
                print(f"\nğŸ” ANÃLISE DE ERROS ({len(errors)} casos):")
                for error in errors[:3]:  # Mostrar apenas os primeiros 3
                    print(f"   â€¢ Esperado: {error['expected']} | Obtido: {error['actual']}")
                    if 'reasoning' in error:
                        print(f"     RaciocÃ­nio: {error['reasoning'][:80]}...")

        # ğŸ¯ RECOMENDAÃ‡Ã•ES FINAIS
        print(f"\n{'='*60}")
        print("ğŸ¯ RECOMENDAÃ‡Ã•ES E CONCLUSÃ•ES")
        print(f"{'='*60}")
        
        best_accuracy = max((data['accuracy'] for data in comparison_data), default=0)
        fastest_model = min(comparison_data, key=lambda x: x['avg_time'])['model'] if comparison_data else "N/A"
        most_accurate = max(comparison_data, key=lambda x: x['accuracy'])['model'] if comparison_data else "N/A"
        
        print(f"\nğŸ† Modelo mais preciso: {most_accurate} ({best_accuracy:.1f}% acurÃ¡cia)")
        print(f"âš¡ Modelo mais rÃ¡pido: {fastest_model}")
        
        if best_accuracy >= 80:
            print("\nâœ… RESULTADO POSITIVO: Pelo menos um modelo atingiu boa acurÃ¡cia (â‰¥80%)")
        else:
            print("\nâš ï¸  ATENÃ‡ÃƒO: Nenhum modelo atingiu acurÃ¡cia satisfatÃ³ria (â‰¥80%)")
            print("   Considere ajustar prompts ou testar outros modelos.")
        
        # ğŸ’¡ RECOMENDAÃ‡ÃƒO DE USO
        balanced_scores = []
        for data in comparison_data:
            # Score balanceado: acurÃ¡cia ponderada por velocidade
            balanced_score = data['accuracy'] * (60 / data['avg_time']) if data['avg_time'] > 0 else 0
            balanced_scores.append((data['model'], balanced_score, data['accuracy'], data['avg_time']))
        
        best_balanced = max(balanced_scores, key=lambda x: x[1]) if balanced_scores else None
        
        if best_balanced:
            print(f"\nğŸ’¡ RECOMENDAÃ‡ÃƒO PARA PRODUÃ‡ÃƒO:")
            print(f"   Modelo: {best_balanced[0]}")
            print(f"   Justificativa: Melhor equilÃ­brio entre acurÃ¡cia ({best_balanced[2]:.1f}%) e velocidade ({best_balanced[3]:.1f}s)")

    async def run_comprehensive_tests(self):
        """ğŸ¯ Executar bateria completa de testes"""
        try:
            self.logger.info("ğŸš€ INICIANDO TESTES EXPANDIDOS DE VALIDAÃ‡ÃƒO LLM")
            self.logger.info("=" * 80)
            
            # ğŸ“Š BUSCAR DADOS REAIS
            self.logger.info("ğŸ“Š Buscando dados reais do Supabase...")
            licitacoes = self.fetch_real_licitacoes(limit=10)  # Mais licitaÃ§Ãµes
            empresas = self.fetch_real_empresas(limit=8)       # Mais empresas
            
            if not licitacoes or not empresas:
                self.logger.error("âŒ NÃ£o foi possÃ­vel buscar dados suficientes")
                return
            
            # ğŸ¯ CRIAR CENÃRIOS DE TESTE
            scenarios = self.create_test_scenarios(licitacoes, empresas)
            
            if not scenarios:
                self.logger.error("âŒ Nenhum cenÃ¡rio de teste foi criado")
                return
            
            # ğŸ“Š RESUMO PRÃ‰-TESTE
            self.logger.info(f"ğŸ“‹ Dados coletados:")
            self.logger.info(f"   â€¢ {len(licitacoes)} licitaÃ§Ãµes")
            self.logger.info(f"   â€¢ {len(empresas)} empresas") 
            self.logger.info(f"   â€¢ {len(scenarios)} cenÃ¡rios de teste")
            
            # ğŸš€ TESTAR CADA MODELO
            all_results = {}
            
            for model_name in self.models_to_test:
                try:
                    self.logger.info(f"\nğŸ¯ Iniciando testes com {model_name}...")
                    results = await self.test_model_performance(model_name, scenarios)
                    all_results[model_name] = results
                    
                except Exception as e:
                    self.logger.error(f"âŒ Erro ao testar {model_name}: {e}")
                    all_results[model_name] = {'error': str(e)}
            
            # ğŸ“Š RELATÃ“RIO FINAL DETALHADO
            self.print_comprehensive_report(all_results)
            
            # ğŸ’¾ SALVAR RESULTADOS
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"test_results_expanded_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'timestamp': timestamp,
                    'test_config': {
                        'models_tested': self.models_to_test,
                        'total_scenarios': len(scenarios),
                        'licitacoes_count': len(licitacoes),
                        'empresas_count': len(empresas)
                    },
                    'results': all_results,
                    'scenarios': scenarios
                }, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"\nğŸ’¾ Resultados salvos em: {filename}")
            
        except Exception as e:
            self.logger.error(f"âŒ Erro geral nos testes: {e}")
            raise

if __name__ == "__main__":
    print("ğŸ”¬ TESTE EXPANDIDO DE VALIDAÃ‡ÃƒO LLM COM DADOS REAIS")
    print("=" * 60)
    print("ğŸ¯ Testando modelos OLLAMA com prompts otimizados")
    print("ğŸ“Š Usando dados reais do Supabase")
    print("=" * 60)
    
    tester = EnhancedRealDataLLMTester()
    asyncio.run(tester.run_comprehensive_tests()) 