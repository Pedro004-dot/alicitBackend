#!/usr/bin/env python3
"""
ğŸ”¬ TESTE EXPANDIDO DE VALIDAÃ‡ÃƒO LLM COM DADOS REAIS DO SUPABASE
=================================================================

Este script testa os validadores LLM (OLLAMA local) com dados reais do banco Supabase.
Agora com prompts otimizados e cenÃ¡rios de teste mais abrangentes.

Prompts Otimizados:
- QWEN2.5: Foco na velocidade mantendo qualidade
- LLAMA3.2: Mais reflexÃ£o e critÃ©rio rigoroso

Funcionalidades:
âœ… Busca licitaÃ§Ãµes e empresas reais do Supabase  
âœ… Testa validadores OLLAMA (qwen2.5:7b e llama3.2)
âœ… AnÃ¡lise de produtos vs itens detalhada
âœ… Prompts otimizados para cada modelo
âœ… CenÃ¡rios de teste variados (muito especÃ­ficos, genÃ©ricos, incompatÃ­veis)
âœ… MÃ©tricas de performance (tempo + acurÃ¡cia)
"""

import os
import sys
import json
import time
import logging
import asyncio
import psycopg2
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# ğŸ”§ CONFIGURAÃ‡ÃƒO DO AMBIENTE
sys.path.append(os.path.dirname(__file__))
load_dotenv('config.env')

# ImportaÃ§Ãµes do projeto
from src.matching.ollama_match_validator import OllamaMatchValidator
from src.matching.llm_config import LLMValidatorFactory

class EnhancedRealDataLLMTester:
    """ğŸ”¬ Testador avanÃ§ado com dados reais do Supabase"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        self.validators = {}
        
        # ğŸ¯ MODELOS A TESTAR
        self.models_to_test = [
            'qwen2.5:7b',  # Mais rÃ¡pido com prompts otimizados
            'llama3.2'     # Mais criterioso com reflexÃ£o
        ]
        
        self.results = {
            'test_start_time': time.time(),
            'models_tested': {},
            'summary': {}
        }
        
    def _setup_logging(self) -> logging.Logger:
        """Configurar logging detalhado"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('test_real_data_llm.log')
            ]
        )
        return logging.getLogger(__name__)

    def get_db_connection(self):
        """ğŸ”— Conectar ao Supabase usando DATABASE_URL"""
        try:
            database_url = os.getenv('DATABASE_URL')
            if not database_url:
                raise ValueError("DATABASE_URL nÃ£o encontrada no ambiente")
            
            self.logger.info(f"ğŸ”— Conectando ao banco: {database_url[:30]}...")
            conn = psycopg2.connect(database_url)
            self.logger.info("âœ… ConexÃ£o estabelecida com sucesso")
            return conn
            
        except Exception as e:
            self.logger.error(f"âŒ Erro na conexÃ£o: {e}")
            raise
    
    def fetch_real_licitacoes(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ğŸ›ï¸ Buscar licitaÃ§Ãµes reais com mais variedade"""
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            # ğŸ”¥ BUSCA EXPANDIDA - Mais licitaÃ§Ãµes de setores variados
            query = """
                SELECT DISTINCT
                    l.pncp_id,
                    l.objeto_compra,
                    l.situacao_compra_nome,
                    COALESCE(
                        CASE 
                            WHEN COUNT(li.id) > 0 
                            THEN json_agg(
                                json_build_object(
                                    'numero_item', li.numero_item,
                                    'descricao', li.descricao,
                                    'quantidade', li.quantidade,
                                    'valor_unitario_estimado', li.valor_unitario_estimado,
                                    'unidade_medida', li.unidade_medida
                                ) ORDER BY li.numero_item
                            )
                            ELSE '[]'::json
                        END, '[]'::json
                    ) as itens
                FROM licitacoes l
                LEFT JOIN licitacao_itens li ON l.id = li.licitacao_id
                WHERE l.objeto_compra IS NOT NULL 
                    AND LENGTH(l.objeto_compra) > 50
                    AND (
                        l.objeto_compra ILIKE '%equipamento%' OR
                        l.objeto_compra ILIKE '%software%' OR
                        l.objeto_compra ILIKE '%medicamento%' OR
                        l.objeto_compra ILIKE '%combustÃ­vel%' OR
                        l.objeto_compra ILIKE '%limpeza%' OR
                        l.objeto_compra ILIKE '%material%' OR
                        l.objeto_compra ILIKE '%serviÃ§o%' OR
                        l.objeto_compra ILIKE '%Ã¡gua%' OR
                        l.objeto_compra ILIKE '%escritÃ³rio%' OR
                        l.objeto_compra ILIKE '%veÃ­culo%'
                    )
                GROUP BY l.pncp_id, l.objeto_compra, l.situacao_compra_nome
                ORDER BY RANDOM()
                LIMIT %s;
            """
            
            cursor.execute(query, (limit,))
            results = cursor.fetchall()
            
            licitacoes = []
            for row in results:
                licitacao = {
                    'pncp_id': row[0],
                    'objeto_compra': row[1],
                    'situacao_compra_nome': row[2] or 'Em andamento',
                    'itens': row[3] if row[3] else []
                }
                licitacoes.append(licitacao)
            
            cursor.close()
            conn.close()
            
            self.logger.info(f"ğŸ“‹ Buscadas {len(licitacoes)} licitaÃ§Ãµes do Supabase")
            return licitacoes
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao buscar licitaÃ§Ãµes: {e}")
            return []

    def fetch_real_empresas(self, limit: int = 8) -> List[Dict[str, Any]]:
        """ğŸ¢ Buscar empresas reais com mais diversidade"""
        try:
            conn = self.get_db_connection()
            cursor = conn.cursor()
            
            # ğŸ”¥ BUSCA EXPANDIDA - Empresas de setores bem diversos
            query = """
                SELECT 
                    nome_fantasia,
                    descricao_servicos_produtos,
                    produtos
                FROM empresas 
                WHERE descricao_servicos_produtos IS NOT NULL 
                    AND LENGTH(descricao_servicos_produtos) > 20
                    AND produtos IS NOT NULL 
                    AND jsonb_array_length(produtos) > 0
                ORDER BY RANDOM()
                LIMIT %s;
            """
            
            cursor.execute(query, (limit,))
            results = cursor.fetchall()
            
            empresas = []
            for row in results:
                empresa = {
                    'nome_fantasia': row[0],
                    'descricao_servicos_produtos': row[1],
                    'produtos': row[2] if row[2] else []
                }
                empresas.append(empresa)
            
            cursor.close()
            conn.close()
            
            self.logger.info(f"ğŸ¢ Buscadas {len(empresas)} empresas do Supabase")
            return empresas
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao buscar empresas: {e}")
            return []

    def create_test_scenarios(self, licitacoes: List[Dict], empresas: List[Dict]) -> List[Dict]:
        """ğŸ¯ Criar cenÃ¡rios de teste mais inteligentes e variados"""
        scenarios = []
        scenario_id = 1
        
        # ğŸ“Š ANÃLISE INTELIGENTE PARA CRIAR CENÃRIOS DIVERSOS
        self.logger.info("ğŸ§  Analisando dados para criar cenÃ¡rios de teste...")
        
        for licitacao in licitacoes:
            for empresa in empresas:
                
                # ğŸ¯ CATEGORIZAÃ‡ÃƒO INTELIGENTE
                objeto_lower = licitacao['objeto_compra'].lower()
                empresa_desc_lower = empresa['descricao_servicos_produtos'].lower()
                produtos_texto = ' '.join(empresa['produtos']).lower() if empresa['produtos'] else ''
                
                # ğŸ” DETECÃ‡ÃƒO DE COMPATIBILIDADE
                match_type = self._detect_match_type(objeto_lower, empresa_desc_lower, produtos_texto)
                expected_result = self._determine_expected_result(match_type, objeto_lower, empresa_desc_lower, produtos_texto)
                
                scenario = {
                    'id': scenario_id,
                    'match_type': match_type,
                    'expected_result': expected_result,
                    'confidence': self._calculate_confidence(match_type),
                    'empresa_nome': empresa['nome_fantasia'],
                    'empresa_descricao': empresa['descricao_servicos_produtos'],
                    'empresa_produtos': empresa['produtos'],
                    'licitacao_objeto': licitacao['objeto_compra'],
                    'licitacao_itens': licitacao['itens'],
                    'similarity_score': 0.85,  # Score alto para forÃ§ar validaÃ§Ã£o LLM
                    'description': self._generate_scenario_description(match_type, empresa['nome_fantasia'], licitacao['objeto_compra'])
                }
                
                scenarios.append(scenario)
                scenario_id += 1
        
        self.logger.info(f"ğŸ¯ Criados {len(scenarios)} cenÃ¡rios de teste")
        return scenarios

    def _detect_match_type(self, objeto: str, empresa_desc: str, produtos: str) -> str:
        """ğŸ” Detectar tipo de compatibilidade"""
        
        # ğŸ¯ ALTA COMPATIBILIDADE
        high_match_keywords = {
            'medicamento': ['medicamento', 'farmac', 'saÃºde', 'hospital'],
            'software': ['software', 'sistema', 'tecnologia', 'informÃ¡tica', 'ti'],
            'equipamento': ['equipamento', 'mÃ¡quina', 'ferramenta', 'aparelho'],
            'combustÃ­vel': ['combustÃ­vel', 'gasolina', 'diesel', 'posto'],
            'limpeza': ['limpeza', 'higiene', 'detergente', 'produto de limpeza'],
            'escritÃ³rio': ['escritÃ³rio', 'papelaria', 'material escolar'],
            'veÃ­culo': ['veÃ­culo', 'carro', 'caminhÃ£o', 'transporte'],
            'Ã¡gua': ['Ã¡gua', 'mineral', 'bebida']
        }
        
        for categoria, keywords in high_match_keywords.items():
            if any(keyword in objeto for keyword in keywords):
                if any(keyword in empresa_desc or keyword in produtos for keyword in keywords):
                    return 'highly_compatible'
        
        # ğŸ¯ COMPATIBILIDADE GENÃ‰RICA
        generic_keywords = ['material', 'produto', 'serviÃ§o', 'fornecimento', 'aquisiÃ§Ã£o']
        if any(keyword in objeto for keyword in generic_keywords):
            return 'generic_compatible'
        
        # ğŸ¯ INCOMPATÃVEL
        return 'incompatible'

    def _determine_expected_result(self, match_type: str, objeto: str, empresa_desc: str, produtos: str) -> str:
        """ğŸ¯ Determinar resultado esperado"""
        if match_type == 'highly_compatible':
            return 'APROVADO'
        elif match_type == 'generic_compatible':
            # AnÃ¡lise mais detalhada para genÃ©ricos
            if any(word in empresa_desc for word in ['diversos', 'geral', 'variados']):
                return 'APROVADO'
            return 'REJEITADO'
        else:
            return 'REJEITADO'

    def _calculate_confidence(self, match_type: str) -> float:
        """ğŸ“Š Calcular confianÃ§a na prediÃ§Ã£o"""
        confidence_map = {
            'highly_compatible': 0.9,
            'generic_compatible': 0.6,
            'incompatible': 0.95
        }
        return confidence_map.get(match_type, 0.5)

    def _generate_scenario_description(self, match_type: str, empresa: str, licitacao: str) -> str:
        """ğŸ“ Gerar descriÃ§Ã£o do cenÃ¡rio"""
        descriptions = {
            'highly_compatible': f"âœ… MATCH ESPERADO: {empresa} vs {licitacao[:50]}...",
            'generic_compatible': f"â“ GENÃ‰RICO: {empresa} vs {licitacao[:50]}...",
            'incompatible': f"âŒ INCOMPATÃVEL: {empresa} vs {licitacao[:50]}..."
        }
        return descriptions.get(match_type, f"Teste: {empresa} vs {licitacao[:50]}...")

    async def test_model_performance(self, model_name: str, scenarios: List[Dict]) -> Dict:
        """ğŸš€ Testar performance de um modelo especÃ­fico"""
        self.logger.info(f"\nğŸš€ TESTANDO MODELO: {model_name}")
        self.logger.info("=" * 60)
        
        try:
            # ğŸ—ï¸ INICIALIZAR VALIDADOR
            factory = LLMValidatorFactory()
            validator = factory.get_validator()
            
            if not isinstance(validator, OllamaMatchValidator):
                self.logger.error(f"âŒ Esperado OllamaMatchValidator, got {type(validator)}")
                return {'error': 'Invalid validator type'}
            
            # ğŸ”§ CONFIGURAR MODELO
            validator.model_name = model_name
            self.logger.info(f"ğŸ”§ Modelo configurado: {model_name}")
            
            results = {
                'model_name': model_name,
                'total_tests': len(scenarios),
                'correct_predictions': 0,
                'incorrect_predictions': 0,
                'total_time': 0.0,
                'average_time': 0.0,
                'test_details': [],
                'performance_summary': {}
            }
            
            # ğŸ¯ EXECUTAR TESTES
            for i, scenario in enumerate(scenarios):
                self.logger.info(f"\nğŸ“ TESTE {i+1}/{len(scenarios)}: {scenario['description']}")
                
                start_time = time.time()
                
                try:
                    # ğŸ” EXECUTAR VALIDAÃ‡ÃƒO
                    resultado = await validator.validate_match(
                        empresa_nome=scenario['empresa_nome'],
                        empresa_descricao=scenario['empresa_descricao'],
                        licitacao_objeto=scenario['licitacao_objeto'],
                        similarity_score=scenario['similarity_score'],
                        licitacao_itens=scenario['licitacao_itens'],
                        empresa_produtos=scenario['empresa_produtos']
                    )
                    
                    end_time = time.time()
                    test_time = end_time - start_time
                    
                    # ğŸ“Š ANALISAR RESULTADO
                    is_correct = resultado.approved == (scenario['expected_result'] == 'APROVADO')
                    
                    if is_correct:
                        results['correct_predictions'] += 1
                        status = "âœ… CORRETO"
                    else:
                        results['incorrect_predictions'] += 1
                        status = "âŒ INCORRETO"
                    
                    # ğŸ“‹ REGISTRAR DETALHES
                    test_detail = {
                        'scenario_id': scenario['id'],
                        'expected': scenario['expected_result'],
                        'actual': 'APROVADO' if resultado.approved else 'REJEITADO',
                        'correct': is_correct,
                        'time_seconds': round(test_time, 2),
                        'confidence': scenario['confidence'],
                        'reasoning': resultado.reasoning[:200] + "..." if len(resultado.reasoning) > 200 else resultado.reasoning
                    }
                    
                    results['test_details'].append(test_detail)
                    results['total_time'] += test_time
                    
                    # ğŸ“Š LOG DO RESULTADO
                    self.logger.info(f"   {status} | Tempo: {test_time:.2f}s | Esperado: {scenario['expected_result']} | Obtido: {'APROVADO' if resultado.approved else 'REJEITADO'}")
                    self.logger.info(f"   ğŸ’­ RaciocÃ­nio: {resultado.reasoning[:100]}...")
                    
                except Exception as e:
                    self.logger.error(f"   âŒ Erro no teste: {e}")
                    test_detail = {
                        'scenario_id': scenario['id'],
                        'expected': scenario['expected_result'],
                        'actual': 'ERRO',
                        'correct': False,
                        'time_seconds': 0,
                        'error': str(e)
                    }
                    results['test_details'].append(test_detail)
                    results['incorrect_predictions'] += 1
            
            # ğŸ“Š CALCULAR MÃ‰TRICAS FINAIS
            results['average_time'] = results['total_time'] / len(scenarios) if scenarios else 0
            results['accuracy'] = results['correct_predictions'] / len(scenarios) if scenarios else 0
            
            # ğŸ“ˆ RESUMO DE PERFORMANCE
            results['performance_summary'] = {
                'accuracy_percentage': round(results['accuracy'] * 100, 1),
                'total_time_minutes': round(results['total_time'] / 60, 2),
                'avg_time_per_test': round(results['average_time'], 2),
                'tests_per_minute': round(60 / results['average_time'], 1) if results['average_time'] > 0 else 0
            }
            
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ Erro geral no teste do modelo {model_name}: {e}")
            return {'error': str(e)}

    def print_comprehensive_report(self, all_results: Dict):
        """ğŸ“Š RelatÃ³rio detalhado de todos os testes"""
        print("\n" + "=" * 80)
        print("ğŸ¯ RELATÃ“RIO DETALHADO DE TESTES LLM COM DADOS REAIS")
        print("=" * 80)
        
        print(f"\nâ° Tempo total dos testes: {time.time() - self.results['test_start_time']:.1f}s")
        print(f"ğŸ“‹ Total de cenÃ¡rios testados: {len(all_results.get(list(all_results.keys())[0], {}).get('test_details', []))}")
        
        # ğŸ“Š COMPARAÃ‡ÃƒO ENTRE MODELOS
        print(f"\n{'='*50}")
        print("ğŸ“Š COMPARAÃ‡ÃƒO DE PERFORMANCE ENTRE MODELOS")
        print(f"{'='*50}")
        
        comparison_data = []
        for model_name, results in all_results.items():
            if 'performance_summary' in results:
                summary = results['performance_summary']
                comparison_data.append({
                    'model': model_name,
                    'accuracy': summary['accuracy_percentage'],
                    'avg_time': summary['avg_time_per_test'],
                    'tests_per_min': summary['tests_per_minute']
                })
        
        # ğŸ† RANKING POR ACURÃCIA
        comparison_data.sort(key=lambda x: x['accuracy'], reverse=True)
        
        print(f"\nğŸ† RANKING POR ACURÃCIA:")
        print(f"{'Modelo':<15} {'AcurÃ¡cia':<10} {'Tempo/Teste':<12} {'Testes/Min':<12}")
        print("-" * 55)
        
        for i, data in enumerate(comparison_data):
            medal = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
            print(f"{medal} {data['model']:<13} {data['accuracy']:<9.1f}% {data['avg_time']:<11.2f}s {data['tests_per_min']:<11.1f}")
        
        # âš¡ RANKING POR VELOCIDADE
        comparison_data.sort(key=lambda x: x['avg_time'])
        
        print(f"\nâš¡ RANKING POR VELOCIDADE:")
        print(f"{'Modelo':<15} {'Tempo/Teste':<12} {'Testes/Min':<12} {'AcurÃ¡cia':<10}")
        print("-" * 55)
        
        for i, data in enumerate(comparison_data):
            medal = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else "  "
            print(f"{medal} {data['model']:<13} {data['avg_time']:<11.2f}s {data['tests_per_min']:<11.1f} {data['accuracy']:<9.1f}%")
        
        # ğŸ“ˆ DETALHES POR MODELO
        for model_name, results in all_results.items():
            if 'error' in results:
                print(f"\nâŒ ERRO NO MODELO {model_name}: {results['error']}")
                continue
                
            summary = results['performance_summary']
            
            print(f"\n{'='*60}")
            print(f"ğŸ“‹ DETALHES - {model_name.upper()}")
            print(f"{'='*60}")
            
            print(f"âœ… Acertos: {results['correct_predictions']}/{results['total_tests']} ({summary['accuracy_percentage']}%)")
            print(f"âŒ Erros: {results['incorrect_predictions']}/{results['total_tests']}")
            print(f"â±ï¸  Tempo total: {summary['total_time_minutes']} min")
            print(f"âš¡ Tempo mÃ©dio por teste: {summary['avg_time_per_test']}s")
            print(f"ğŸš€ Testes por minuto: {summary['tests_per_minute']}")
            
            # ğŸ” ANÃLISE DE ERROS
            errors = [detail for detail in results['test_details'] if not detail['correct']]
            if errors:
                print(f"\nğŸ” ANÃLISE DE ERROS ({len(errors)} casos):")
                for error in errors[:3]:  # Mostrar apenas os primeiros 3
                    print(f"   â€¢ Esperado: {error['expected']} | Obtido: {error['actual']}")
                    if 'reasoning' in error:
                        print(f"     RaciocÃ­nio: {error['reasoning'][:80]}...")

        # ğŸ¯ RECOMENDAÃ‡Ã•ES FINAIS
        print(f"\n{'='*60}")
        print("ğŸ¯ RECOMENDAÃ‡Ã•ES E CONCLUSÃ•ES")
        print(f"{'='*60}")
        
        best_accuracy = max((data['accuracy'] for data in comparison_data), default=0)
        fastest_model = min(comparison_data, key=lambda x: x['avg_time'])['model'] if comparison_data else "N/A"
        most_accurate = max(comparison_data, key=lambda x: x['accuracy'])['model'] if comparison_data else "N/A"
        
        print(f"\nğŸ† Modelo mais preciso: {most_accurate} ({best_accuracy:.1f}% acurÃ¡cia)")
        print(f"âš¡ Modelo mais rÃ¡pido: {fastest_model}")
        
        if best_accuracy >= 80:
            print("\nâœ… RESULTADO POSITIVO: Pelo menos um modelo atingiu boa acurÃ¡cia (â‰¥80%)")
        else:
            print("\nâš ï¸  ATENÃ‡ÃƒO: Nenhum modelo atingiu acurÃ¡cia satisfatÃ³ria (â‰¥80%)")
            print("   Considere ajustar prompts ou testar outros modelos.")
        
        # ğŸ’¡ RECOMENDAÃ‡ÃƒO DE USO
        balanced_scores = []
        for data in comparison_data:
            # Score balanceado: acurÃ¡cia ponderada por velocidade
            balanced_score = data['accuracy'] * (60 / data['avg_time']) if data['avg_time'] > 0 else 0
            balanced_scores.append((data['model'], balanced_score, data['accuracy'], data['avg_time']))
        
        best_balanced = max(balanced_scores, key=lambda x: x[1]) if balanced_scores else None
        
        if best_balanced:
            print(f"\nğŸ’¡ RECOMENDAÃ‡ÃƒO PARA PRODUÃ‡ÃƒO:")
            print(f"   Modelo: {best_balanced[0]}")
            print(f"   Justificativa: Melhor equilÃ­brio entre acurÃ¡cia ({best_balanced[2]:.1f}%) e velocidade ({best_balanced[3]:.1f}s)")

    async def run_comprehensive_tests(self):
        """ğŸ¯ Executar bateria completa de testes"""
        try:
            self.logger.info("ğŸš€ INICIANDO TESTES EXPANDIDOS DE VALIDAÃ‡ÃƒO LLM")
            self.logger.info("=" * 80)
            
            # ğŸ“Š BUSCAR DADOS REAIS
            self.logger.info("ğŸ“Š Buscando dados reais do Supabase...")
            licitacoes = self.fetch_real_licitacoes(limit=10)  # Mais licitaÃ§Ãµes
            empresas = self.fetch_real_empresas(limit=8)       # Mais empresas
            
            if not licitacoes or not empresas:
                self.logger.error("âŒ NÃ£o foi possÃ­vel buscar dados suficientes")
                return
            
            # ğŸ¯ CRIAR CENÃRIOS DE TESTE
            scenarios = self.create_test_scenarios(licitacoes, empresas)
            
            if not scenarios:
                self.logger.error("âŒ Nenhum cenÃ¡rio de teste foi criado")
                return
            
            # ğŸ“Š RESUMO PRÃ‰-TESTE
            self.logger.info(f"ğŸ“‹ Dados coletados:")
            self.logger.info(f"   â€¢ {len(licitacoes)} licitaÃ§Ãµes")
            self.logger.info(f"   â€¢ {len(empresas)} empresas") 
            self.logger.info(f"   â€¢ {len(scenarios)} cenÃ¡rios de teste")
            
            # ğŸš€ TESTAR CADA MODELO
            all_results = {}
            
            for model_name in self.models_to_test:
                try:
                    self.logger.info(f"\nğŸ¯ Iniciando testes com {model_name}...")
                    results = await self.test_model_performance(model_name, scenarios)
                    all_results[model_name] = results
                    
                except Exception as e:
                    self.logger.error(f"âŒ Erro ao testar {model_name}: {e}")
                    all_results[model_name] = {'error': str(e)}
            
            # ğŸ“Š RELATÃ“RIO FINAL DETALHADO
            self.print_comprehensive_report(all_results)
            
            # ğŸ’¾ SALVAR RESULTADOS
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"test_results_expanded_{timestamp}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'timestamp': timestamp,
                    'test_config': {
                        'models_tested': self.models_to_test,
                        'total_scenarios': len(scenarios),
                        'licitacoes_count': len(licitacoes),
                        'empresas_count': len(empresas)
                    },
                    'results': all_results,
                    'scenarios': scenarios
                }, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"\nğŸ’¾ Resultados salvos em: {filename}")
            
        except Exception as e:
            self.logger.error(f"âŒ Erro geral nos testes: {e}")
            raise

if __name__ == "__main__":
    print("ğŸ”¬ TESTE EXPANDIDO DE VALIDAÃ‡ÃƒO LLM COM DADOS REAIS")
    print("=" * 60)
    print("ğŸ¯ Testando modelos OLLAMA com prompts otimizados")
    print("ğŸ“Š Usando dados reais do Supabase")
    print("=" * 60)
    
    tester = EnhancedRealDataLLMTester()
    asyncio.run(tester.run_comprehensive_tests()) 