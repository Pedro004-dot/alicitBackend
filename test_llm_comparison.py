#!/usr/bin/env python3
"""
üß™ TESTE COMPARATIVO: llama3.2 vs qwen2.5:7b
Compara velocidade e acur√°cia dos dois modelos LLM para valida√ß√£o de matches
"""

import time
import json
import logging
import requests
from datetime import datetime
from typing import Dict, List, Any, Optional

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OllamaModelTester:
    def __init__(self):
        self.ollama_base_url = "http://localhost:11434"
        self.models = ["llama3.2:latest", "qwen2.5:7b"]
        self.test_results = {}
    
    def check_ollama_health(self):
        """Verificar se Ollama est√° rodando"""
        try:
            response = requests.get(f"{self.ollama_base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                available_models = [model['name'] for model in response.json().get('models', [])]
                logger.info(f"ü¶ô Ollama ativo. Modelos dispon√≠veis: {available_models}")
                return available_models
            return []
        except Exception as e:
            logger.error(f"‚ùå Erro ao conectar com Ollama: {e}")
            return []
    
    def build_optimized_prompt_llama32(self, empresa_nome: str, empresa_descricao: str, 
                                       empresa_produtos: List[str], licitacao_objeto: str, 
                                       licitacao_itens: List[str], similarity_score: float) -> str:
        """
        Prompt OTIMIZADO para llama3.2 - mais direto e rigoroso
        """
        produtos_texto = "\n".join([f"‚Ä¢ {p}" for p in empresa_produtos]) if empresa_produtos else "N√£o especificados"
        itens_texto = "\n".join([f"‚Ä¢ {i}" for i in licitacao_itens]) if licitacao_itens else "N√£o especificados"
        
        return f"""AN√ÅLISE RIGOROSA DE COMPATIBILIDADE

**EMPRESA: {empresa_nome}**
Descri√ß√£o: {empresa_descricao}
Produtos/Servi√ßos:
{produtos_texto}

**LICITA√á√ÉO:**
Objeto: {licitacao_objeto}
Itens espec√≠ficos:
{itens_texto}

**INSTRU√á√ïES CR√çTICAS:**
1. SEJA RIGOROSO! Apenas aprove se houver CLARA compatibilidade
2. A empresa DEVE ter produtos/servi√ßos DIRETAMENTE relacionados
3. N√ÉO fa√ßa conex√µes for√ßadas ou interpreta√ß√µes vagas
4. REJEITE qualquer d√∫vida ou incompatibilidade

**CRIT√âRIOS DE APROVA√á√ÉO:**
‚úÖ Empresa tem produtos ESPEC√çFICOS para os itens solicitados
‚úÖ Experi√™ncia COMPROVADA na √°rea
‚úÖ Match t√©cnico √ìBVIO

Responda APENAS em JSON:
{{"compativel": true/false, "confianca": 0.XX, "justificativa": "explica√ß√£o concisa"}}"""

    def build_current_prompt_qwen(self, empresa_nome: str, empresa_descricao: str, 
                                  empresa_produtos: List[str], licitacao_objeto: str, 
                                  licitacao_itens: List[str], similarity_score: float) -> str:
        """
        Prompt atual usado pelo qwen2.5:7b
        """
        produtos_texto = "\n".join([f"‚Ä¢ {p}" for p in empresa_produtos]) if empresa_produtos else "N√£o especificados"
        itens_texto = "\n".join([f"‚Ä¢ {i}" for i in licitacao_itens]) if licitacao_itens else "N√£o especificados"
        
        return f"""AN√ÅLISE DE COMPATIBILIDADE DETALHADA (PRODUTO A PRODUTO)

### DADOS DA EMPRESA
- **NOME:** {empresa_nome}
- **DESCRI√á√ÉO:** {empresa_descricao}
- **PRODUTOS/SERVI√áOS:**
{produtos_texto}

### DADOS DA LICITA√á√ÉO
- **OBJETO:** {licitacao_objeto}
- **ITENS ESPEC√çFICOS:**
{itens_texto}

**Score Sem√¢ntico:** {similarity_score:.1%}

Analise se a empresa tem CAPACIDADE REAL de atender esta licita√ß√£o, considerando:
1. Produtos espec√≠ficos da empresa vs itens da licita√ß√£o
2. Experi√™ncia no setor
3. Viabilidade t√©cnica e comercial

Responda APENAS com o formato JSON:
{{"compativel": true/false, "confianca": 0.XX, "justificativa": "explica√ß√£o detalhada"}}"""

    def call_ollama_model(self, model: str, prompt: str, timeout: int = 60) -> Dict[str, Any]:
        """Chamar modelo espec√≠fico no Ollama"""
        try:
            start_time = time.time()
            
            response = requests.post(
                f"{self.ollama_base_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "top_p": 0.9,
                        "num_predict": 500
                    }
                },
                timeout=timeout
            )
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            if response.status_code == 200:
                result = response.json()
                raw_response = result.get("response", "").strip()
                
                # Tentar fazer parse do JSON
                try:
                    # Extrair JSON da resposta
                    json_start = raw_response.find('{')
                    json_end = raw_response.rfind('}') + 1
                    
                    if json_start >= 0 and json_end > json_start:
                        json_str = raw_response[json_start:json_end]
                        parsed_result = json.loads(json_str)
                        
                        return {
                            "success": True,
                            "processing_time": processing_time,
                            "raw_response": raw_response,
                            "parsed_result": parsed_result,
                            "compativel": parsed_result.get("compativel", False),
                            "confianca": parsed_result.get("confianca", 0.0),
                            "justificativa": parsed_result.get("justificativa", "")
                        }
                    else:
                        raise ValueError("JSON n√£o encontrado na resposta")
                        
                except (json.JSONDecodeError, ValueError) as e:
                    return {
                        "success": False,
                        "processing_time": processing_time,
                        "error": f"Erro ao parsear JSON: {e}",
                        "raw_response": raw_response
                    }
            else:
                return {
                    "success": False,
                    "processing_time": processing_time,
                    "error": f"HTTP {response.status_code}: {response.text}"
                }
                
        except Exception as e:
            return {
                "success": False,
                "processing_time": 0,
                "error": str(e)
            }

    def run_comparison_test(self):
        """Executar teste comparativo completo"""
        
        # Casos de teste representativos
        test_cases = [
            {
                "nome": "Match √ìbvio - DEVE APROVAR",
                "empresa_nome": "Materias de escritorio",
                "empresa_descricao": "Distribuidora de materiais de escrit√≥rio e papelaria",
                "empresa_produtos": ["caneta", "papel", "grampeador", "pastas", "lapis", "borrachas"],
                "licitacao_objeto": "Aquisi√ß√£o de materiais de expediente para escrit√≥rio",
                "licitacao_itens": ["Canetas esferogr√°ficas", "Papel A4", "Grampeadores", "Pastas suspensas"],
                "expected": True
            },
            {
                "nome": "Match Imposs√≠vel - DEVE REJEITAR", 
                "empresa_nome": "AgroFrare",
                "empresa_descricao": "Empresa especializada em solu√ß√µes para o agroneg√≥cio",
                "empresa_produtos": ["fertilizantes", "defensivos", "sementes", "consultoria agron√¥mica"],
                "licitacao_objeto": "Aquisi√ß√£o de licen√ßas de software de an√°lise de dados",
                "licitacao_itens": ["Licen√ßas perp√©tuas I2 Analyst", "Treinamento online", "Suporte t√©cnico"],
                "expected": False
            },
            {
                "nome": "Match Lim√≠trofe - Software vs TI",
                "empresa_nome": "Clonex", 
                "empresa_descricao": "Empresa desenvolvedora de softwares com IA",
                "empresa_produtos": ["Inteligencia artificial", "Software"],
                "licitacao_objeto": "Aquisi√ß√£o de materiais de expediente",
                "licitacao_itens": ["Baterias", "Calculadoras", "Dispositivos de armazenamento"],
                "expected": False
            },
            {
                "nome": "Match Question√°vel - Agro vs Constru√ß√£o",
                "empresa_nome": "AgroFrare",
                "empresa_descricao": "Empresa especializada em solu√ß√µes para o agroneg√≥cio", 
                "empresa_produtos": ["fertilizantes", "defensivos", "sementes"],
                "licitacao_objeto": "Contrata√ß√£o para execu√ß√£o de pavimenta√ß√£o asf√°ltica",
                "licitacao_itens": ["Asfalto", "Brita", "M√£o de obra especializada"],
                "expected": False
            },
            {
                "nome": "Match Correto - Limpeza vs Limpeza",
                "empresa_nome": "Risadinha company",
                "empresa_descricao": "Distribuidora de produtos de limpeza e descart√°veis",
                "empresa_produtos": ["produtos de limpeza", "materiais descart√°veis", "produtos de higiene"],
                "licitacao_objeto": "Aquisi√ß√£o de materiais de higiene e limpeza",
                "licitacao_itens": ["Detergente", "Desinfetante", "Papel toalha", "Sacos de lixo"],
                "expected": True
            }
        ]
        
        print("üß™ INICIANDO TESTE COMPARATIVO LLM")
        print("=" * 60)
        
        # Verificar se modelos est√£o dispon√≠veis
        available_models = self.check_ollama_health()
        for model in self.models:
            if model not in available_models:
                logger.error(f"‚ùå Modelo {model} n√£o est√° dispon√≠vel!")
                return
        
        results = {}
        
        for model in self.models:
            print(f"\nü§ñ TESTANDO MODELO: {model}")
            print("-" * 40)
            
            results[model] = {
                "total_time": 0,
                "successful_tests": 0,
                "failed_tests": 0,
                "correct_decisions": 0,
                "test_details": []
            }
            
            for i, test_case in enumerate(test_cases, 1):
                print(f"\n[{i}/5] {test_case['nome']}")
                
                # Escolher prompt baseado no modelo
                if model == "llama3.2":
                    prompt = self.build_optimized_prompt_llama32(
                        test_case["empresa_nome"],
                        test_case["empresa_descricao"], 
                        test_case["empresa_produtos"],
                        test_case["licitacao_objeto"],
                        test_case["licitacao_itens"],
                        0.75
                    )
                else:
                    prompt = self.build_current_prompt_qwen(
                        test_case["empresa_nome"],
                        test_case["empresa_descricao"],
                        test_case["empresa_produtos"], 
                        test_case["licitacao_objeto"],
                        test_case["licitacao_itens"],
                        0.75
                    )
                
                result = self.call_ollama_model(model, prompt)
                
                if result["success"]:
                    decision = result["compativel"]
                    confidence = result["confianca"]
                    processing_time = result["processing_time"]
                    
                    # Verificar se a decis√£o est√° correta
                    is_correct = decision == test_case["expected"]
                    
                    print(f"   ‚è±Ô∏è  Tempo: {processing_time:.1f}s")
                    print(f"   üéØ Decis√£o: {'‚úÖ APROVOU' if decision else 'üö´ REJEITOU'}")
                    print(f"   üìä Confian√ßa: {confidence:.0%}")
                    print(f"   ‚úÖ Correto: {'SIM' if is_correct else 'N√ÉO'}")
                    
                    results[model]["total_time"] += processing_time
                    results[model]["successful_tests"] += 1
                    if is_correct:
                        results[model]["correct_decisions"] += 1
                        
                    results[model]["test_details"].append({
                        "test_name": test_case["nome"],
                        "expected": test_case["expected"],
                        "decision": decision,
                        "confidence": confidence,
                        "processing_time": processing_time,
                        "is_correct": is_correct,
                        "justificativa": result["justificativa"]
                    })
                else:
                    print(f"   ‚ùå ERRO: {result['error']}")
                    results[model]["failed_tests"] += 1
        
        # An√°lise comparativa
        self.print_comparison_results(results)
        
        # Salvar resultados detalhados
        self.save_detailed_results(results)
        
        return results

    def print_comparison_results(self, results: Dict):
        """Imprimir an√°lise comparativa dos resultados"""
        print("\n" + "=" * 60)
        print("üìä AN√ÅLISE COMPARATIVA DOS RESULTADOS")
        print("=" * 60)
        
        for model in self.models:
            data = results[model]
            avg_time = data["total_time"] / max(data["successful_tests"], 1)
            accuracy = (data["correct_decisions"] / max(data["successful_tests"], 1)) * 100
            
            print(f"\nü§ñ {model.upper()}:")
            print(f"   ‚è±Ô∏è  Tempo m√©dio por valida√ß√£o: {avg_time:.1f}s")
            print(f"   üéØ Acur√°cia: {accuracy:.1f}% ({data['correct_decisions']}/{data['successful_tests']})")
            print(f"   ‚úÖ Testes bem-sucedidos: {data['successful_tests']}")
            print(f"   ‚ùå Testes falhados: {data['failed_tests']}")
        
        # Compara√ß√£o direta
        if len(self.models) == 2:
            model1, model2 = self.models
            data1, data2 = results[model1], results[model2]
            
            avg_time1 = data1["total_time"] / max(data1["successful_tests"], 1)
            avg_time2 = data2["total_time"] / max(data2["successful_tests"], 1)
            
            accuracy1 = (data1["correct_decisions"] / max(data1["successful_tests"], 1)) * 100
            accuracy2 = (data2["correct_decisions"] / max(data2["successful_tests"], 1)) * 100
            
            print(f"\nüèÜ COMPARA√á√ÉO DIRETA:")
            print(f"   ‚ö° Velocidade: {model1} √© {avg_time2/avg_time1:.1f}x {'mais r√°pido' if avg_time1 < avg_time2 else 'mais lento'}")
            print(f"   üéØ Acur√°cia: {model1} {accuracy1:.1f}% vs {model2} {accuracy2:.1f}%")
            
            if accuracy1 > accuracy2 and avg_time1 < avg_time2:
                print(f"   üèÜ VENCEDOR ABSOLUTO: {model1} (mais r√°pido E mais preciso)")
            elif accuracy1 > accuracy2:
                print(f"   üèÜ MAIS PRECISO: {model1}")
            elif avg_time1 < avg_time2:
                print(f"   üèÜ MAIS R√ÅPIDO: {model1}")
            else:
                print(f"   üèÜ MAIS PRECISO: {model2}")

    def save_detailed_results(self, results: Dict):
        """Salvar resultados detalhados em arquivo JSON"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"llm_comparison_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Resultados detalhados salvos em: {filename}")

def main():
    """Fun√ß√£o principal"""
    tester = OllamaModelTester()
    tester.run_comparison_test()

if __name__ == "__main__":
    main() 