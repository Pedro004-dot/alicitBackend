"""
ðŸ¦™ VALIDADOR LLM LOCAL USANDO OLLAMA
IntegraÃ§Ã£o com qwen2.5:7b para validaÃ§Ã£o de matches de licitaÃ§Ãµes
"""

import os
import json
import logging
import requests
import time
from typing import Dict, Any, Optional
from config.llm_config import LLMConfig
from dotenv import load_dotenv

# Carregar variÃ¡veis de ambiente
load_dotenv('config.env')

logger = logging.getLogger(__name__)

class OllamaMatchValidator:
    """
    ðŸ¦™ Validador de matches usando Ollama local
    
    Usa qwen2.5:7b para anÃ¡lise semÃ¢ntica de compatibilidade
    entre empresas e licitaÃ§Ãµes, com fallback para OpenAI.
    """

    def __init__(self):
        """Inicializar o validador Ollama"""
        self.config = LLMConfig.get_ollama_config()
        self._setup_ollama()
        
        # Thresholds para validaÃ§Ã£o (ajustados conforme config.env)
        self.HIGH_SCORE_THRESHOLD = float(os.getenv('SIMILARITY_THRESHOLD_PHASE1', '0.70'))  # Threshold do config.env
        self.LLM_CONFIDENCE_THRESHOLD = 0.65  # AprovaÃ§Ã£o LLM
        
    def _setup_ollama(self):
        """Configurar conexÃ£o com Ollama"""
        try:
            response = requests.get(
                f"{self.config['url']}/api/tags",
                timeout=5
            )
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m['name'] for m in models]
                
                if self.config['model'] in model_names:
                    logger.info(f"ðŸ¦™ Ollama conectado - Modelo: {self.config['model']}")
                    self.ollama_available = True
                else:
                    logger.error(f"âŒ Modelo {self.config['model']} nÃ£o encontrado. DisponÃ­veis: {model_names}")
                    self.ollama_available = False
            else:
                logger.error(f"âŒ Ollama retornou status {response.status_code}")
                self.ollama_available = False
                
        except Exception as e:
            logger.error(f"âŒ Erro conectando Ollama: {e}")
            self.ollama_available = False

    def should_validate_with_llm(self, score: float) -> bool:
        """
        ðŸ”¥ NOVA POLÃTICA: Validar TODOS os matches acima do threshold
        """
        return score >= self.HIGH_SCORE_THRESHOLD

    def validate_match(
        self,
        empresa_nome: str,
        empresa_descricao: str,
        licitacao_objeto: str,
        pncp_id: str,
        similarity_score: float,
        licitacao_itens: Optional[list] = None,
        empresa_produtos: Optional[list] = None  # ðŸ”€ NOVO: Produtos da empresa
    ) -> Dict[str, Any]:
        """
        ðŸ¦™ Validar match usando Ollama com fallback OpenAI
        
        Returns:
            Dict com 'is_valid', 'confidence', 'reasoning'
        """
        
        if not self.ollama_available:
            logger.warning("ðŸ¦™ Ollama indisponÃ­vel, usando fallback OpenAI")
            return self._fallback_to_openai(
                empresa_nome, empresa_descricao, licitacao_objeto, 
                pncp_id, similarity_score, licitacao_itens, empresa_produtos
            )
        
        try:
            start_time = time.time()
            
            # Construir prompt otimizado para qwen2.5:7b
            prompt = self._build_validation_prompt(
                empresa_nome, empresa_descricao, licitacao_objeto, similarity_score, licitacao_itens, empresa_produtos
            )
            
            # Fazer requisiÃ§Ã£o para Ollama
            payload = {
                "model": self.config['model'],
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": self.config['temperature'],
                    "top_p": 0.9,
                    "repeat_penalty": 1.1
                }
            }
            
            response = requests.post(
                f"{self.config['url']}/api/generate",
                json=payload,
                timeout=self.config['timeout']
            )
            
            if response.status_code != 200:
                logger.error(f"âŒ Ollama erro HTTP {response.status_code}")
                return self._fallback_to_openai(
                    empresa_nome, empresa_descricao, licitacao_objeto, 
                    pncp_id, similarity_score, licitacao_itens, empresa_produtos
                )
            
            result = response.json()
            llm_response = result.get('response', '').strip()
            
            processing_time = time.time() - start_time
            logger.info(f"ðŸ¦™ Ollama processou em {processing_time:.2f}s")
            
            # Analisar resposta do LLM
            validation_result = self._parse_llm_response(llm_response, similarity_score)
            
            # Log detalhado
            logger.info(
                f"ðŸ¦™ OLLAMA VALIDATION - "
                f"Empresa: {empresa_nome[:30]}... | "
                f"Objeto: {licitacao_objeto[:40]}... | "
                f"Score: {similarity_score:.1%} | "
                f"DecisÃ£o: {'âœ… APROVADO' if validation_result['is_valid'] else 'ðŸš« REJEITADO'} | "
                f"ConfianÃ§a: {validation_result['confidence']:.1%}"
            )
            
            return validation_result
            
        except Exception as e:
            logger.error(f"âŒ Erro na validaÃ§Ã£o Ollama: {e}")
            return self._fallback_to_openai(
                empresa_nome, empresa_descricao, licitacao_objeto, 
                pncp_id, similarity_score, licitacao_itens, empresa_produtos
            )

    def _build_validation_prompt(
        self, 
        empresa_nome: str, 
        empresa_descricao: str, 
        licitacao_objeto: str, 
        similarity_score: float,
        licitacao_itens: Optional[list] = None,
        empresa_produtos: Optional[list] = None
    ) -> str:
        """
        ðŸš€ Construir prompt OTIMIZADO especÃ­fico para cada modelo
        """
        
        # ðŸ”€ ValidaÃ§Ã£o robusta dos parÃ¢metros de entrada
        if licitacao_itens is None:
            licitacao_itens = []
        elif isinstance(licitacao_itens, str):
            try:
                import json
                licitacao_itens = json.loads(licitacao_itens)
            except (json.JSONDecodeError, TypeError):
                licitacao_itens = []
        elif not isinstance(licitacao_itens, list):
            licitacao_itens = []
        
        if empresa_produtos is None:
            empresa_produtos = []
        elif isinstance(empresa_produtos, str):
            try:
                import json
                empresa_produtos = json.loads(empresa_produtos)
            except (json.JSONDecodeError, TypeError):
                empresa_produtos = []
        elif not isinstance(empresa_produtos, list):
            empresa_produtos = []
        
        # FormataÃ§Ã£o otimizada dos dados
        produtos_texto = "\n".join([f"â€¢ {p}" for p in empresa_produtos[:5]]) if empresa_produtos else "NÃ£o especificados"
        itens_texto = "\n".join([f"â€¢ {item.get('descricao', str(item))[:80]}..." if isinstance(item, dict) 
                                else f"â€¢ {str(item)[:80]}..." for item in licitacao_itens[:4]]) if licitacao_itens else "NÃ£o especificados"
        
        # ðŸš€ PROMPT OTIMIZADO PARA QWEN2.5:7B - Mais direto e estruturado
        if self.config['model'] in ['qwen2.5:7b', 'qwen2.5']:
            return f"""ANÃLISE RÃPIDA DE COMPATIBILIDADE COMERCIAL

EMPRESA: {empresa_nome}
PRODUTOS: {produtos_texto}

LICITAÃ‡ÃƒO: {licitacao_objeto[:150]}
ITENS: {itens_texto}

SCORE SEMÃ‚NTICO: {similarity_score:.0%}

CRITÃ‰RIOS:
1. Produtos da empresa atendem itens da licitaÃ§Ã£o?
2. Ãrea de atuaÃ§Ã£o Ã© compatÃ­vel?
3. Empresa tem capacidade tÃ©cnica?

DECISÃƒO: [SIM/NÃƒO]
CONFIANÃ‡A: [0-100]%
JUSTIFICATIVA: [mÃ¡ximo 100 caracteres]"""

        # ðŸ¦™ PROMPT MELHORADO PARA LLAMA3.2 - Mais reflexivo e criterioso
        else:
            return f"""VALIDAÃ‡ÃƒO CRITERIOSA DE COMPATIBILIDADE

### ANÃLISE DETALHADA NECESSÃRIA ###

EMPRESA CANDIDATA:
- Nome: {empresa_nome}
- DescriÃ§Ã£o: {empresa_descricao[:100]}
- Produtos/ServiÃ§os: {produtos_texto}

DEMANDA DA LICITAÃ‡ÃƒO:
- Objeto: {licitacao_objeto[:200]}
- Itens EspecÃ­ficos: {itens_texto}
- Score Inicial: {similarity_score:.1%}

### INSTRUÃ‡Ã•ES PARA ANÃLISE RIGOROSA ###

1. **PRIMEIRO PASSO**: Analise se os produtos da empresa sÃ£o DIRETAMENTE relacionados aos itens da licitaÃ§Ã£o
2. **SEGUNDO PASSO**: Verifique se a empresa tem EXPERIÃŠNCIA COMPROVADA na Ã¡rea
3. **TERCEIRO PASSO**: Considere se Ã© COMERCIALMENTE VIÃVEL para a empresa participar
4. **DECISÃƒO FINAL**: Seja CRITERIOSO - aprove apenas matches com alta probabilidade de sucesso

IMPORTANTE: 
- NÃƒO aprove por "possÃ­vel compatibilidade" - exija compatibilidade CLARA
- NÃƒO confunda Ã¡rea geral com especializaÃ§Ã£o especÃ­fica
- REJEITE se houver dÃºvidas significativas sobre capacidade

RESPOSTA OBRIGATÃ“RIA:
DECISÃƒO: [SIM/NÃƒO]
CONFIANÃ‡A: [0-100]%
JUSTIFICATIVA: [explicaÃ§Ã£o da decisÃ£o em atÃ© 150 caracteres]"""

    def _parse_llm_response(self, response: str, similarity_score: float) -> Dict[str, Any]:
        """
        ðŸ§  Analisar resposta do LLM e extrair decisÃ£o estruturada
        """
        response_lower = response.lower().strip()
        
        # Buscar decisÃ£o
        is_valid = False
        confidence = 0.5  # Default
        reasoning = response.strip()
        
        # AnÃ¡lise de padrÃµes na resposta
        if any(palavra in response_lower for palavra in ['sim', 'compatÃ­vel', 'adequado', 'capaz']):
            is_valid = True
            confidence = 0.75
        elif any(palavra in response_lower for palavra in ['nÃ£o', 'incompatÃ­vel', 'inadequado', 'incapaz']):
            is_valid = False
            confidence = 0.8
        
        # Buscar confianÃ§a explÃ­cita (formato CONFIANÃ‡A: XX%)
        import re
        confidence_match = re.search(r'confianÃ§a[:\s]*(\d+)%?', response_lower)
        if confidence_match:
            try:
                confidence = float(confidence_match.group(1)) / 100
            except:
                pass
        
        # Ajustar baseado no score semÃ¢ntico
        if similarity_score >= 0.85:
            confidence = min(confidence + 0.1, 1.0)
        elif similarity_score <= 0.60:
            confidence = max(confidence - 0.1, 0.1)
        
        # Aplicar threshold de confianÃ§a
        final_is_valid = is_valid and confidence >= self.LLM_CONFIDENCE_THRESHOLD
        
        return {
            'is_valid': final_is_valid,
            'confidence': confidence,
            'reasoning': reasoning[:200],  # Limitar tamanho
            'provider': 'ollama',
            'model': self.config['model']
        }

    def _fallback_to_openai(
        self,
        empresa_nome: str,
        empresa_descricao: str,
        licitacao_objeto: str,
        pncp_id: str,
        similarity_score: float,
        licitacao_itens: Optional[list] = None,
        empresa_produtos: Optional[list] = None  # ðŸ”€ NOVO: Produtos da empresa
    ) -> Dict[str, Any]:
        """
        ðŸ”„ Fallback para OpenAI quando Ollama falha
        """
        try:
            from matching.llm_match_validator import LLMMatchValidator
            
            logger.info("ðŸ”„ Usando OpenAI como fallback...")
            openai_validator = LLMMatchValidator()
            
            result = openai_validator.validate_match(
                empresa_nome, empresa_descricao, licitacao_objeto, 
                pncp_id, similarity_score, licitacao_itens, empresa_produtos
            )
            
            # Marcar como fallback
            result['provider'] = 'openai_fallback'
            return result
            
        except Exception as e:
            logger.error(f"âŒ Fallback OpenAI tambÃ©m falhou: {e}")
            
            # Ãšltimo recurso: aprovaÃ§Ã£o conservadora baseada no score
            is_valid = similarity_score >= 0.85  # SÃ³ aprova scores muito altos
            
            return {
                'is_valid': is_valid,
                'confidence': 0.5,
                'reasoning': f"Fallback: Score {similarity_score:.1%} {'â‰¥' if is_valid else '<'} 85%",
                'provider': 'fallback_conservative'
            } 