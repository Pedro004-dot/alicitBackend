#!/usr/bin/env python3
"""
Sistema principal de matching de licita√ß√µes usando API real do PNCP
Busca licita√ß√µes do dia atual em todos os estados brasileiros e faz matching sem√¢ntico
VERS√ÉO OTIMIZADA: Cache apenas Redis LOCAL para m√°xima performance
"""

import os
import datetime
from typing import Dict, Any
import time
from psycopg2.extras import DictCursor

from services.embedding_cache_service import EmbeddingCacheService
from services.deduplication_service import DeduplicationService
from config.database import db_manager

from .vectorizers import (
    BaseTextVectorizer, BrazilianTextVectorizer, OpenAITextVectorizer, VoyageAITextVectorizer,
    HybridTextVectorizer, MockTextVectorizer, calculate_enhanced_similarity
)
from .llm_match_validator import LLMMatchValidator
from .pncp_api import (
    get_db_connection, get_all_companies_from_db, get_processed_bid_ids,
    fetch_bids_from_pncp, fetch_bid_items_from_pncp, save_bid_to_db,
    save_bid_items_to_db, save_match_to_db, update_bid_status,
    get_existing_bids_from_db, get_bid_items_from_db, clear_existing_matches,
    ESTADOS_BRASIL, PNCP_MAX_PAGES
)

# --- Configura√ß√µes do Matching ---
SIMILARITY_THRESHOLD_PHASE1 = float(os.getenv('SIMILARITY_THRESHOLD_PHASE1', '0.65'))
SIMILARITY_THRESHOLD_PHASE2 = float(os.getenv('SIMILARITY_THRESHOLD_PHASE2', '0.70'))


def process_daily_bids(vectorizer: BaseTextVectorizer, enable_llm_validation: bool = True):
    """
    VERS√ÉO REDIS LOCAL: Cache otimizado apenas com Redis local + Valida√ß√£o LLM
    """
    print("üöÄ Iniciando busca de licita√ß√µes com CACHE REDIS LOCAL + VALIDA√á√ÉO LLM...")
    print(f"üîß Vectorizador: {type(vectorizer).__name__}")
    print(f"üìä Thresholds: Fase 1 = {SIMILARITY_THRESHOLD_PHASE1} | Fase 2 = {SIMILARITY_THRESHOLD_PHASE2}")
    print(f"ü§ñ Valida√ß√£o LLM: {'ATIVADA' if enable_llm_validation else 'DESATIVADA'}")
    
    # üî• NOVO: Cache apenas Redis local
    cache_service = EmbeddingCacheService(db_manager)
    dedup_service = DeduplicationService(db_manager, cache_service)
    
    # ü§ñ Inicializar validador LLM
    llm_validator = None
    if enable_llm_validation:
        llm_validator = LLMMatchValidator()
        print(f"ü§ñ Validador LLM configurado (threshold: {llm_validator.HIGH_SCORE_THRESHOLD:.1%})")
    
    # Data de hoje
    today = datetime.date.today()
    date_str = today.strftime("%Y%m%d")
    
    print(f"üìÖ Buscando licita√ß√µes do dia: {today.strftime('%d/%m/%Y')}")
    
    # 1. Carregar empresas 
    print("\nüè¢ Carregando empresas do banco...")
    companies = get_all_companies_from_db()
    print(f"   ‚úÖ {len(companies)} empresas carregadas")
    if not companies:
        print("‚ùå Nenhuma empresa encontrada no banco. Cadastre empresas primeiro.")
        return

    # üî• OTIMIZA√á√ÉO: Vetorizar empresas com cache em lote
    print("üî¢ Vetorizando descri√ß√µes das empresas com CACHE REDIS LOCAL...")
    _vectorize_companies_with_cache(companies, cache_service, vectorizer)
        
    # 2. Buscar licita√ß√µes do PNCP
    print(f"\nüåê Buscando licita√ß√µes do PNCP para todos os estados...")
    processed_bid_ids = get_processed_bid_ids()
    new_bids = []
    total_found = 0
    
    for uf in ESTADOS_BRASIL:
        page = 1
        uf_bids = 0
        
        while page <= PNCP_MAX_PAGES:
            bids, has_more_pages = fetch_bids_from_pncp(date_str, date_str, uf, page)
            
            if not bids:
                break
            
            for bid in bids:
                pncp_id = bid["numeroControlePNCP"]
                if pncp_id not in processed_bid_ids:
                    new_bids.append(bid)
                    uf_bids += 1
                    total_found += 1
            
            if not has_more_pages:
                break
            
            page += 1
            time.sleep(0.5)  # Pausa para n√£o sobrecarregar a API
        
        if uf_bids > 0:
            print(f"   üìç {uf}: {uf_bids} novas licita√ß√µes")
    
    print(f"\nüéØ Total de novas licita√ß√µes encontradas: {total_found}")
    
    # 3. Filtrar licita√ß√µes j√° processadas
    print(f"\nüîç Verificando duplicatas...")
    truly_new_bids = []
    skipped_count = 0
    
    for bid in new_bids:
        licitacao_data = {
            'objeto_compra': bid.get("objetoCompra", ""),
            'pncp_id': bid["numeroControlePNCP"],
            'data_publicacao': bid.get("dataPublicacaoPncp", "")
        }
        
        if dedup_service.should_process_licitacao(bid["numeroControlePNCP"], licitacao_data):
            truly_new_bids.append(bid)
        else:
            skipped_count += 1
    
    print(f"üìä Licita√ß√µes filtradas: {len(truly_new_bids)} novas, {skipped_count} j√° processadas")
    
    if not truly_new_bids:
        print("‚úÖ Nenhuma licita√ß√£o nova para processar hoje.")
        return
    
    # 4. üî• OTIMIZA√á√ÉO: Processar licita√ß√µes com cache em lote Redis
    print(f"\n‚ö° Processando {len(truly_new_bids)} licita√ß√µes com CACHE REDIS LOCAL...")
    matches_encontrados = 0
    redis_cache_hits = 0
    estatisticas = {
        'total_processadas': 0,
        'com_matches': 0,
        'sem_matches': 0,
        'matches_fase1_apenas': 0,
        'matches_fase2': 0,
        'llm_validations_count': 0,
        'llm_approved': 0,
        'llm_rejected': 0
    }
    
    # üî• PR√â-CACHE: Buscar embeddings de todas as licita√ß√µes em lote
    bid_texts = [bid.get("objetoCompra", "") for bid in truly_new_bids if bid.get("objetoCompra")]
    cached_bid_embeddings = cache_service.batch_get_embeddings_from_cache(bid_texts)
    redis_cache_hits = len(cached_bid_embeddings)
    
    print(f"   ‚ö° Cache Redis: {redis_cache_hits}/{len(bid_texts)} embeddings encontrados")
    
    for i, bid in enumerate(truly_new_bids, 1):
        pncp_id = bid["numeroControlePNCP"]
        objeto_compra = bid.get("objetoCompra", "")
        
        print(f"\n[{i}/{len(truly_new_bids)}] üîç Processando: {pncp_id}")
        print(f"   üìù Objeto: {objeto_compra[:100]}...")
        
        if not objeto_compra:
            print("   ‚ö†Ô∏è  Objeto da compra vazio, pulando...")
            continue
        
        # Salvar licita√ß√£o no banco
        licitacao_id = save_bid_to_db(bid)
        
        # Buscar itens da licita√ß√£o
        items = fetch_bid_items_from_pncp(bid)
        if items:
            save_bid_items_to_db(licitacao_id, items)
        
        # üî• CACHE: Usar embedding do cache ou gerar novo
        if objeto_compra in cached_bid_embeddings:
            bid_embedding = cached_bid_embeddings[objeto_compra]
            print(f"   ‚ö° Embedding do cache Redis")
        else:
            bid_embedding = vectorizer.vectorize(objeto_compra)
            if bid_embedding:
                cache_service.save_embedding_to_cache(objeto_compra, bid_embedding)
                print(f"   üÜï Novo embedding gerado e cacheado")
        
        if not bid_embedding:
            print("   ‚ùå Erro ao vetorizar objeto da compra")
            continue
        
        estatisticas['total_processadas'] += 1
        
        # FASE 1: Matching do objeto completo
        potential_matches = []
        print("   üîç FASE 1 - An√°lise sem√¢ntica do objeto da compra:")
        
        for company in companies:
            if not company.get("embedding"):
                continue
            
            score, justificativa = calculate_enhanced_similarity(
                bid_embedding, 
                company["embedding"], 
                objeto_compra, 
                company["descricao_servicos_produtos"]
            )
            
            print(f"      üè¢ {company['nome']}: Score = {score:.3f}")
            
            if score >= SIMILARITY_THRESHOLD_PHASE1:
                # ü§ñ VALIDA√á√ÉO LLM PARA SCORES ALTOS
                should_accept_match = True
                final_score = score
                final_justificativa = justificativa
                
                if llm_validator and llm_validator.should_validate_with_llm(score):
                    print(f"         ü§ñ VALIDA√á√ÉO LLM (score {score:.1%} > {llm_validator.HIGH_SCORE_THRESHOLD:.1%})")
                    
                    validation = llm_validator.validate_match(
                        empresa_nome=company['nome'],
                        empresa_descricao=company['descricao_servicos_produtos'],
                        licitacao_objeto=objeto_compra,
                        pncp_id=pncp_id,
                        similarity_score=score
                    )
                    
                    estatisticas['llm_validations_count'] += 1
                    
                    if validation['is_valid']:
                        print(f"         üéØ LLM APROVOU! Confian√ßa: {validation['confidence']:.1%}")
                        final_score = validation['confidence']
                        final_justificativa += f" | LLM: {validation['reasoning'][:100]}..."
                        estatisticas['llm_approved'] += 1
                    else:
                        print(f"         üö´ LLM REJEITOU: {validation['reasoning'][:80]}...")
                        should_accept_match = False
                        estatisticas['llm_rejected'] += 1
                
                if should_accept_match:
                    potential_matches.append((company, final_score, final_justificativa))
                    print(f"         ‚úÖ POTENCIAL MATCH ACEITO!")
                else:
                    print(f"         ‚ùå Rejeitado pela valida√ß√£o LLM")
        
        if potential_matches:
            print(f"   üéØ {len(potential_matches)} potenciais matches encontrados!")
            estatisticas['com_matches'] += 1
            
            # FASE 2: Refinamento com itens (se dispon√≠vel)
            if items:
                print(f"   üìã {len(items)} itens encontrados. Iniciando FASE 2...")
                _process_phase2_matching(items, potential_matches, pncp_id, cache_service, vectorizer, estatisticas)
            else:
                print("   üìã Sem itens - usando apenas Fase 1")
                _process_phase1_only_matching(potential_matches, pncp_id, estatisticas)
                
            matches_encontrados += len(potential_matches)
        else:
            print("   ‚ùå Nenhum potencial match na Fase 1")
            estatisticas['sem_matches'] += 1
        
        # Marcar como processada
        licitacao_data = {
            'objeto_compra': objeto_compra,
            'pncp_id': pncp_id,
            'data_publicacao': bid.get("dataPublicacaoPncp", "")
        }
        dedup_service.mark_licitacao_processed(pncp_id, licitacao_data)
        update_bid_status(pncp_id, "processada")
        
        time.sleep(0.1)  # Pausa menor devido ao cache otimizado
    
    # Relat√≥rio final
    print(f"\nüìä ESTAT√çSTICAS REDIS CACHE:")
    print(f"   ‚ö° Cache hits Redis: {redis_cache_hits}/{len(bid_texts)}")
    
    # Exibir stats do cache
    cache_stats = cache_service.get_cache_stats()
    if cache_stats.get('status') == 'active':
        print(f"   üìà Total embeddings em cache: {cache_stats['cache_stats']['match_keys']}")
        print(f"   üéØ Efici√™ncia do cache: {cache_stats['performance']['cache_efficiency']}")
    
    _print_final_report(matches_encontrados, estatisticas)


def _vectorize_companies_with_cache(companies, cache_service, vectorizer):
    """üî• OTIMIZA√á√ÉO: Vetorizar empresas usando cache em lote Redis"""
    company_texts = [company["descricao_servicos_produtos"] for company in companies]
    
    # Buscar embeddings em lote do Redis
    cached_embeddings = cache_service.batch_get_embeddings_from_cache(company_texts)
    cache_hits = len(cached_embeddings)
    
    # Processar empresas que n√£o est√£o no cache
    texts_to_generate = []
    companies_to_update = []
    
    for i, company in enumerate(companies):
        texto_empresa = company["descricao_servicos_produtos"]
        
        if texto_empresa in cached_embeddings:
            company["embedding"] = cached_embeddings[texto_empresa]
        else:
            # Marcar para gerar embedding
            texts_to_generate.append(texto_empresa)
            companies_to_update.append(company)
    
    print(f"   ‚ö° Cache Redis hits: {cache_hits}/{len(companies)} empresas")
    
    # Gerar embeddings faltantes em lote
    if texts_to_generate:
        print(f"   üîÑ Gerando {len(texts_to_generate)} novos embeddings...")
        
        new_embeddings = vectorizer.batch_vectorize(texts_to_generate)
        
        if new_embeddings:
            # Salvar novos embeddings no cache em lote
            texts_and_embeddings = list(zip(texts_to_generate, new_embeddings))
            cache_service.batch_save_embeddings_to_cache(texts_and_embeddings)
            
            # Atribuir aos objetos empresa
            for i, company in enumerate(companies_to_update):
                if i < len(new_embeddings):
                    company["embedding"] = new_embeddings[i]
                else:
                    company["embedding"] = []
    
    # Verificar quantas empresas ficaram com embedding
    companies_with_embeddings = sum(1 for c in companies if c.get("embedding"))
    print(f"   ‚úÖ {companies_with_embeddings}/{len(companies)} empresas vetorizadas")


def _process_phase2_matching(items, potential_matches, pncp_id, cache_service, vectorizer, estatisticas):
    """Processa Fase 2 com otimiza√ß√£o de cache"""
    item_descriptions = [item.get("descricao", "") for item in items]
    
    # üî• OTIMIZA√á√ÉO: Buscar embeddings dos itens em lote do cache
    cached_item_embeddings = cache_service.batch_get_embeddings_from_cache(item_descriptions)
    
    # Gerar embeddings faltantes
    texts_to_generate = [desc for desc in item_descriptions if desc not in cached_item_embeddings]
    
    if texts_to_generate:
        new_embeddings = vectorizer.batch_vectorize(texts_to_generate)
        if new_embeddings:
            # Salvar no cache em lote
            texts_and_embeddings = list(zip(texts_to_generate, new_embeddings))
            cache_service.batch_save_embeddings_to_cache(texts_and_embeddings)
            
            # Adicionar aos embeddings cached
            for text, emb in zip(texts_to_generate, new_embeddings):
                cached_item_embeddings[text] = emb
    
    # Processar matches com embeddings otimizados
    for company, score_fase1, justificativa_fase1 in potential_matches:
        item_matches = 0
        total_item_score = 0.0
        
        for desc in item_descriptions:
            if desc not in cached_item_embeddings:
                continue
                
            item_embedding = cached_item_embeddings[desc]
            item_score, item_justificativa = calculate_enhanced_similarity(
                item_embedding, 
                company["embedding"],
                desc,
                company["descricao_servicos_produtos"]
            )
            
            if item_score >= SIMILARITY_THRESHOLD_PHASE2:
                item_matches += 1
                total_item_score += item_score
        
        if item_matches > 0:
            final_score = (score_fase1 + (total_item_score / item_matches)) / 2
            combined_justificativa = f"Fase 1: {justificativa_fase1} | Fase 2: {item_matches} itens matched"
            
            save_match_to_db(pncp_id, company["id"], final_score, "objeto_e_itens", combined_justificativa)
            estatisticas['matches_fase2'] += 1
            
            print(f"      üéØ MATCH FINAL! {company['nome']} - Score: {final_score:.3f}")


def _process_phase1_only_matching(potential_matches, pncp_id, estatisticas):
    """Processa matches apenas da Fase 1"""
    for company, score, justificativa in potential_matches:
        save_match_to_db(pncp_id, company["id"], score, "objeto_completo", 
                        f"Apenas Fase 1: {justificativa}")
        estatisticas['matches_fase1_apenas'] += 1
        print(f"      üéØ MATCH! {company['nome']} - Score: {score:.3f}")


def reevaluate_existing_bids(vectorizer: BaseTextVectorizer, clear_matches: bool = True, enable_llm_validation: bool = True):
    """
    Reavalia todas as licita√ß√µes existentes no banco contra as empresas cadastradas
    VERS√ÉO REDIS LOCAL: Cache otimizado apenas com Redis local + Valida√ß√£o LLM
    """
    print("=" * 80)
    print("üîÑ REAVALIA√á√ÉO COM CACHE REDIS LOCAL + VALIDA√á√ÉO LLM")
    print("=" * 80)
    print(f"üîß Vectorizador: {type(vectorizer).__name__}")
    print(f"üìä Thresholds: Fase 1 = {SIMILARITY_THRESHOLD_PHASE1} | Fase 2 = {SIMILARITY_THRESHOLD_PHASE2}")
    print(f"ü§ñ Valida√ß√£o LLM: {'ATIVADA' if enable_llm_validation else 'DESATIVADA'}")
    
    # üî• Cache apenas Redis local
    cache_service = EmbeddingCacheService(db_manager)
    
    # ü§ñ Inicializar validador LLM
    llm_validator = None
    if enable_llm_validation:
        llm_validator = LLMMatchValidator()
        print(f"ü§ñ Validador LLM configurado (threshold: {llm_validator.HIGH_SCORE_THRESHOLD:.1%})")
    
    if clear_matches:
        clear_existing_matches()

    # 1. Carregar empresas e vetorizar com cache Redis
    print("\nüè¢ Carregando empresas do banco...")
    companies = get_all_companies_from_db()
    print(f"   ‚úÖ {len(companies)} empresas carregadas")
    
    if not companies:
        print("‚ùå Nenhuma empresa encontrada no banco. Cadastre empresas primeiro.")
        return
    
    # üî• OTIMIZA√á√ÉO: Vetorizar empresas com cache em lote
    _vectorize_companies_with_cache(companies, cache_service, vectorizer)
    
    # 2. Carregar licita√ß√µes existentes
    print(f"\nüìÑ Carregando licita√ß√µes do banco...")
    existing_bids = get_existing_bids_from_db()
    print(f"   ‚úÖ {len(existing_bids)} licita√ß√µes encontradas")
    
    if not existing_bids:
        print("‚ùå Nenhuma licita√ß√£o encontrada no banco.")
        return
    
    # 3. üî• OTIMIZA√á√ÉO: Processar licita√ß√µes com cache em lote Redis
    print(f"\n‚ö° Iniciando reavalia√ß√£o com CACHE REDIS LOCAL...")
    
    # Pr√©-carregar embeddings de licita√ß√µes em lote
    bid_texts = [bid['objeto_compra'] for bid in existing_bids if bid['objeto_compra']]
    cached_bid_embeddings = cache_service.batch_get_embeddings_from_cache(bid_texts)
    redis_cache_hits = len(cached_bid_embeddings)
    
    print(f"   ‚ö° Cache Redis: {redis_cache_hits}/{len(bid_texts)} embeddings de licita√ß√µes encontrados")
    
    matches_encontrados = 0
    estatisticas = {
        'total_processadas': 0,
        'com_matches': 0,
        'sem_matches': 0,
        'matches_fase1_apenas': 0,
        'matches_fase2': 0,
        'vetorizacao_falhou': 0,
        'llm_validations_count': 0,
        'llm_approved': 0,
        'llm_rejected': 0
    }
    
    for i, bid in enumerate(existing_bids, 1):
        objeto_compra = bid['objeto_compra']
        pncp_id = bid['pncp_id']
        
        print(f"\n[{i}/{len(existing_bids)}] üîç Reavaliando: {pncp_id}")
        print(f"   üìù Objeto: {objeto_compra[:100]}...")
        
        if not objeto_compra:
            print("   ‚ö†Ô∏è  Objeto da compra vazio, pulando...")
            continue
        
        # üî• CACHE: Usar embedding do cache ou gerar novo
        if objeto_compra in cached_bid_embeddings:
            bid_embedding = cached_bid_embeddings[objeto_compra]
            print(f"   ‚ö° Embedding do cache Redis")
        else:
            bid_embedding = vectorizer.vectorize(objeto_compra)
            if bid_embedding:
                cache_service.save_embedding_to_cache(objeto_compra, bid_embedding)
                print(f"   üÜï Novo embedding gerado")
        
        if not bid_embedding:
            print("   ‚ùå Erro ao vetorizar objeto da compra")
            estatisticas['vetorizacao_falhou'] += 1
            continue
        
        estatisticas['total_processadas'] += 1
        
        # FASE 1: Matching do objeto completo
        potential_matches = []
        for company in companies:
            if not company.get("embedding"):
                continue
            
            score, justificativa = calculate_enhanced_similarity(
                bid_embedding, 
                company["embedding"], 
                objeto_compra, 
                company["descricao_servicos_produtos"]
            )
            
            if score >= SIMILARITY_THRESHOLD_PHASE1:
                # ü§ñ VALIDA√á√ÉO LLM PARA SCORES ALTOS
                should_accept_match = True
                final_score = score
                final_justificativa = justificativa
                
                if llm_validator and llm_validator.should_validate_with_llm(score):
                    validation = llm_validator.validate_match(
                        empresa_nome=company['nome'],
                        empresa_descricao=company['descricao_servicos_produtos'],
                        licitacao_objeto=objeto_compra,
                        pncp_id=pncp_id,
                        similarity_score=score
                    )
                    
                    estatisticas['llm_validations_count'] += 1
                    
                    if validation['is_valid']:
                        final_score = validation['confidence']
                        final_justificativa += f" | LLM: {validation['reasoning'][:100]}..."
                        estatisticas['llm_approved'] += 1
                    else:
                        should_accept_match = False
                        estatisticas['llm_rejected'] += 1
                
                if should_accept_match:
                    potential_matches.append((company, final_score, final_justificativa))
        
        if potential_matches:
            estatisticas['com_matches'] += 1
            
            # Buscar itens da licita√ß√£o
            items = get_bid_items_from_db(bid['id'])
            
            # FASE 2: Refinamento com itens (se dispon√≠vel)
            if items:
                _process_phase2_matching(items, potential_matches, pncp_id, cache_service, vectorizer, estatisticas)
            else:
                _process_phase1_only_matching(potential_matches, pncp_id, estatisticas)
                
            matches_encontrados += len(potential_matches)
        else:
            estatisticas['sem_matches'] += 1
        
        print("-" * 60)
    
    # Relat√≥rio final com estat√≠sticas de cache Redis
    print(f"\nüìä ESTAT√çSTICAS REDIS CACHE:")
    print(f"   ‚ö° Cache hits Redis: {redis_cache_hits}/{len(bid_texts)}")
    
    # Exibir stats do cache
    cache_stats = cache_service.get_cache_stats()
    if cache_stats.get('status') == 'active':
        print(f"   üìà Total embeddings em cache: {cache_stats['cache_stats']['match_keys']}")
        print(f"   üéØ Efici√™ncia do cache: {cache_stats['performance']['cache_efficiency']}")
        print(f"   üíæ Mem√≥ria Redis: {cache_stats['redis_info']['memory_used']}")
    
    result = _print_detailed_final_report(matches_encontrados, estatisticas)
    
    print(f"üöÄ Processo de reavalia√ß√£o finalizado com Redis LOCAL!")
    return result


def _print_final_report(matches_encontrados: int, estatisticas: Dict[str, int]):
    """Imprime relat√≥rio final resumido"""
    print(f"\n" + "="*80)
    print(f"üéâ PROCESSAMENTO CONCLU√çDO!")
    print(f"="*80)
    print(f"üìä ESTAT√çSTICAS:")
    print(f"   üîç Licita√ß√µes processadas: {estatisticas['total_processadas']}")
    print(f"   üéØ Licita√ß√µes com matches: {estatisticas['com_matches']}")
    print(f"   ‚ùå Licita√ß√µes sem matches: {estatisticas['sem_matches']}")
    print(f"   üìã Matches apenas Fase 1: {estatisticas['matches_fase1_apenas']}")
    print(f"   üî¨ Matches com Fase 2: {estatisticas['matches_fase2']}")
    print(f"   üéØ Total de matches: {matches_encontrados}")
    if estatisticas.get('llm_validations_count', 0) > 0:
        print(f"   ü§ñ Valida√ß√µes LLM: {estatisticas['llm_validations_count']}")
        print(f"   ‚úÖ LLM aprovados: {estatisticas['llm_approved']}")
        print(f"   ‚ùå LLM rejeitados: {estatisticas['llm_rejected']}")
    
    if estatisticas['total_processadas'] > 0:
        taxa_sucesso = (estatisticas['com_matches'] / estatisticas['total_processadas']) * 100
        print(f"   üìà Taxa de sucesso: {taxa_sucesso:.1f}%")


def _print_detailed_final_report(matches_encontrados: int, estatisticas: Dict[str, int]) -> Dict[str, Any]:
    """Imprime relat√≥rio final detalhado e retorna resultado"""
    print(f"\n" + "="*80)
    print(f"üéâ REAVALIA√á√ÉO CONCLU√çDA!")
    print(f"="*80)
    print(f"üìä ESTAT√çSTICAS DETALHADAS:")
    print(f"   üîç Licita√ß√µes processadas: {estatisticas['total_processadas']}")
    print(f"   ‚ùå Falhas na vetoriza√ß√£o: {estatisticas['vetorizacao_falhou']}")
    print(f"   üéØ Licita√ß√µes com matches: {estatisticas['com_matches']}")
    print(f"   ‚ùå Licita√ß√µes sem matches: {estatisticas['sem_matches']}")
    print(f"   üìã Matches apenas Fase 1: {estatisticas['matches_fase1_apenas']}")
    print(f"   üî¨ Matches com Fase 2: {estatisticas['matches_fase2']}")
    print(f"   üéØ Total de matches: {matches_encontrados}")
    if estatisticas.get('llm_validations_count', 0) > 0:
        print(f"   ü§ñ Valida√ß√µes LLM: {estatisticas['llm_validations_count']}")
        print(f"   ‚úÖ LLM aprovados: {estatisticas['llm_approved']}")
        print(f"   ‚ùå LLM rejeitados: {estatisticas['llm_rejected']}")
        if estatisticas['llm_validations_count'] > 0:
            taxa_aprovacao_llm = (estatisticas['llm_approved'] / estatisticas['llm_validations_count']) * 100
            print(f"   üìà Taxa aprova√ß√£o LLM: {taxa_aprovacao_llm:.1f}%")
    
    if estatisticas['total_processadas'] > 0:
        taxa_sucesso = (estatisticas['com_matches'] / estatisticas['total_processadas']) * 100
        print(f"   üìà Taxa de sucesso: {taxa_sucesso:.1f}%")
    
    return {
        'matches_encontrados': matches_encontrados,
        'estatisticas': estatisticas
    }


if __name__ == "__main__":
    print("=" * 80)
    print("üáßüá∑ SISTEMA DE MATCHING BRASILEIRO - LICITA√á√ïES PNCP")
    print("=" * 80)
    
    # Menu de configura√ß√£o do vectorizer
    print("\nüîß Escolha o sistema de vetoriza√ß√£o:")
    print("1. üáßüá∑ Sistema Brasileiro (NeralMind BERT) - RECOMENDADO")
    print("2. Sistema H√≠brido (Brasileiro + fallbacks internacionais)")
    print("3. VoyageAI (internacional)")
    print("4. OpenAI Embeddings (internacional)")
    print("5. MockTextVectorizer (apenas para teste)")
    
    vectorizer_choice = input("\nEscolha o vetorizador (1-5, padr√£o: 1): ").strip() or "1"
    
    # Configurar vectorizer
    try:
        if vectorizer_choice == "1":
            print("\nüáßüá∑ Inicializando Sistema Brasileiro...")
            vectorizer = BrazilianTextVectorizer()
        elif vectorizer_choice == "2":
            print("\nüî• Inicializando Sistema H√≠brido (Brasileiro + Internacional)...")
            vectorizer = HybridTextVectorizer()
        elif vectorizer_choice == "3":
            print("\nüö¢ Inicializando VoyageAI...")
            vectorizer = VoyageAITextVectorizer()
        elif vectorizer_choice == "4":
            print("\nüî• Inicializando OpenAI Embeddings...")
            vectorizer = OpenAITextVectorizer()
        elif vectorizer_choice == "5":
            print("\nüß™ Inicializando MockTextVectorizer...")
            vectorizer = MockTextVectorizer()
        else:
            print(f"\n‚ùå Op√ß√£o inv√°lida '{vectorizer_choice}'. Usando Sistema Brasileiro...")
            vectorizer = BrazilianTextVectorizer()
    except Exception as e:
        print(f"\n‚ùå Erro ao inicializar vetorizador: {e}")
        print("üîÑ Tentando fallback para Sistema Brasileiro...")
        try:
            vectorizer = BrazilianTextVectorizer()
        except Exception as e2:
            print(f"üîÑ Fallback final: MockTextVectorizer...")
            try:
                vectorizer = MockTextVectorizer()
            except Exception as e3:
                print(f"‚ùå Erro cr√≠tico: {e3}")
                exit(1)
    
    # Menu de opera√ß√µes
    print("\nüìã Opera√ß√µes dispon√≠veis:")
    print("1. Buscar novas licita√ß√µes do PNCP (process_daily_bids)")
    print("2. Reavaliar licita√ß√µes existentes no banco (reevaluate_existing_bids)")
    print("3. Teste r√°pido de vetoriza√ß√£o")
    print("4. Limpar cache Redis")
    
    opcao = input("\nEscolha uma opera√ß√£o (1-4, padr√£o: 2): ").strip() or "2"
    
    try:
        if opcao == "1":
            print("\nüåê Executando busca de novas licita√ß√µes...")
            process_daily_bids(vectorizer)
        elif opcao == "2":
            print("\nüîÑ Executando reavalia√ß√£o de licita√ß√µes existentes...")
            result = reevaluate_existing_bids(vectorizer, clear_matches=True)
            
            if result:
                print(f"\nüìà RESULTADO FINAL:")
                print(f"   üéØ Matches encontrados: {result['matches_encontrados']}")
                print(f"   üìä Taxa de sucesso: {result['estatisticas']['com_matches']/result['estatisticas']['total_processadas']*100 if result['estatisticas']['total_processadas'] > 0 else 0:.1f}%")
        elif opcao == "3":
            print("\nüß™ Executando teste r√°pido de vetoriza√ß√£o...")
            
            test_texts = [
                "Contrata√ß√£o de servi√ßos de tecnologia da informa√ß√£o",
                "Aquisi√ß√£o de equipamentos de inform√°tica e suprimentos",
                "Servi√ßos de manuten√ß√£o de impressoras e equipamentos",
                "Fornecimento de papel A4 e material de escrit√≥rio"
            ]
            
            print("   üìù Textos de teste:")
            for i, text in enumerate(test_texts, 1):
                print(f"      {i}. {text}")
            
            print("\n   üîÑ Vetorizando...")
            embeddings = vectorizer.batch_vectorize(test_texts)
            
            if embeddings:
                print(f"   ‚úÖ Sucesso! {len(embeddings)} embeddings gerados")
                print(f"   üìè Dimens√µes: {len(embeddings[0])} cada")
                
                # Teste de cache
                cache_service = EmbeddingCacheService(db_manager)
                cache_service.batch_save_embeddings_to_cache(list(zip(test_texts, embeddings)))
                print(f"   üíæ Embeddings salvos no cache Redis")
                
                # Testar recupera√ß√£o do cache
                cached = cache_service.batch_get_embeddings_from_cache(test_texts)
                print(f"   ‚ö° Cache test: {len(cached)}/{len(test_texts)} recuperados")
            else:
                print("   ‚ùå Falha na vetoriza√ß√£o")
        elif opcao == "4":
            print("\nüßπ Limpando cache Redis...")
            cache_service = EmbeddingCacheService(db_manager)
            deleted = cache_service.clear_cache()
            print(f"   üóëÔ∏è {deleted} entradas removidas do cache")
        else:
            print("‚ùå Op√ß√£o inv√°lida. Executando reavalia√ß√£o por padr√£o...")
            reevaluate_existing_bids(vectorizer, clear_matches=True)
        
        print(f"\n‚úÖ Processo finalizado com sucesso!")
        
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è  Processo interrompido pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro durante execu√ß√£o: {e}")
        import traceback
        traceback.print_exc() 