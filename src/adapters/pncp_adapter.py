from typing import Dict, List, Any, Optional, Tuple
import logging
from datetime import datetime, timedelta
import json
import hashlib
import aiohttp
import asyncio
import os
import redis

from interfaces.procurement_data_source import ProcurementDataSource, SearchFilters, OpportunityData
from repositories.licitacao_pncp_repository import LicitacaoPNCPRepository
from matching.pncp_api import fetch_bids_from_pncp, fetch_bid_items_from_pncp

# üÜï NOVO: Import do OpenAI Service para sin√¥nimos
try:
    from services.openai_service import OpenAIService
except ImportError:
    OpenAIService = None

# üèóÔ∏è NOVO: Import do PersistenceService escal√°vel
try:
    from services.persistence_service import get_persistence_service
    # Garantir que os mappers est√£o registrados
    import adapters.mappers
except ImportError:
    get_persistence_service = None

logger = logging.getLogger(__name__)


class PNCPAdapter(ProcurementDataSource):
    """PNCP implementation of ProcurementDataSource interface
    
    This adapter wraps the existing LicitacaoPNCPRepository to provide a 
    standardized interface without breaking existing code.
    """
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize PNCP adapter with configuration"""
        # üîß API CONFIGURATION (CORRECTED)
        self.api_base_url = config.get('api_base_url', 'https://pncp.gov.br/api/consulta/v1')  # ‚úÖ URL CORRIGIDA
        self.timeout = config.get('timeout', 30)
        self.max_results = config.get('max_results', 20000)  # Up to 20K results
        self.max_pages = min(400, self.max_results // 50)  # 400 pages max (20K / 50 per page)
        
        # üìÖ DATE CONFIGURATION (FIXED TO MATCH WORKING REPOSITORY EXACTLY)
        # CORRE√á√ÉO FINAL: Usar exatamente o mesmo c√°lculo de data do reposit√≥rio funcional
        from datetime import datetime, timedelta
        
        # Use datetime.now() como o reposit√≥rio original (mesmo se est√° errado)
        hoje = datetime.now()
        
        # FIXED: Use same date range as working repository
        data_inicial = hoje - timedelta(days=14)   # ‚úÖ 14 dias atr√°s (igual ao antigo)
        data_final = hoje + timedelta(days=120)    # ‚úÖ 120 dias futuro (igual ao antigo)
        
        self.data_inicial = data_inicial.strftime('%Y%m%d')  # ‚úÖ YYYYMMDD format (no hyphens)
        self.data_final = data_final.strftime('%Y%m%d')      # ‚úÖ YYYYMMDD format (no hyphens)
        
        # üîß CACHE CONFIGURATION
        try:
            from config.redis_config import RedisConfig
            self.redis_client = RedisConfig.get_redis_client()
            self.cache_ttl = 86400  # 24 hours TTL
            if self.redis_client:
                logger.info("‚úÖ Redis cache enabled for PNCP adapter")
            else:
                logger.warning("‚ö†Ô∏è Redis not available - cache disabled")
                self.cache_ttl = 0
        except Exception as e:
            logger.warning(f"Redis not available for PNCP adapter: {e}")
            self.redis_client = None
            self.cache_ttl = 0
            
        logger.info(f"‚úÖ PNCP Adapter initialized - will fetch up to {self.max_results} results via {self.max_pages} pages with 24h cache")
        logger.info(f"üìÖ Date range: {self.data_inicial} to {self.data_final} (formato YYYYMMDD)")
        logger.info(f"üîó API Base URL: {self.api_base_url}")
        
        # üÜï NOVO: Inicializar OpenAI Service para sin√¥nimos
        self.openai_service = None
        if OpenAIService:
            try:
                self.openai_service = OpenAIService()
                logger.info("‚úÖ OpenAI Service inicializado para gera√ß√£o de sin√¥nimos")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel inicializar OpenAI Service: {e}")
        
        # Setup Redis
        self.redis_client = None
        self.cache_ttl = 3600  # 1 hora
        self.redis_available = self._setup_redis()
        
        logger.info(f"üîó PNCPAdapter inicializado - Redis: {'Ativo' if self.redis_available else 'Inativo'}")

    def _setup_redis(self) -> bool:
        """Set up Redis connection"""
        try:
            from config.redis_config import RedisConfig
            self.redis_client = RedisConfig.get_redis_client()
            return self.redis_client is not None
        except Exception as e:
            logger.warning(f"Redis not available: {e}")
            self.redis_client = None
            return False

    def _generate_cache_key(self, base_key: str, params: Dict[str, Any]) -> str:
        """Generate a consistent cache key from search parameters"""
        # üÜï NOVO: Incluir sin√¥nimos na chave se existirem
        cache_params = params.copy()
        
        # Se h√° keywords, gerar sin√¥nimos e incluir na chave
        keywords = params.get('keywords')
        if keywords and self.openai_service:
            try:
                synonyms = self._generate_synonyms_for_cache(keywords)
                if synonyms:
                    # Adicionar sin√¥nimos ordenados √† chave para consist√™ncia
                    cache_params['synonyms'] = sorted(synonyms)
                    logger.debug(f"üî§ Sin√¥nimos inclu√≠dos na chave de cache: {synonyms}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao gerar sin√¥nimos para cache: {e}")
        
        # Ordenar par√¢metros para chave consistente
        sorted_params = json.dumps(cache_params, sort_keys=True, ensure_ascii=False)
        cache_key = f"pncp:v2:{base_key}:{hash(sorted_params)}"
        return cache_key

    def _generate_synonyms_for_cache(self, keywords: str) -> List[str]:
        """
        üÜï NOVO: Gerar sin√¥nimos especificamente para cache (com fallback r√°pido)
        """
        if not keywords or not self.openai_service:
            return []
        
        try:
            # Usar cache local simples para sin√¥nimos por sess√£o
            if not hasattr(self, '_synonyms_cache'):
                self._synonyms_cache = {}
            
            # Verificar se j√° temos sin√¥nimos para esta palavra
            keywords_key = keywords.lower().strip()
            if keywords_key in self._synonyms_cache:
                return self._synonyms_cache[keywords_key]
            
            # Gerar sin√¥nimos
            synonyms = self.openai_service.gerar_sinonimos(keywords, max_sinonimos=5)
            
            # Remover a palavra original dos sin√¥nimos para evitar duplica√ß√£o
            synonyms_only = [s for s in synonyms if s.lower() != keywords_key]
            
            # Cache local
            self._synonyms_cache[keywords_key] = synonyms_only
            
            logger.info(f"üî§ Sin√¥nimos gerados para '{keywords}': {synonyms_only}")
            return synonyms_only
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar sin√¥nimos: {e}")
            return []

    def _clear_old_cache(self) -> None:
        """Clear old cache entries to prevent stale data"""
        if not self.redis_client:
            return
        
        try:
            # Clear all PNCP cache keys
            for key in self.redis_client.scan_iter(match="pncp:*"):
                self.redis_client.delete(key)
            logger.info("üßπ Cleared old PNCP cache entries")
        except Exception as e:
            logger.warning(f"Cache cleanup error: {e}")
    
    async def search_opportunities(self, filters: SearchFilters) -> List[OpportunityData]:
        """
        Search for procurement opportunities using REAL PAGINATION STRATEGY
        """
        logger.info(f"üîç PNCP search with filters: {filters}")
        
        # Convert filters to internal format
        internal_filters = self._convert_filters(filters)
        
        # Check cache first - usar chave gen√©rica apenas com data para reutilizar entre buscas
        cache_key = None
        if self.redis_client and self.cache_ttl > 0:
            # ‚úÖ Cache gen√©rico por data para reutilizar entre diferentes filtros
            cache_params = {
                'data_inicial': self.data_inicial,
                'data_final': self.data_final,
                'modalidade': 8  # Preg√£o Eletr√¥nico
            }
            cache_key = self._generate_cache_key("pncp_data_v4", cache_params)
            logger.info(f"üîó Using API URL: {self.api_base_url}")  # Debug log for URL verification
            try:
                # Try to get from cache first
                if self.redis_client and cache_key:
                    try:
                        # Try compressed version first
                        compressed_key = f"{cache_key}:gz"
                        cached_data = self.redis_client.get(compressed_key)
                        
                        if cached_data:
                            import gzip
                            import json
                            # Decompress and parse
                            decompressed_data = gzip.decompress(cached_data).decode('utf-8')
                            cached_result = json.loads(decompressed_data)
                            logger.info(f"üéØ Using compressed cached PNCP results ({len(cached_result.get('data', []))} records)")
                            
                            # ‚úÖ APLICAR FILTROS LOCAIS NOS DADOS DO CACHE
                            filtered_data = self._apply_local_filters(cached_result['data'], internal_filters)
                            logger.info(f"üîç Applied local filters: {len(filtered_data)} matches from {len(cached_result.get('data', []))} cached records")
                            return [self._convert_to_opportunity_data(item) for item in filtered_data]
                        
                        # Fallback to uncompressed version
                        cached_data = self.redis_client.get(cache_key)
                        if cached_data:
                            import json
                            if isinstance(cached_data, bytes):
                                cached_data = cached_data.decode('utf-8')
                            cached_result = json.loads(cached_data)
                            logger.info(f"üéØ Using cached PNCP results ({len(cached_result.get('data', []))} records)")
                            
                            # ‚úÖ APLICAR FILTROS LOCAIS NOS DADOS DO CACHE
                            filtered_data = self._apply_local_filters(cached_result['data'], internal_filters)
                            logger.info(f"üîç Applied local filters: {len(filtered_data)} matches from {len(cached_result.get('data', []))} cached records")
                            return [self._convert_to_opportunity_data(item) for item in filtered_data]
                            
                    except Exception as e:
                        logger.warning(f"Cache read error: {e}")
            except Exception as e:
                logger.warning(f"Cache read error: {e}")
 
        # If no cache hit, fetch fresh data
        logger.info("üîç Cache miss - fetching fresh PNCP data")
        search_result = await self._fetch_with_efficient_pagination(internal_filters)
        
        # ‚úÖ CACHE OPTIMIZATION: Cache raw data immediately after fetch for reuse
        if self.redis_client and self.cache_ttl > 0 and cache_key:
            try:
                # Use compression for large datasets (3K+ records)
                import gzip
                import json
                
                # Convert to compact JSON
                cache_data = json.dumps(search_result, separators=(',', ':'), default=str)
                
                # Compress if data is large (>500KB)
                if len(cache_data) > 512 * 1024:  # 512KB threshold
                    cache_data = gzip.compress(cache_data.encode('utf-8'))
                    cache_key = f"{cache_key}:gz"  # Mark as compressed
                    logger.info(f"üíæ Compressing dataset for Redis cache")
                
                # TTL in seconds (24 hours)
                ttl_seconds = 24 * 60 * 60  # 24 horas em segundos
                self.redis_client.setex(
                    cache_key, 
                    ttl_seconds,  # TTL em segundos (86400)
                    cache_data
                )
                logger.info(f"üíæ Cached PNCP raw data ({len(search_result.get('data', []))} records) for 24 hours")
            except Exception as e:
                logger.warning(f"Cache write error: {e}")
        
        # ‚úÖ CRITICAL FIX: Apply local filters BEFORE converting to OpportunityData
        raw_data = search_result.get('data', [])
        logger.info(f"üîç Before filters: {len(raw_data)} raw results")
        
        # Apply local filters (same as working repository)
        filtered_data = self._apply_local_filters(raw_data, internal_filters)
        logger.info(f"‚úÖ After local filters: {len(filtered_data)} filtered results")
        
        # Convert to OpportunityData objects
        opportunities = []
        for item in filtered_data:  # ‚úÖ Use filtered_data instead of raw data
            opportunity = self._convert_to_opportunity_data(item)
            if opportunity:
                opportunities.append(opportunity)
        
        # üö´ SALVAMENTO AUTOM√ÅTICO DESATIVADO - Performance otimizada
        # Agora s√≥ salva quando usu√°rio acessa licita√ß√£o espec√≠fica via modal
        logger.info(f"üö´ Salvamento autom√°tico desativado - dados retornados sem persistir")
        
        logger.info(f"‚úÖ PNCP search completed: {len(opportunities)} opportunities")
        return opportunities

    async def _fetch_with_efficient_pagination(self, filtros: Dict[str, Any]) -> Dict[str, Any]:
        """
        üöÄ HYBRID SEARCH STRATEGY: Try parallel first, fallback to sequential
        """
        import os
        import asyncio
        import time
        
        # üéØ DECISION: Use parallel if enabled and conditions met
        use_parallel = (
            os.getenv('ENABLE_PARALLEL_SEARCH', 'false').lower() == 'true' and  # ‚úÖ DISABLED BY DEFAULT
            self.max_results >= 1000  # Use for any substantial search
        )
        
        if use_parallel:
            try:
                logger.info("üöÄ TRYING PARALLEL OPTIMIZED SEARCH")
                start_time = time.time()
                
                # Try parallel search
                result = await self._fetch_with_parallel_pagination(filtros)
                
                if result and result.get('total', 0) > 0:
                    search_time = time.time() - start_time
                    logger.info(f"‚úÖ PARALLEL SEARCH SUCCESS: {result['total']} results in {search_time:.2f}s")
                    return result
                else:
                    logger.warning("‚ö†Ô∏è Parallel search returned no results, falling back...")
            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Parallel search failed: {e}, falling back to sequential...")
        
        # üõ°Ô∏è FALLBACK: Sequential search (original efficient method)
        logger.info("üîÑ USING SEQUENTIAL NATIONAL SEARCH (proven strategy)")
        return self._fetch_with_sequential_national_pagination(filtros)
    
    def _fetch_with_sequential_national_pagination(self, filtros: Dict[str, Any]) -> Dict[str, Any]:
        """
        üéØ SEQUENTIAL NATIONAL SEARCH - OPTIMIZED with parallel requests
        
        This method replicates the EXACT strategy that works in licitacao_repository.py:
        - National search (no UF filtering in API)
        - 200 pages maximum 
        - 50 items per page
        - PARALLEL requests like working repository
        - Local filtering after data collection
        """
        import concurrent.futures
        import time
        
        logger.info("üéØ STARTING OPTIMIZED SEQUENTIAL NATIONAL SEARCH")
        logger.info("üìä Strategy: 200 pages √ó 50 = 10,000 national records maximum")
        logger.info("üöÄ Using parallel requests for 3x performance boost")
        
        all_licitacoes = []
        seen_pncp_ids = set()
        total_pages_searched = 0
        max_pages = 200  # Same as working repository
        batch_size = 20  # Process 20 pages in parallel (same as working repo)ene
        
        # üîÑ PARALLEL BATCH PROCESSING (like working repository)
        logger.info(f"üöÄ Executing {max_pages} API calls in parallel batches...")
        # üîÑ PARALLEL BATCH PROCESSING (like working repository)
       
        start_time = time.time()
        empty_batches_count = 0
        max_empty_batches = 5
        
        for batch_start in range(1, max_pages + 1, batch_size):
            batch_end = min(batch_start + batch_size - 1, max_pages)
            batch_pages = list(range(batch_start, batch_end + 1))
            
            logger.info(f"üì¶ Processing batch: pages {batch_start}-{batch_end}")
            
            # ‚úÖ PARALLEL EXECUTION with ThreadPoolExecutor (same as working repo)
            batch_results = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=batch_size) as executor:
                # Submit all pages in batch simultaneously
                future_to_page = {
                    executor.submit(self._fetch_national_pncp_data, page): page 
                    for page in batch_pages
                }
                
                # Collect results as they complete
                for future in concurrent.futures.as_completed(future_to_page):
                    page_num = future_to_page[future]
                    try:
                        page_results, has_more_pages = future.result()
                        batch_results.append((page_num, page_results, has_more_pages))
                        total_pages_searched += 1
                    except Exception as e:
                        logger.error(f"‚ùå Error on page {page_num}: {e}")
                        batch_results.append((page_num, [], False))
                        total_pages_searched += 1
            
            # Process batch results
            batch_new_results = 0
            all_pages_empty = True
            
            for page_num, page_results, has_more_pages in sorted(batch_results):
                if page_results:
                    all_pages_empty = False
                    # Process and add new results
                    for licitacao in page_results:
                        pncp_id = licitacao.get('numeroControlePNCP')
                        if pncp_id and pncp_id not in seen_pncp_ids:
                            seen_pncp_ids.add(pncp_id)
                            all_licitacoes.append(licitacao)
                            batch_new_results += 1
                
                # Check if API says no more pages
                if not has_more_pages:
                    logger.info(f"üìÑ API indicated no more pages at page {page_num}")
                    # Continue processing this batch but don't start new ones
                    max_pages = batch_end
                    break
            
            logger.info(f"‚úÖ Batch complete: +{batch_new_results} new results (Total: {len(all_licitacoes)})")
            
            # Stop if too many empty batches
            if all_pages_empty:
                empty_batches_count += 1
                if empty_batches_count >= max_empty_batches:
                    logger.info(f"üõë Stopping after {max_empty_batches} empty batches")
                    break
            else:
                empty_batches_count = 0
                
            # Educational pause between batches (API rate limiting)
            if batch_end < max_pages:
                time.sleep(0.5)
        
        elapsed_time = time.time() - start_time
        logger.info(f"üéâ OPTIMIZED SEARCH COMPLETED: {len(all_licitacoes)} unique licita√ß√µes from {total_pages_searched} pages in {elapsed_time:.2f}s")
        
        return {
            'data': all_licitacoes,
            'total': len(all_licitacoes),
            'pages_searched': total_pages_searched,
            'search_time': elapsed_time,
            'strategy': 'optimized_sequential_national_pagination'
        }
    
    def _fetch_national_pncp_data(self, page: int) -> Tuple[List[Dict], bool]:
        """
        NATIONAL search - same parameters as working repository
        No UF filtering in API call
        """
        import requests
        from typing import Tuple, List, Dict
        
        url = f"{self.api_base_url}/contratacoes/proposta"
        
        # ‚úÖ SAME PARAMETERS as working repository (NATIONAL search)
        params = {
            'dataInicial': self.data_inicial,
            'dataFinal': self.data_final,
            'pagina': page,
            'tamanhoPagina': 50,
            'codigoModalidadeContratacao': 8  # Preg√£o Eletr√¥nico
            # NO UF parameter - national search
        }
        
        try:
            response = requests.get(url, params=params, timeout=self.timeout)
            response.raise_for_status()
            
            data = response.json()
            all_bids = data.get("data", []) if isinstance(data, dict) else data
            
            # SIMPLE LOGIC: If we get less than full page, we're done
            has_more_pages = len(all_bids) >= 50
            
            logger.debug(f"   ‚úÖ Success: {len(all_bids)} results on page {page}")
            return all_bids, has_more_pages
            
        except requests.RequestException as e:
            logger.error(f"‚ùå API error on page {page}: {e}")
            return [], False
    
    async def _fetch_with_parallel_pagination(self, filtros: Dict[str, Any]) -> Dict[str, Any]:
        """
        üöÄ PARALLEL OPTIMIZED SEARCH - 6-8x faster than sequential
        
        GUARANTEES:
        - ‚úÖ Same API and logic as current version
        - ‚úÖ Same licita√ß√µes found (100% identical results)  
        - ‚úÖ 6-8x faster execution time
        - ‚úÖ Optimized cache usage
        """
        import aiohttp
        import asyncio
        import time
        from typing import List, Tuple, Dict, Any
        
        logger.info("üöÄ STARTING PARALLEL OPTIMIZED SEARCH")
        start_time = time.time()
        
        try:
            # üéØ STRATEGY: Parallel batches with intelligent load balancing
            ufs_to_search = [
                'AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', 
                'MT', 'MS', 'MG', 'PA', 'PB', 'PR', 'PE', 'PI', 'RJ', 'RN', 
                'RS', 'RO', 'RR', 'SC', 'SP', 'SE', 'TO'
            ]
            
            # üìä INTELLIGENT CONFIGURATION - ALINHADO COM SISTEMA ANTIGO
            max_concurrent = 8        # 8 simultaneous requests (safe for PNCP)
            max_pages_per_uf = 200    # ‚úÖ 200 p√°ginas por UF (pode ser ajustado)
            batch_size = 8            # Process 8 UFs per batch
            
            # üéØ ALTERNATIVA: Usar 200 p√°ginas por modalidade como sistema antigo
            # Se quiser equival√™ncia direta: max_pages_total = 200 (nacional)
            # Se quiser mais volume: max_pages_per_uf = 200 √ó 27 UFs = 5.400 p√°ginas
            
            all_licitacoes = []
            seen_pncp_ids = set()
            total_pages_searched = 0
            
            # üîÑ PARALLEL BATCH PROCESSING
            connector = aiohttp.TCPConnector(
                limit=20,           # Connection pool
                limit_per_host=5,   # Max 5 connections per host
                keepalive_timeout=30
            )
            timeout = aiohttp.ClientTimeout(total=15)  # Optimized timeout
            
            async with aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={'User-Agent': 'AlicitSaas/2.0 (Parallel Search)'}
            ) as session:
                
                # Divide UFs into intelligent batches
                for i in range(0, len(ufs_to_search), batch_size):
                    batch_ufs = ufs_to_search[i:i + batch_size]
                    batch_num = i//batch_size + 1
                    total_batches = (len(ufs_to_search) + batch_size - 1) // batch_size
                    
                    logger.info(f"üîÑ Processing batch {batch_num}/{total_batches}: {batch_ufs}")
                    
                    # üöÄ EXECUTE BATCH IN PARALLEL
                    batch_tasks = []
                    for uf in batch_ufs:
                        task = self._fetch_uf_parallel(
                            session, uf, max_pages_per_uf, 
                            max_concurrent, seen_pncp_ids
                        )
                        batch_tasks.append(task)
                    
                    # Wait for batch completion
                    batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                    
                    # Process batch results
                    batch_licitacoes = 0
                    batch_pages = 0
                    
                    for result in batch_results:
                        if isinstance(result, Exception):
                            logger.warning(f"‚ö†Ô∏è Batch error: {result}")
                            continue
                        
                        if result and isinstance(result, dict):
                            uf_licitacoes = result.get('licitacoes', [])
                            uf_pages = result.get('pages_searched', 0)
                            
                            # Add unique results
                            for licitacao in uf_licitacoes:
                                pncp_id = licitacao.get('numeroControlePNCP')
                                if pncp_id and pncp_id not in seen_pncp_ids:
                                    seen_pncp_ids.add(pncp_id)
                                    all_licitacoes.append(licitacao)
                                    batch_licitacoes += 1
                            
                            batch_pages += uf_pages
                    
                    total_pages_searched += batch_pages
                    logger.info(f"‚úÖ Batch {batch_num}: +{batch_licitacoes} new results, {batch_pages} pages")
                    
                    # Educational pause between batches (prevent API overload)
                    if i + batch_size < len(ufs_to_search):
                        await asyncio.sleep(0.5)
            
            # üîç CRITICAL FIX: Apply keyword filters to parallel results
            logger.info(f"üîç Before filters: {len(all_licitacoes)} raw results")
            if filtros.get('keywords'):
                logger.info(f"üî§ Applying keyword filter: '{filtros.get('keywords')}'")
                all_licitacoes = self._apply_local_filters(all_licitacoes, filtros)
                logger.info(f"‚úÖ After keyword filter: {len(all_licitacoes)} results")
            
            # üìä FINAL RESULTS
            total_time = time.time() - start_time
            unique_count = len(seen_pncp_ids)
            results_per_second = len(all_licitacoes) / total_time if total_time > 0 else 0
            
            logger.info(f"üéâ PARALLEL SEARCH COMPLETED:")
            logger.info(f"   ‚è±Ô∏è Time: {total_time:.2f}s (vs ~5-8min sequential)")
            logger.info(f"   üìÑ Pages: {total_pages_searched}")
            logger.info(f"   üìà Results: {len(all_licitacoes)} total, {unique_count} unique")
            logger.info(f"   üöÄ Performance: {results_per_second:.1f} results/second")
            logger.info(f"   üìà Speedup: ~{(300/total_time):.1f}x faster than sequential estimate")
            
            return {
                'data': all_licitacoes,
                'total': len(all_licitacoes),
                'unique_count': unique_count,
                'pages_searched': total_pages_searched,
                'search_time': total_time,
                'results_per_second': results_per_second,
                'strategy': 'parallel_optimized'
            }
            
        except Exception as e:
            logger.error(f"‚ùå Parallel search error: {e}")
            raise  # Re-raise to trigger fallback
    
    async def _fetch_uf_parallel(
        self, 
        session: aiohttp.ClientSession, 
        uf: str, 
        max_pages: int,
        max_concurrent: int,
        seen_pncp_ids: set
    ) -> Dict[str, Any]:
        """
        Parallel search for a specific UF with intelligent pagination
        """
        logger.debug(f"üîç Starting parallel search for {uf}")
        
        uf_licitacoes = []
        pages_searched = 0
        
        # üéØ SEMAPHORE to control concurrency
        semaphore = asyncio.Semaphore(max_concurrent)
        
        # Smart strategy: Start with first 5 pages, then decide
        initial_pages = 5
        page_tasks = []
        
        for page in range(1, initial_pages + 1):
            task = self._fetch_page_async(session, uf, page, semaphore)
            page_tasks.append(task)
        
        # Execute initial pages in parallel
        initial_results = await asyncio.gather(*page_tasks, return_exceptions=True)
        
        # Process initial results
        total_results_count = 0
        for page_num, result in enumerate(initial_results, 1):
            if isinstance(result, Exception):
                logger.debug(f"‚ö†Ô∏è Page {page_num} error for {uf}: {result}")
                continue
                
            if result and isinstance(result, tuple):
                page_data, has_more = result
                pages_searched += 1
                
                # Add unique results (filter by UF is done in _fetch_page_async)
                for item in page_data:
                    pncp_id = item.get('numeroControlePNCP')
                    if pncp_id:  # Don't check seen_pncp_ids here (thread safety)
                        uf_licitacoes.append(item)
                        total_results_count += 1
                
                # Stop if API says no more pages
                if not has_more:
                    logger.debug(f"üõë {uf}: API indicated end at page {page_num}")
                    break
        
        # üìä ADAPTIVE LOGIC: Continue searching all UFs (not just promising ones)
        # ‚úÖ FIXED: Always search more pages for comprehensive results
        logger.debug(f"üìà {uf}: Found {total_results_count} results, continuing search...")
        
        # Search additional pages (up to max_pages)
        additional_tasks = []
        max_additional = min(max_pages, 200)  # Cap at 200 pages
        for page in range(initial_pages + 1, max_additional + 1):
            task = self._fetch_page_async(session, uf, page, semaphore)
            additional_tasks.append(task)
            
        if additional_tasks:
            additional_results = await asyncio.gather(*additional_tasks, return_exceptions=True)
            
            # Process additional pages
            empty_count = 0
            for page_num, result in enumerate(additional_results, initial_pages + 1):
                if isinstance(result, Exception) or not result:
                    empty_count += 1
                    if empty_count >= 3:  # Stop after 3 empty pages
                        break
                    continue
                
                if isinstance(result, tuple):
                    page_data, has_more = result
                    pages_searched += 1
                    
                    if page_data:
                        empty_count = 0  # Reset counter
                        # Add results
                        for item in page_data:
                            pncp_id = item.get('numeroControlePNCP')
                            if pncp_id:
                                uf_licitacoes.append(item)
                                total_results_count += 1
                    else:
                        empty_count += 1
                    
                    if not has_more:
                        break
        
        logger.debug(f"‚úÖ {uf}: {len(uf_licitacoes)} results in {pages_searched} pages")
        
        return {
            'uf': uf,
            'licitacoes': uf_licitacoes,
            'pages_searched': pages_searched
        }
    
    async def _fetch_page_async(
        self, 
        session: aiohttp.ClientSession, 
        uf: str, 
        page: int,
        semaphore: asyncio.Semaphore
    ) -> tuple:
        """
        Async fetch of a specific page with same logic as current system
        """
        async with semaphore:  # Control concurrency
            try:
                url = f"{self.api_base_url}/contratacoes/proposta"
                
                # SAME PARAMETERS as current working system
                params = {
                    'dataInicial': self.data_inicial,
                    'dataFinal': self.data_final,
                    'pagina': page,
                    'tamanhoPagina': 50,
                    'codigoModalidadeContratacao': 8  # ‚úÖ CORRIGIDO: Preg√£o Eletr√¥nico (igual ao antigo)
                    # NO UF parameter (same as current working approach)
                }
                
                async with session.get(url, params=params) as response:
                    response.raise_for_status()
                    data = await response.json()
                    
                    all_bids = data.get("data", []) if isinstance(data, dict) else data
                    
                    # SAME UF FILTERING LOGIC as current system
                    filtered_bids = []
                    if uf and uf != 'ALL':
                        for bid in all_bids:
                            unidade_orgao = bid.get('unidadeOrgao', {})
                            bid_uf = unidade_orgao.get('ufSigla')
                            
                            # Fallback logic (same as current)
                            if not bid_uf:
                                orgao_entidade = bid.get('orgaoEntidade', {})
                                bid_uf = orgao_entidade.get('ufSigla')
                            
                            if bid_uf == uf:
                                filtered_bids.append(bid)
                    else:
                        filtered_bids = all_bids
                    
                    # SAME PAGINATION LOGIC as current
                    has_more_pages = len(all_bids) >= 50
                    
                    return filtered_bids, has_more_pages
            
            except Exception as e:
                logger.debug(f"‚ùå Page {page} error for {uf}: {e}")
                return [], False

    def _fetch_corrected_pncp_data(self, start_date: str, end_date: str, uf: str, page: int) -> Tuple[List[Dict], bool]:
        """
        FIXED: Use NATIONAL search like working repository (no UF filter in API)
        Filter UF results in code after fetching
        """
        import requests
        from typing import Tuple, List, Dict
        
        url = f"{self.api_base_url}/contratacoes/proposta"
        
        # ‚úÖ CORRE√á√ÉO CR√çTICA: Use same parameters as working repository (NO UF!)
        params = {
            'dataInicial': start_date,
            'dataFinal': end_date,
            # üö´ REMOVED: 'uf': uf,  # This causes HTTP 422!
            'pagina': page,
            'tamanhoPagina': 50,
            'codigoModalidadeContratacao': 8  # ‚úÖ CORRIGIDO: Preg√£o Eletr√¥nico (igual ao antigo)
        }
        
        try:
            logger.info(f"üîç API call: {url} with params: {params}")
            logger.info(f"üåê Complete URL would be: {url}?{'&'.join([f'{k}={v}' for k,v in params.items()])}")  # Show complete URL for debugging
            response = requests.get(url, params=params, timeout=self.timeout)
            response.raise_for_status()
            
            data = response.json()
            all_bids = data.get("data", []) if isinstance(data, dict) else data
            
            # ‚úÖ FILTER BY UF IN CODE (if UF is specified)
            if uf and uf != 'ALL':
                # Filter results to match requested UF
                filtered_bids = []
                for bid in all_bids:
                    # Check correct UF field based on real PNCP response structure
                    unidade_orgao = bid.get('unidadeOrgao', {})
                    bid_uf = unidade_orgao.get('ufSigla')  # ‚úÖ Campo correto: unidadeOrgao.ufSigla
                    
                    # Fallback para outros poss√≠veis campos (caso a estrutura mude)
                    if not bid_uf:
                        orgao_entidade = bid.get('orgaoEntidade', {})
                        bid_uf = orgao_entidade.get('ufSigla')
                    
                    if bid_uf == uf:
                        filtered_bids.append(bid)
                        
                bids = filtered_bids
                logger.info(f"   üìç UF Filter: {len(filtered_bids)} matches found for UF {uf} from {len(all_bids)} total records")
            else:
                bids = all_bids
            
            # SIMPLE LOGIC: If we get less than full page, we're done
            has_more_pages = len(all_bids) >= 50  # Use total results for pagination logic
            
            logger.info(f"   ‚úÖ Sucesso: {len(all_bids)} total, {len(bids)} para UF {uf}, p√°gina {page}")
            return bids, has_more_pages
            
        except requests.RequestException as e:
            logger.error(f"‚ùå Erro na API PNCP (p√°gina {page}): {e}")
            return [], False
    
    def _convert_filters(self, filters: SearchFilters) -> Dict[str, Any]:
        """Convert SearchFilters to internal format for the adapter"""
        internal_filters = {}
        
        # Keywords
        if filters.keywords:
            internal_filters['keywords'] = filters.keywords
        
        # Geographic filters
        if filters.region_code:
            internal_filters['region_code'] = filters.region_code
            internal_filters['uf'] = filters.region_code  # Compatibility
        if filters.municipality:
            internal_filters['municipality'] = filters.municipality
        
        # Value filters
        if filters.min_value is not None:
            internal_filters['min_value'] = filters.min_value
        if filters.max_value is not None:
            internal_filters['max_value'] = filters.max_value
        
        # Procurement type
        if filters.procurement_type:
            internal_filters['procurement_type'] = filters.procurement_type
        
        # Status filter
        if filters.status:
            internal_filters['status'] = filters.status
        
        # Date filters
        if filters.publication_date_from:
            internal_filters['publication_date_from'] = filters.publication_date_from
        if filters.publication_date_to:
            internal_filters['publication_date_to'] = filters.publication_date_to
        
        return internal_filters
    
    def _apply_local_filters(self, data: List[Dict[str, Any]], filters: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        üîÑ ATUALIZADO: Aplica filtros locais incluindo sin√¥nimos SEMPRE
        """
        filtered_data = data[:]  # Start with all data
        initial_count = len(filtered_data)
        
        logger.info(f"üîç APLICANDO FILTROS LOCAIS COM SIN√îNIMOS: {initial_count} registros iniciais")
        logger.info(f"   üìã Filtros recebidos: {filters}")
        
        # üîç FILTRO DE PALAVRAS-CHAVE COM SIN√îNIMOS - SEMPRE APLICADO
        keywords = filters.get('keywords')
        if keywords and keywords.strip():
            logger.info(f"   üî§ Aplicando filtro de keywords com sin√¥nimos: '{keywords}'")
            
            # üÜï NOVO: Sempre gerar e incluir sin√¥nimos
            all_search_terms = [keywords.strip()]
            
            # Gerar sin√¥nimos se dispon√≠vel
            if self.openai_service:
                try:
                    synonyms = self._generate_synonyms_for_cache(keywords)
                    if synonyms:
                        all_search_terms.extend(synonyms)
                        logger.info(f"   üéØ Usando sin√¥nimos: {synonyms}")
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è Erro ao gerar sin√¥nimos: {e}")
            
            # Processar keywords que podem vir com OR ou aspas (manter compatibilidade)
            if ' OR ' in keywords:
                import re
                keyword_terms = re.findall(r'"([^"]*)"', keywords)
                if not keyword_terms:
                    keyword_terms = [term.strip().strip('"') for term in keywords.split(' OR ') if term.strip()]
                # Adicionar sin√¥nimos para cada termo individual se necess√°rio
                final_terms = keyword_terms[:]
                for term in keyword_terms:
                    if self.openai_service and term not in all_search_terms:
                        try:
                            term_synonyms = self._generate_synonyms_for_cache(term)
                            final_terms.extend(term_synonyms)
                        except:
                            pass
                all_search_terms = list(set(final_terms))  # Remove duplicates
            else:
                # Normalizar e dividir por espa√ßos + adicionar sin√¥nimos
                clean_keywords = self._normalizar_simples(keywords)
                basic_terms = [term.strip() for term in clean_keywords.split() if term.strip()]
                all_search_terms.extend(basic_terms)
                all_search_terms = list(set(all_search_terms))  # Remove duplicates
            
            logger.info(f"   üéØ Termos finais de busca (incluindo sin√¥nimos): {all_search_terms}")
            
            if all_search_terms:
                keyword_filtered = []
                matches_found = 0
                
                for item in filtered_data:
                    # Buscar nos MESMOS 3 campos do sistema antigo
                    objeto_compra = (item.get('objetoCompra') or '').lower()
                    objeto_detalhado = (item.get('objetoDetalhado') or '').lower()
                    info_complementar = (item.get('informacaoComplementar') or '').lower()
                    
                    # Combinar todos os textos
                    texto_completo = f"{objeto_compra} {objeto_detalhado} {info_complementar}".strip()
                    
                    if not texto_completo:
                        continue
                    
                    # Aplicar mesma normaliza√ß√£o
                    texto_normalizado = self._normalizar_simples(texto_completo)
                    
                    # üîÑ CORRE√á√ÉO: Verificar se QUALQUER termo da busca (incluindo sin√¥nimos) est√° presente
                    match_found = False
                    matched_term = None
                    for term in all_search_terms:
                        if not term:
                            continue
                        term_normalizado = self._normalizar_simples(term)
                        if term_normalizado and term_normalizado in texto_normalizado:
                            match_found = True
                            matched_term = term
                            break
                    
                    if match_found:
                        keyword_filtered.append(item)
                        matches_found += 1
                        
                        # Log dos primeiros 3 matches para debug
                        if matches_found <= 3:
                            logger.info(f"      ‚úÖ Match #{matches_found} (termo: '{matched_term}'): {objeto_compra[:100]}...")
                
                filtered_data = keyword_filtered
                logger.info(f"   üî§ Filtro keywords COM sin√¥nimos: {len(filtered_data)} matches de {initial_count}")
            else:
                logger.warning("   ‚ö†Ô∏è Nenhum termo v√°lido para busca")

        # üîç FILTRO DE REGI√ÉO
        region_code = filters.get('region_code')
        if region_code:
            logger.info(f"   üó∫Ô∏è Aplicando filtro de regi√£o: {region_code}")
            region_filtered = []
            for item in filtered_data:
                item_uf = item.get('unidadeOrgao', {}).get('ufSigla', '').upper()
                if item_uf == region_code.upper():
                    region_filtered.append(item)
            
            filtered_data = region_filtered
            logger.info(f"   üó∫Ô∏è Filtro regi√£o: {len(filtered_data)} restantes")

        # üîç FILTRO DE MUNIC√çPIO
        municipality = filters.get('municipality')
        if municipality:
            logger.info(f"   üèôÔ∏è Aplicando filtro de munic√≠pio: {municipality}")
            municipality_filtered = []
            municipality_normalized = self._normalizar_simples(municipality)
            
            for item in filtered_data:
                item_municipio = item.get('unidadeOrgao', {}).get('municipioNome', '')
                item_municipio_normalized = self._normalizar_simples(item_municipio)
                
                if municipality_normalized in item_municipio_normalized:
                    municipality_filtered.append(item)
            
            filtered_data = municipality_filtered
            logger.info(f"   üèôÔ∏è Filtro munic√≠pio: {len(filtered_data)} restantes")

        # üîç FILTRO DE VALOR M√çNIMO
        min_value = filters.get('min_value')
        if min_value is not None:
            logger.info(f"   üí∞ Aplicando filtro valor m√≠nimo: R$ {min_value:,.2f}")
            value_filtered = []
            for item in filtered_data:
                item_value = item.get('valorTotalEstimado')
                if item_value is not None:
                    try:
                        if float(item_value) >= float(min_value):
                            value_filtered.append(item)
                    except (ValueError, TypeError):
                        continue
            
            filtered_data = value_filtered
            logger.info(f"   üí∞ Filtro valor m√≠nimo: {len(filtered_data)} restantes")

        # üîç FILTRO DE VALOR M√ÅXIMO
        max_value = filters.get('max_value')
        if max_value is not None:
            logger.info(f"   üí∞ Aplicando filtro valor m√°ximo: R$ {max_value:,.2f}")
            value_filtered = []
            for item in filtered_data:
                item_value = item.get('valorTotalEstimado')
                if item_value is not None:
                    try:
                        if float(item_value) <= float(max_value):
                            value_filtered.append(item)
                    except (ValueError, TypeError):
                        continue
            
            filtered_data = value_filtered
            logger.info(f"   üí∞ Filtro valor m√°ximo: {len(filtered_data)} restantes")

        logger.info(f"üéØ FILTROS LOCAIS CONCLU√çDOS: {len(filtered_data)} registros finais de {initial_count} iniciais")
        
        return filtered_data
    
    def _normalizar_simples(self, texto: str) -> str:
        """
        CORRE√á√ÉO: Normaliza√ß√£o ID√äNTICA ao sistema antigo (licitacao_repository.py)
        - Remove acentos
        - Lowercase
        - Remove pontua√ß√£o
        - N√ÉO aplica stemmer agressivo
        """
        import unicodedata
        import re
        
        if not texto:
            return ""
        
        # Lowercase
        texto = texto.lower()
        
        # Remove acentos
        texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('ASCII')
        
        # Remove pontua√ß√£o mas mant√©m espa√ßos
        texto = re.sub(r'[^a-z0-9\s]', ' ', texto)
        
        # Remove espa√ßos m√∫ltiplos
        texto = ' '.join(texto.split())
        
        return texto

    def _extract_search_terms(self, keywords: Optional[str]) -> List[str]:
        """Extract search terms from keywords string"""
        if not keywords:
            return []
    
        # Simple split by common separators
        import re
        terms = re.split(r'[,;\s]+', keywords.strip())
        return [term.strip() for term in terms if term.strip()]
    
    def _convert_to_opportunity_data(self, licitacao: Dict[str, Any]) -> OpportunityData:
        """Convert PNCP licita√ß√£o to OpportunityData format"""
        try:
            # Extract location data
            location_data = self._get_location_data(licitacao)
            
            # üîß CORRE√á√ÉO CR√çTICA: Extrair CNPJ do √≥rg√£o para evitar erro de banco
            orgao_cnpj = self._extract_orgao_cnpj(licitacao)
            
            # üîß CORRE√á√ÉO: Extrair dados espec√≠ficos da API de consulta PNCP
            numero_controle = licitacao.get('numeroControlePNCP', '')
            
            # Informa√ß√µes b√°sicas
            titulo = licitacao.get('objetoCompra', '') or licitacao.get('objetoContratacao', '')
            descricao = licitacao.get('informacaoComplementar', '') or licitacao.get('objetoDetalhado', '')
            
            # Valores
            valor_estimado = licitacao.get('valorTotalEstimado', 0.0)
            
            # üìÖ DATAS (com m√∫ltiplas fontes de fallback)
            start_date = None
            end_date = None
            publication_date = None
            
            # üîß CORRE√á√ÉO CR√çTICA: Data de abertura das propostas 
            data_abertura_raw = (
                licitacao.get('dataAberturaProposta') or
                licitacao.get('dataRecebimentoProposta') or
                licitacao.get('dataInicioProposta') or
                licitacao.get('dataInicioRecebimento')
            )
            
            if data_abertura_raw:
                start_date = self._parse_pncp_date(data_abertura_raw)
                if start_date:
                    logger.debug(f"‚úÖ Data de abertura extra√≠da: {start_date} de '{data_abertura_raw}'")
                else:
                    logger.warning(f"‚ö†Ô∏è Falha ao parsear data de abertura: '{data_abertura_raw}'")
            else:
                logger.warning(f"‚ö†Ô∏è Data de abertura n√£o encontrada para {numero_controle}")
            
            # Data de encerramento
            data_encerramento_raw = (
                licitacao.get('dataEncerramentoProposta') or
                licitacao.get('dataFinalProposta') or
                licitacao.get('dataFimRecebimento')
            )
            
            if data_encerramento_raw:
                end_date = self._parse_pncp_date(data_encerramento_raw)
            
            # Data de publica√ß√£o PNCP
            data_publicacao_raw = (
                licitacao.get('dataPublicacaoPncp') or
                licitacao.get('dataPublicacao') or
                licitacao.get('dataInclusao')
            )
            
            if data_publicacao_raw:
                publication_date = self._parse_pncp_date(data_publicacao_raw)
            
            # Modalidade
            modalidade_nome = licitacao.get('modalidadeNome', '')
            modalidade_id = licitacao.get('modalidadeId')
            
            # √ìrg√£o respons√°vel
            orgao_entidade = licitacao.get('orgaoEntidade', {})
            orgao_nome = orgao_entidade.get('razaoSocial', '') if isinstance(orgao_entidade, dict) else ''
            
            # Unidade
            unidade_orgao = licitacao.get('unidadeOrgao', {})
            unidade_nome = unidade_orgao.get('nomeUnidade', '') if isinstance(unidade_orgao, dict) else ''
            
            # Status
            situacao_nome = licitacao.get('situacaoCompraNome', '')
            status = self._determine_status(licitacao)
            
            # üîß CORRIGIDO: Criar OpportunityData tempor√°rio primeiro
            opportunity_data = OpportunityData(
                external_id=numero_controle,
                title=titulo,
                description=descricao,
                estimated_value=float(valor_estimado) if valor_estimado else 0.0,
                currency_code='BRL',  # üîß CORRIGIDO: currency_code
                country_code='BR',    # üîß CORRIGIDO: country_code
                region_code=location_data['region'],      # üîß CORRIGIDO: region_code
                municipality=location_data['city'],       # üîß CORRIGIDO: municipality
                publication_date=publication_date,         # üîß CORRIGIDO: publication_date
                submission_deadline=end_date,    # üîß CORRIGIDO: submission_deadline
                procuring_entity_id=orgao_cnpj,          # üîß ADICIONADO: procuring_entity_id
                procuring_entity_name=orgao_nome,        # üîß ADICIONADO: procuring_entity_name
                provider_specific_data={
                    # üîß DADOS PARA SALVAR NO BANCO (evitar erro NOT NULL)
                    'orgao_cnpj': orgao_cnpj or '',  # CNPJ obrigat√≥rio
                    'orgao_nome': orgao_nome,
                    'unidade_nome': unidade_nome,
                    'modalidade_id': modalidade_id,
                    'modalidade_nome': modalidade_nome,
                    'situacao_nome': situacao_nome,
                    'numero_compra': licitacao.get('numeroCompra', ''),
                    'processo': licitacao.get('processo', ''),
                    'link_sistema_origem': licitacao.get('linkSistemaOrigem', ''),
                    'ano_compra': licitacao.get('anoCompra'),
                    'sequencial_compra': licitacao.get('sequencialCompra'),
                    'data_publicacao_pncp': publication_date,
                    'data_abertura_proposta': start_date,
                    'data_encerramento_proposta': end_date,
                    'amparo_legal': licitacao.get('amparoLegal', {}),
                    'modo_disputa': licitacao.get('modoDisputaNome', ''),
                    'valor_total_homologado': licitacao.get('valorTotalHomologado'),
                    'srp': licitacao.get('srp', False),
                    'tipo_instrumento': licitacao.get('tipoInstrumentoConvocatorioNome', ''),
                    'status': status,  # üîß ADICIONADO: status para compatibilidade
                    
                    # üîß DADOS BRUTOS PARA DEBUG E EXPANS√ïES FUTURAS
                    'raw_data': licitacao
                }
            )
            
            # üîß CORRE√á√ÉO CR√çTICA: Adicionar provider_name dinamicamente (PersistenceService precisa dele)
            # Como OpportunityData n√£o tem provider_name na defini√ß√£o, precisamos adicion√°-lo dinamicamente
            opportunity_data.provider_name = self.get_provider_name()
            opportunity_data.contracting_authority = orgao_nome
            
            return opportunity_data
            
        except Exception as e:
            logger.error(f"‚ùå Error converting PNCP data to OpportunityData: {e}")
            # üîß FALLBACK: Retornar dados m√≠nimos para evitar crash
            fallback_opportunity = OpportunityData(
                external_id=licitacao.get('numeroControlePNCP', 'unknown'),
                title='Erro na convers√£o de dados',
                description=f'Erro ao processar licita√ß√£o: {str(e)}',
                estimated_value=0.0,
                currency_code='BRL',
                country_code='BR',
                provider_specific_data={
                    'orgao_cnpj': '',  # Campo obrigat√≥rio vazio
                    'error': str(e),
                    'raw_data': licitacao
                }
            )
            
            # üîß CORRE√á√ÉO CR√çTICA: Adicionar provider_name no fallback tamb√©m
            fallback_opportunity.provider_name = self.get_provider_name()
            fallback_opportunity.contracting_authority = orgao_nome if 'orgao_nome' in locals() else None
            
            return fallback_opportunity
    
    def _extract_orgao_cnpj(self, licitacao: Dict[str, Any]) -> Optional[str]:
        """
        Extrai o CNPJ do √≥rg√£o respons√°vel pela licita√ß√£o
        
        Args:
            licitacao: Dados da licita√ß√£o do PNCP (API consulta ou API v1)
            
        Returns:
            CNPJ do √≥rg√£o ou None se n√£o encontrado
        """
        try:
            # üîç ESTRAT√âGIA 0: Se o CNPJ j√° foi extra√≠do do numeroControlePNCP, usar ele
            numero_controle = licitacao.get('numeroControlePNCP')
            if numero_controle:
                cnpj_from_controle, _, _ = self._parse_numero_controle_pncp(numero_controle)
                if cnpj_from_controle:
                    logger.debug(f"‚úÖ CNPJ extra√≠do do numeroControlePNCP: {cnpj_from_controle}")
                    return cnpj_from_controle
            
            # üîç ESTRAT√âGIA 1: orgaoEntidade.cnpj (API de consulta - PRINCIPAL)
            orgao_entidade = licitacao.get('orgaoEntidade', {})
            if orgao_entidade and isinstance(orgao_entidade, dict):
                cnpj = orgao_entidade.get('cnpj')
                if cnpj:
                    logger.debug(f"‚úÖ CNPJ extra√≠do do orgaoEntidade.cnpj: {cnpj}")
                    return cnpj
            
            # üîç ESTRAT√âGIA 2: Campos diretos de CNPJ (API v1)
            campos_cnpj_diretos = [
                'cnpjOrgao', 'cnpj', 'cnpjEntidade', 'cnpjUnidade',
                'identificadorOrgao', 'codigoUnidadeCompradora', 'cnpjUnidadeCompradora'
            ]
            
            for campo in campos_cnpj_diretos:
                cnpj = licitacao.get(campo)
                if cnpj and isinstance(cnpj, str):
                    cnpj_limpo = cnpj.replace('.', '').replace('/', '').replace('-', '')
                    if len(cnpj_limpo) == 14 and cnpj_limpo.isdigit():
                        logger.debug(f"‚úÖ CNPJ extra√≠do do campo {campo}: {cnpj}")
                        return cnpj
            
            # üîç ESTRAT√âGIA 3: Buscar na unidadeOrgao (API consulta)
            unidade_orgao = licitacao.get('unidadeOrgao', {})
            if unidade_orgao and isinstance(unidade_orgao, dict):
                cnpj = unidade_orgao.get('cnpj') or unidade_orgao.get('cnpjUnidade')
                if cnpj:
                    logger.debug(f"‚úÖ CNPJ extra√≠do da unidadeOrgao: {cnpj}")
                    return cnpj
            
            # üîç ESTRAT√âGIA 4: Buscar em orgao (API v1)
            orgao = licitacao.get('orgao', {})
            if orgao and isinstance(orgao, dict):
                cnpj = orgao.get('cnpj') or orgao.get('cnpjOrgao')
                if cnpj:
                    logger.debug(f"‚úÖ CNPJ extra√≠do do orgao (API v1): {cnpj}")
                    return cnpj
            
            # üîç ESTRAT√âGIA 5: Buscar em unidadeCompradora (API v1)
            unidade_compradora = licitacao.get('unidadeCompradora', {})
            if unidade_compradora and isinstance(unidade_compradora, dict):
                cnpj = unidade_compradora.get('cnpj') or unidade_compradora.get('cnpjUnidade')
                if cnpj:
                    logger.debug(f"‚úÖ CNPJ extra√≠do da unidadeCompradora (API v1): {cnpj}")
                    return cnpj
            
            # üîç ESTRAT√âGIA 6: Debug - mostrar estrutura dos dados
            logger.debug(f"üîç Estrutura dos dados para debug CNPJ:")
            logger.debug(f"   Campos principais: {list(licitacao.keys())}")
            if 'orgaoEntidade' in licitacao:
                orgao_ent = licitacao['orgaoEntidade']
                if isinstance(orgao_ent, dict):
                    logger.debug(f"   orgaoEntidade: {list(orgao_ent.keys())}")
                else:
                    logger.debug(f"   orgaoEntidade: {type(orgao_ent)} - {orgao_ent}")
            if 'unidadeOrgao' in licitacao:
                unid_org = licitacao['unidadeOrgao']
                if isinstance(unid_org, dict):
                    logger.debug(f"   unidadeOrgao: {list(unid_org.keys())}")
                else:
                    logger.debug(f"   unidadeOrgao: {type(unid_org)} - {unid_org}")
            
            logger.warning(f"‚ö†Ô∏è CNPJ n√£o encontrado para licita√ß√£o {licitacao.get('numeroControlePNCP', 'N/A')}")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair CNPJ da licita√ß√£o: {e}")
            return None
    
    def _get_organization_name(self, licitacao: Dict[str, Any]) -> str:
        """Extract organization name from licita√ß√£o"""
        orgao = licitacao.get('orgaoEntidade', {})
        if orgao and orgao.get('razaoSocial'):
            return orgao['razaoSocial']
        
        unidade = licitacao.get('unidadeOrgao', {})
        if unidade and unidade.get('nomeUnidade'):
            return unidade['nomeUnidade']
        
        return 'N/A'
    
    def _get_location_data(self, licitacao: Dict[str, Any]) -> Dict[str, Optional[str]]:
        """Extract location data from licita√ß√£o"""
        unidade = licitacao.get('unidadeOrgao', {})
        if unidade:
            cidade = unidade.get('municipioNome', '')
            uf = unidade.get('ufSigla', '')  # ‚úÖ Campo correto
            if cidade and uf:
                return {'city': cidade, 'region': uf}  # üîß CORRIGIDO: chaves corretas
            elif uf:
                return {'city': None, 'region': uf}  # üîß CORRIGIDO: chaves corretas
        
        # Fallback para orgaoEntidade (caso necess√°rio)
        orgao = licitacao.get('orgaoEntidade', {})
        if orgao and orgao.get('ufSigla'):  # ‚úÖ Corrigido campo
            return {'city': None, 'region': orgao['ufSigla']}  # üîß CORRIGIDO: chaves corretas
        
        return {'city': None, 'region': None}  # üîß CORRIGIDO: chaves corretas
    
    def _determine_status(self, licitacao: Dict[str, Any]) -> str:
        """Determine opportunity status based on dates and situation
        
        ÔøΩÔøΩ CORRE√á√ÉO: Retornar APENAS valores aceitos pelo banco de dados
        Valores permitidos: 'coletada', 'processada', 'matched'
        """
        from datetime import datetime
        
        try:
            # üîß MELHORIA: Analisar situacaoCompraNome primeiro (mais confi√°vel)
            situacao_compra = licitacao.get('situacaoCompraNome', '').lower()
            
            # üîß CORRE√á√ÉO CR√çTICA: Mapear para APENAS os 3 valores aceitos pelo banco
            if any(keyword in situacao_compra for keyword in ['divulgad', 'publicad', 'aberta', 'ativa']):
                return 'coletada'  # Licita√ß√£o dispon√≠vel para coleta
            elif any(keyword in situacao_compra for keyword in ['homologad', 'adjudicad', 'finaliz', 'encerr']):
                return 'processada'  # Licita√ß√£o finalizada
            elif any(keyword in situacao_compra for keyword in ['deserta', 'fracassad', 'suspens', 'cancel']):
                return 'processada'  # Licita√ß√£o n√£o teve sucesso, mas processada
            else:
                # üõ°Ô∏è FALLBACK: Para qualquer situa√ß√£o n√£o mapeada, usar 'coletada'
                logger.debug(f"Situa√ß√£o n√£o mapeada: '{situacao_compra}', usando 'coletada'")
                return 'coletada'
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao determinar status: {e}")
            return 'coletada'  # Valor padr√£o seguro
    
    def _parse_pncp_date(self, date_str: str) -> Optional[datetime]:
        """
        Parse PNCP date string to datetime object
        
        Args:
            date_str: Data no formato PNCP (pode ser ISO ou outro formato)
            
        Returns:
            datetime object ou None se n√£o conseguir fazer parse
        """
        if not date_str:
            return None
            
        try:
            # üîç FORMATO 1: ISO format with timezone (2025-07-10T07:59:59Z)
            if 'T' in date_str:
                # Remove timezone info if present
                clean_date = date_str.split('T')[0]
                return datetime.strptime(clean_date, '%Y-%m-%d')
            
            # üîç FORMATO 2: ISO date only (2025-07-10)
            if '-' in date_str and len(date_str) == 10:
                return datetime.strptime(date_str, '%Y-%m-%d')
            
            # üîç FORMATO 3: Brazilian format (10/07/2025)
            if '/' in date_str:
                if len(date_str) == 10:  # DD/MM/YYYY
                    return datetime.strptime(date_str, '%d/%m/%Y')
                elif len(date_str) == 8:   # DD/MM/YY
                    return datetime.strptime(date_str, '%d/%m/%y')
            
            # üîç FORMATO 4: YYYYMMDD
            if date_str.isdigit() and len(date_str) == 8:
                return datetime.strptime(date_str, '%Y%m%d')
            
            logger.warning(f"‚ö†Ô∏è Formato de data n√£o reconhecido: {date_str}")
            return None
            
        except ValueError as e:
            logger.warning(f"‚ö†Ô∏è Erro ao fazer parse da data '{date_str}': {e}")
            return None
    
    def _generate_source_url(self, licitacao: Dict[str, Any]) -> str:
        """Generate source URL for the opportunity"""
        pncp_id = licitacao.get('numeroControlePNCP', '')
        if pncp_id:
            return f"https://pncp.gov.br/app/editais/{pncp_id}"
        return "https://pncp.gov.br"
    
    def get_provider_metadata(self) -> Dict[str, Any]:
        """Get PNCP provider metadata"""
        return {
            'name': 'PNCP - Portal Nacional de Contrata√ß√µes P√∫blicas',
            'country': 'BR',
            'supported_filters': self.get_supported_filters(),
            'rate_limit': self.max_results, # Assuming max_results is the rate limit
            'data_coverage': 'Brazil government procurement',
            'update_frequency': 'Real-time',
            'api_version': 'v1',
            'documentation': 'https://pncp.gov.br/api',
            'default_currency': 'BRL',
            'pagination_info': {
                'api_page_size': 50, # Default page size
                'target_total_results': self.max_results,
                'strategy': 'efficient_national_search'
            }
        }
    
    def get_supported_filters(self) -> Dict[str, Any]:
        """Get filters supported by PNCP provider"""
        return {
            'keywords': {
                'type': 'string',
                'description': 'Search terms for procurement object',
                'required': False
            },
            'region_code': {
                'type': 'string', 
                'description': 'Brazilian state code (UF)',
                'required': False,
                'options': ['AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', 'MT', 'MS', 'MG', 'PA', 'PB', 'PR', 'PE', 'PI', 'RJ', 'RN', 'RS', 'RO', 'RR', 'SC', 'SP', 'SE', 'TO']
            },
            'municipality': {
                'type': 'string',
                'description': 'City name',
                'required': False
            },
            'min_value': {
                'type': 'number',
                'description': 'Minimum estimated value in BRL',
                'required': False
            },
            'max_value': {
                'type': 'number', 
                'description': 'Maximum estimated value in BRL',
                'required': False
            },
            'procurement_type': {
                'type': 'array',
                'description': 'Procurement modalities',
                'required': False,
                'options': ['pregao_eletronico', 'concorrencia', 'dispensa', 'tomada_precos', 'convite', 'inexigibilidade']
            },
            'status': {
                'type': 'string',
                'description': 'Filter by opportunity status',
                'required': False,
                'options': ['open', 'closed', 'all']
            }
        }
    
    def get_provider_name(self) -> str:
        """Get the provider name"""
        return "pncp"
    
    def get_opportunity_details(self, external_id: str) -> Optional[OpportunityData]:
        """Get detailed information for a specific opportunity
        
        Args:
            external_id: The PNCP numeroControlePNCP identifier
            
        Returns:
            OpportunityData with detailed information or None if not found
        """
        try:
            logger.info(f"üîç Buscando detalhes da licita√ß√£o PNCP: {external_id}")
            
            # IMPLEMENTA√á√ÉO REAL: Buscar via API PNCP usando o numeroControlePNCP
            detailed_data = self._fetch_bid_details_from_pncp(external_id)
            
            if detailed_data:
                logger.info(f"‚úÖ Detalhes encontrados para licita√ß√£o {external_id}")
                return self._convert_to_opportunity_data(detailed_data)
            else:
                logger.warning(f"‚ö†Ô∏è Licita√ß√£o {external_id} n√£o encontrada no PNCP")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao buscar detalhes da licita√ß√£o PNCP {external_id}: {e}")
            return None
    
    def _fetch_bid_details_from_pncp(self, numero_controle_pncp: str) -> Optional[Dict[str, Any]]:
        """
        Busca os detalhes completos de uma licita√ß√£o espec√≠fica via API PNCP v1
        
        üîß CORRE√á√ÉO CR√çTICA: URL da API v1 foi movida
        URL NOVA: https://pncp.gov.br/api/consulta/v1/orgaos/{CNPJ}/compras/{ANO}/{SEQUENCIAL}
        
        Args:
            numero_controle_pncp: N√∫mero de controle PNCP (formato: CNPJ-ANO-SEQUENCIAL/ANO)
            
        Returns:
            Dict com dados detalhados da licita√ß√£o ou None se n√£o encontrada
        """
        import requests
        
        try:
            logger.info(f"üîç Buscando licita√ß√£o espec√≠fica via API v1: {numero_controle_pncp}")
            
            # üîß EXTRAIR COMPONENTES DO numeroControlePNCP
            cnpj, ano, sequencial = self._parse_numero_controle_pncp(numero_controle_pncp)
            
            if not all([cnpj, ano, sequencial]):
                logger.error(f"‚ùå N√£o foi poss√≠vel extrair CNPJ/ANO/SEQUENCIAL de: {numero_controle_pncp}")
                return None
            
            logger.info(f"üìã Componentes extra√≠dos - CNPJ: {cnpj}, ANO: {ano}, SEQUENCIAL: {sequencial}")
            
            # üåê MONTAR URL DA API v1 CORRIGIDA
            api_v1_base = "https://pncp.gov.br/api/consulta/v1"  # üîß URL CORRIGIDA
            url_detalhes = f"{api_v1_base}/orgaos/{cnpj}/compras/{ano}/{sequencial}"
            
            logger.info(f"üåê Buscando detalhes via URL CORRIGIDA: {url_detalhes}")
            
            # üîç BUSCAR DADOS PRINCIPAIS DA LICITA√á√ÉO
            response = requests.get(url_detalhes, timeout=self.timeout)
            response.raise_for_status()
            
            licitacao_detalhada = response.json()
            logger.info(f"‚úÖ Detalhes encontrados para {numero_controle_pncp}")
            
            # üîç BUSCAR ITENS DA LICITA√á√ÉO (OPCIONAL)
            try:
                url_itens = f"{url_detalhes}/itens"
                logger.info(f"üîç Buscando itens via: {url_itens}")
                
                response_itens = requests.get(url_itens, timeout=self.timeout)
                response_itens.raise_for_status()
                
                itens = response_itens.json()
                logger.info(f"‚úÖ {len(itens)} itens encontrados")
                
                # Adicionar itens aos dados da licita√ß√£o
                licitacao_detalhada['itens'] = itens
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel buscar itens: {e}")
                licitacao_detalhada['itens'] = []
            
            return licitacao_detalhada
                
        except requests.RequestException as e:
            logger.error(f"‚ùå Erro na API PNCP v1 ao buscar detalhes de {numero_controle_pncp}: {e}")
            
            # üîÑ FALLBACK: Tentar com a estrat√©gia antiga se API v1 falhar
            logger.info("üîÑ Tentando fallback com busca abrangente...")
            return self._fetch_bid_details_fallback(numero_controle_pncp)
            
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado ao buscar detalhes de {numero_controle_pncp}: {e}")
            return None
    
    def _parse_numero_controle_pncp(self, numero_controle: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """
        Extrai CNPJ, ANO e SEQUENCIAL do numeroControlePNCP
        
        Formatos esperados:
        - CNPJ-ANO-SEQUENCIAL/ANO (ex: 17217985000104-1-000156/2025)
        - CNPJ-SEQUENCIAL-ANO (varia√ß√µes)
        
        Args:
            numero_controle: N√∫mero de controle PNCP
            
        Returns:
            Tuple com (CNPJ, ANO, SEQUENCIAL) ou (None, None, None) se n√£o conseguir extrair
        """
        try:
            if not numero_controle or '-' not in numero_controle:
                return None, None, None
            
            logger.debug(f"üîç Analisando numeroControlePNCP: {numero_controle}")
            
            # üîç PADR√ÉO 1: CNPJ-X-SEQUENCIAL/ANO
            if '/' in numero_controle:
                parte_principal, ano_final = numero_controle.split('/')
                partes = parte_principal.split('-')
                
                if len(partes) >= 3:
                    cnpj = partes[0]
                    sequencial = partes[2]  # Terceira parte √© o sequencial
                    ano = ano_final  # Ano ap√≥s a barra
                    
                    logger.debug(f"   üìã Padr√£o 1 - CNPJ: {cnpj}, ANO: {ano}, SEQUENCIAL: {sequencial}")
                    return cnpj, ano, sequencial
            
            # üîç PADR√ÉO 2: CNPJ-ANO-SEQUENCIAL (sem barra)
            partes = numero_controle.split('-')
            if len(partes) >= 3:
                cnpj = partes[0]
                ano = partes[1]
                sequencial = partes[2]
                
                logger.debug(f"   üìã Padr√£o 2 - CNPJ: {cnpj}, ANO: {ano}, SEQUENCIAL: {sequencial}")
                return cnpj, ano, sequencial
            
            logger.warning(f"‚ö†Ô∏è Formato n√£o reconhecido: {numero_controle}")
            return None, None, None
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao fazer parse do numeroControlePNCP '{numero_controle}': {e}")
            return None, None, None
    
    def _fetch_bid_details_fallback(self, numero_controle_pncp: str) -> Optional[Dict[str, Any]]:
        """
        FALLBACK: Busca usando a estrat√©gia antiga se API v1 falhar
        """
        import requests
        
        try:
            logger.info(f"üîÑ FALLBACK: Busca abrangente para {numero_controle_pncp}")
            
            # URL da API de consulta (a original)
            url = f"{self.api_base_url}/contratacoes/proposta"
            
            # Par√¢metros b√°sicos
            params = {
                'dataInicial': self.data_inicial,
                'dataFinal': self.data_final,
                'tamanhoPagina': 50,
                'pagina': 1,
                'codigoModalidadeContratacao': 8  # Preg√£o Eletr√¥nico
            }
            
            # Buscar em at√© 5 p√°ginas
            for pagina in range(1, 6):
                params['pagina'] = pagina
                
                response = requests.get(url, params=params, timeout=self.timeout)
                response.raise_for_status()
                
                data = response.json()
                licitacoes = data.get('data', []) if isinstance(data, dict) else data
                
                # Procurar o n√∫mero de controle espec√≠fico
                for licitacao in licitacoes:
                    numero_controle_atual = licitacao.get('numeroControlePNCP')
                    if numero_controle_atual == numero_controle_pncp:
                        logger.info(f"‚úÖ FALLBACK: Licita√ß√£o encontrada na p√°gina {pagina}")
                        return licitacao
                
                if len(licitacoes) < 50:
                    break
            
            logger.warning(f"‚ö†Ô∏è FALLBACK: Licita√ß√£o {numero_controle_pncp} n√£o encontrada")
            return None
                
        except Exception as e:
            logger.error(f"‚ùå FALLBACK falhou: {e}")
            return None
    
    def get_opportunity_items(self, external_id: str) -> List[Dict[str, Any]]:
        """Get items/products for a specific opportunity
        
        Args:
            external_id: The PNCP numeroControlePNCP identifier
            
        Returns:
            List of items/products for this opportunity
        """
        try:
            logger.info(f"üîç Buscando itens da licita√ß√£o PNCP: {external_id}")
            
            # üîß EXTRAIR COMPONENTES DO numeroControlePNCP
            cnpj, ano, sequencial = self._parse_numero_controle_pncp(external_id)
            
            if not all([cnpj, ano, sequencial]):
                logger.error(f"‚ùå N√£o foi poss√≠vel extrair CNPJ/ANO/SEQUENCIAL de: {external_id}")
                return []
            
            # üåê MONTAR URL DA API v1 PARA ITENS
            api_v1_base = "https://pncp.gov.br/api/consulta/v1"  # üîß URL CORRIGIDA
            url_itens = f"{api_v1_base}/orgaos/{cnpj}/compras/{ano}/{sequencial}/itens"
            
            logger.info(f"üåê Buscando itens via API v1 CORRIGIDA: {url_itens}")
            
            # üîç BUSCAR ITENS DA LICITA√á√ÉO
            import requests
            response = requests.get(url_itens, timeout=self.timeout)
            response.raise_for_status()
            
            itens = response.json()
            logger.info(f"‚úÖ {len(itens)} itens encontrados para {external_id}")
            
            return itens
            
        except requests.RequestException as e:
            logger.error(f"‚ùå Erro na API PNCP v1 ao buscar itens de {external_id}: {e}")
            
            # üîÑ FALLBACK: Tentar extrair itens dos dados principais
            try:
                logger.info("üîÑ Tentando extrair itens dos dados principais...")
                detalhes = self.get_opportunity_details(external_id)
                if detalhes and detalhes.provider_specific_data:
                    raw_data = detalhes.provider_specific_data.get('raw_data', {})
                    itens_embedded = raw_data.get('itens', [])
                    if itens_embedded:
                        logger.info(f"‚úÖ FALLBACK: {len(itens_embedded)} itens extra√≠dos dos dados principais")
                        return itens_embedded
                
                logger.warning(f"‚ö†Ô∏è Nenhum item encontrado para {external_id}")
                return []
                
            except Exception as fallback_error:
                logger.error(f"‚ùå FALLBACK tamb√©m falhou: {fallback_error}")
                return []
            
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado ao buscar itens de {external_id}: {e}")
            return []

    async def validate_connection(self) -> bool:
        """Test connection to PNCP API"""
        try:
            # Try a simple search
            test_filters = SearchFilters(keywords="teste")
            results = await self.search_opportunities(test_filters)
            logger.info("‚úÖ PNCP connection validated successfully")
            return True
        except Exception as e:
            logger.error(f"‚ùå PNCP connection validation failed: {e}")
            return False