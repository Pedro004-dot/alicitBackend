import logging
from typing import Dict, Any, Optional, List
import time
from datetime import datetime

# Importar componentes RAG
from rag.document_processor import DocumentProcessor
from rag.embedding_service import EmbeddingService
from rag.vector_store import VectorStore
from rag.cache_manager import CacheManager
from rag.retrieval_engine import RetrievalEngine

# üÜï NOVO: Importar servi√ßos de cache e deduplica√ß√£o
from services.embedding_cache_service import EmbeddingCacheService
from services.deduplication_service import DeduplicationService

logger = logging.getLogger(__name__)

class RAGService:
    """Servi√ßo principal de RAG com cache inteligente"""
    
    def __init__(self, db_manager, unified_processor, openai_api_key: str, 
                 redis_host: str = "localhost"):
        self.db_manager = db_manager
        self.unified_processor = unified_processor
        
        # üÜï NOVO: Inicializar cache e deduplica√ß√£o
        self.cache_service = EmbeddingCacheService(db_manager, redis_host)
        self.dedup_service = DeduplicationService(db_manager, self.cache_service)
        
        # Componentes RAG existentes
        self.document_processor = DocumentProcessor()
        self.embedding_service = EmbeddingService()  # Manter VoyageAI como fallback
        self.vector_store = VectorStore(db_manager)
        self.cache_manager = CacheManager(redis_host=redis_host)
        self.retrieval_engine = RetrievalEngine(openai_api_key)
        
        logger.info("‚úÖ RAGService inicializado com cache inteligente")
    
    def process_or_query(self, licitacao_id: str, query: str) -> Dict[str, Any]:
        """Fun√ß√£o principal: processa documentos se necess√°rio e responde query"""
        try:
            logger.info(f"üöÄ Iniciando RAG para licita√ß√£o: {licitacao_id}")
            
            # 1. Verificar cache primeiro
            cached_result = self.cache_manager.get_cached_query_result(query, licitacao_id)
            if cached_result:
                logger.info("‚ö° Resultado encontrado no cache")
                return {
                    'success': True,
                    'cached': True,
                    **cached_result
                }
            
            # 2. Verificar se documentos est√£o vetorizados
            status = self.vector_store.check_vectorization_status(licitacao_id)
                         
            if not status.get('vetorizado_completo', False):
                # 3. Processar documentos se necess√°rio (inclui extra√ß√£o recursiva de ZIPs)
                logger.info("üìù Documentos n√£o vetorizados, iniciando processamento completo...")
                print("üìù Documentos n√£o vetorizados, iniciando processamento completo...")
                vectorization_result = self._vectorize_licitacao(licitacao_id)
                print(f"vectorization_result: {vectorization_result}")
                
                if not vectorization_result['success']:
                    # Incluir detalhes de diagn√≥stico no resultado
                    error_response = {
                        'success': False,
                        'error': vectorization_result.get('error'),
                        'processing_details': {
                            'action': vectorization_result.get('action', 'unknown'),
                            'suggestion': vectorization_result.get('suggestion'),
                            'licitacao_info': vectorization_result.get('licitacao_info')
                        }
                    }
                    return error_response
            
            # 4. Responder query
            response_result = self._answer_query(query, licitacao_id)
            
            # 5. Cachear resultado
            if response_result['success']:
                self.cache_manager.cache_query_result(
                    query, licitacao_id, response_result, ttl=3600
                )
                
                # Adicionar informa√ß√µes de processamento se documentos foram processados nesta sess√£o
                if not status.get('vetorizado_completo', False):
                    response_result['processing_info'] = {
                        'documents_processed_this_session': True,
                        'processing_method': 'recursive_zip_extraction',
                        'vectorization_completed': True
                    }
            
            return response_result
            
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento RAG: {e}")
            return {
                'success': False,
                'error': f'Erro interno: {str(e)}'
            }
    
    def _vectorize_licitacao(self, licitacao_id: str) -> Dict[str, Any]:
        """VERS√ÉO ATUALIZADA: Vetoriza documentos evitando reprocessamento"""
        try:
            # 1. Verificar e processar documentos automaticamente se necess√°rio
            documentos_result = self._ensure_documents_processed(licitacao_id)
            if not documentos_result['success']:
                return documentos_result
            
            # 2. Obter documentos do banco
            documentos = self.unified_processor.obter_documentos_licitacao(licitacao_id)
            
            if not documentos:
                return {
                    'success': False,
                    'error': 'Nenhum documento encontrado para vetoriza√ß√£o'
                }
            
            # 3. üÜï NOVO: Verificar quais documentos j√° foram processados
            documentos_para_processar = []
            documentos_ja_processados = 0
            
            for documento in documentos:
                documento_data = {
                    'arquivo_url': documento.get('arquivo_nuvem_url', ''),
                    'tamanho_arquivo': documento.get('tamanho_arquivo', 0),
                    'hash_arquivo': documento.get('hash_arquivo', '')
                }
                
                # Verificar se j√° tem chunks vetorizados OU se j√° foi processado antes
                existing_chunks = self.vector_store.count_document_chunks(documento['id'])
                already_processed = self.dedup_service.should_process_rag_document(documento['id'], documento_data)
                
                if existing_chunks > 0 or not already_processed:
                    documentos_ja_processados += 1
                    logger.info(f"‚è≠Ô∏è Documento j√° processado: {documento['titulo']} ({existing_chunks} chunks)")
                else:
                    documentos_para_processar.append(documento)
            
            if documentos_ja_processados > 0:
                logger.info(f"üìä Documentos: {documentos_ja_processados} j√° processados, {len(documentos_para_processar)} para processar")
            
            # Se todos j√° processados, retornar sucesso
            if not documentos_para_processar:
                total_chunks = sum(self.vector_store.count_document_chunks(doc['id']) for doc in documentos)
                return {
                    'success': True,
                    'message': 'Todos os documentos j√° estavam vetorizados',
                    'processed_documents': len(documentos),
                    'total_chunks': total_chunks,
                    'already_vectorized': True
                }
            
            # 4. üÜï NOVO: Processar apenas documentos necess√°rios
            total_chunks = 0
            processed_docs = 0
            embedding_cache_hits = 0
            
            logger.info(f"üìã Processando {len(documentos_para_processar)} documentos para vetoriza√ß√£o...")
            
            for idx, documento in enumerate(documentos_para_processar, 1):
                logger.info(f"üìÑ Processando documento {idx}/{len(documentos_para_processar)}: {documento['titulo']}")
                
                # Marcar como processando
                self._update_document_status(documento['id'], 'processando')
                
                try:
                    # Extrair texto (SEM MUDAN√áA)
                    texto_completo = self.document_processor.extract_text_from_url(
                        documento['arquivo_nuvem_url']
                    )
                    
                    if not texto_completo:
                        self._update_document_status(documento['id'], 'erro')
                        continue
                    
                    # Salvar texto extra√≠do
                    self._save_extracted_text(documento['id'], texto_completo)
                    
                    # Criar chunks inteligentes (SEM MUDAN√áA)
                    chunks = self.document_processor.create_intelligent_chunks(
                        texto_completo, documento['id']
                    )
                    
                    if not chunks:
                        self._update_document_status(documento['id'], 'erro')
                        continue
                    
                    # üÜï NOVO: Gerar embeddings com cache
                    chunk_texts = [chunk.text for chunk in chunks]
                    embeddings = []
                    
                    # Verificar cache para cada chunk
                    for chunk_text in chunk_texts:
                        cached_embedding = self.cache_service.get_embedding_from_cache(
                            chunk_text, "sentence-transformers"
                        )
                        
                        if cached_embedding:
                            embeddings.append(cached_embedding)
                            embedding_cache_hits += 1
                        else:
                            # Gerar novo embedding (usar sentence-transformers primeiro)
                            new_embedding = self._generate_embedding_with_fallback(chunk_text)
                            embeddings.append(new_embedding)
                            
                            if new_embedding:
                                self.cache_service.save_embedding_to_cache(
                                    chunk_text, new_embedding, "sentence-transformers"
                                )
                    
                    # Verificar se todos embeddings foram gerados
                    valid_embeddings = [emb for emb in embeddings if emb]
                    
                    if len(valid_embeddings) != len(chunks):
                        logger.error(f"‚ùå Embeddings incompletos para {documento['titulo']}")
                        logger.error(f"üìä Esperado: {len(chunks)} embeddings, Gerado: {len(valid_embeddings)}")
                        self._update_document_status(documento['id'], 'erro_embedding')
                        continue
                    
                    # Salvar no vector store (SEM MUDAN√áA)
                    chunk_dicts = [self._chunk_to_dict(chunk) for chunk in chunks]
                    success = self.vector_store.save_chunks_with_embeddings(
                        documento['id'], licitacao_id, chunk_dicts, valid_embeddings
                    )
                    
                    if success:
                        total_chunks += len(chunks)
                        processed_docs += 1
                        
                        # üÜï NOVO: Marcar documento como processado
                        documento_data = {
                            'arquivo_url': documento.get('arquivo_nuvem_url', ''),
                            'tamanho_arquivo': documento.get('tamanho_arquivo', 0),
                            'hash_arquivo': documento.get('hash_arquivo', '')
                        }
                        self.dedup_service.mark_rag_document_processed(documento['id'], documento_data)
                        
                        logger.info(f"‚úÖ Documento vetorizado: {documento['titulo']} ({len(chunks)} chunks)")
                    else:
                        self._update_document_status(documento['id'], 'erro')
                
                except Exception as e:
                    logger.error(f"‚ùå Erro ao vetorizar {documento['t√≠tulo']}: {e}")
                    self._update_document_status(documento['id'], 'erro')
                    continue
            
            logger.info(f"üìä Cache hits de embeddings: {embedding_cache_hits}")
            
            if processed_docs > 0 or documentos_ja_processados > 0:
                total_existing_chunks = sum(self.vector_store.count_document_chunks(doc['id']) for doc in documentos)
                return {
                    'success': True,
                    'message': 'Vetoriza√ß√£o conclu√≠da com cache',
                    'processed_documents': processed_docs,
                    'already_processed_documents': documentos_ja_processados,
                    'total_chunks': total_chunks + total_existing_chunks,
                    'embedding_cache_hits': embedding_cache_hits
                }
            else:
                return {
                    'success': False,
                    'error': 'Nenhum documento foi vetorizado com sucesso'
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erro na vetoriza√ß√£o: {e}")
            return {
                'success': False,
                'error': f'Erro na vetoriza√ß√£o: {str(e)}'
            }
    
    def _generate_embedding_with_fallback(self, text: str) -> List[float]:
        """üÜï NOVO: Gera embedding com fallback sentence-transformers -> voyage -> openai"""
        
        # 1. Tentar sentence-transformers primeiro (se dispon√≠vel)
        try:
            from services.sentence_transformer_service import SentenceTransformerService
            st_service = SentenceTransformerService()
            embedding = st_service.generate_single_embedding(text)
            if embedding:
                return embedding
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è SentenceTransformers falhou: {e}")
        
        # 2. Fallback para VoyageAI
        try:
            embedding = self.embedding_service.generate_single_embedding(text)
            if embedding:
                return embedding
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è VoyageAI falhou: {e}")
        
        # 3. √öltimo recurso: usar vectorizer do matching
        try:
            from matching.vectorizers import OpenAITextVectorizer
            openai_vectorizer = OpenAITextVectorizer()
            embedding = openai_vectorizer.vectorize(text)
            if embedding:
                return embedding
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è OpenAI fallback falhou: {e}")
        
        logger.warning("üîÑ Todos os m√©todos de embedding falharam")
        return []
    
    def _chunk_to_dict(self, chunk) -> Dict[str, Any]:
        """Converte chunk para dicion√°rio"""
        return {
            'text': chunk.text,
            'chunk_type': chunk.chunk_type,
            'page_number': chunk.page_number,
            'section_title': chunk.section_title,
            'token_count': chunk.token_count,
            'char_count': chunk.char_count,
            'metadata': chunk.metadata or {}
        }
    
    def _answer_query(self, query: str, licitacao_id: str) -> Dict[str, Any]:
        """Responde uma query usando RAG"""
        try:
            start_time = time.time()
            
            # 1. Gerar embedding da query
            query_embedding = self.embedding_service.generate_single_embedding(query)
            
            if not query_embedding:
                return {
                    'success': False,
                    'error': 'Erro ao gerar embedding da consulta'
                }
            
            # 2. Buscar chunks relevantes (h√≠brida)
            chunks = self.vector_store.hybrid_search(
                query, query_embedding, licitacao_id, limit=12  # Buscar mais para reranking
            )
            
            if not chunks:
                return {
                    'success': False,
                    'error': 'Nenhum conte√∫do relevante encontrado nos documentos'
                }
            
            # 3. Aplicar reranking
            top_chunks = self.retrieval_engine.rerank_chunks(query, chunks, top_k=8)
            
            # 4. Obter informa√ß√µes da licita√ß√£o
            licitacao_info = self.unified_processor.extrair_info_licitacao(licitacao_id)
            
            # 5. Gerar resposta
            response_result = self.retrieval_engine.generate_response(
                query, top_chunks, licitacao_info
            )
            
            if response_result.get('error'):
                return {
                    'success': False,
                    'error': response_result['answer']
                }
            
            processing_time = time.time() - start_time
            
            # 6. Montar resultado final
            result = {
                'success': True,
                'answer': response_result['answer'],
                'query': query,
                'licitacao_id': licitacao_id,
                'chunks_used': response_result['chunks_used'],
                'sources': response_result['sources'],
                'processing_time': round(processing_time, 2),
                'model_response_time': response_result.get('response_time'),
                'cost_usd': response_result.get('cost_usd'),
                'model': response_result.get('model'),
                'cached': False
            }
            
            logger.info(f"‚úÖ Query respondida em {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao responder query: {e}")
            return {
                'success': False,
                'error': f'Erro ao processar consulta: {str(e)}'
            }
    
    def _ensure_documents_processed(self, licitacao_id: str) -> Dict[str, Any]:
        """
        üöÄ NOVA FUN√á√ÉO: Garante que documentos est√£o processados usando UnifiedDocumentProcessor recursivo
        Integra√ß√£o completa do processamento recursivo de ZIPs no fluxo RAG
        """
        try:
            logger.info(f"üîç Verificando documentos para licita√ß√£o: {licitacao_id}")
            
            # 1. Verificar se documentos j√° existem
            documentos_existem = self.unified_processor.verificar_documentos_existem(licitacao_id)
            
            if documentos_existem:
                # Verificar se documentos s√£o v√°lidos (n√£o corrompidos/vazios)
                documentos = self.unified_processor.obter_documentos_licitacao(licitacao_id)
                
                # Validar se temos documentos √∫teis
                documentos_validos = []
                for doc in documentos:
                    # Verificar se √© arquivo √∫til (PDF, DOC, etc) e n√£o est√° corrompido
                    if (doc.get('tipo_arquivo') in ['application/pdf', 'application/msword'] and 
                        doc.get('tamanho_arquivo', 0) > 1000):  # Arquivos > 1KB
                        documentos_validos.append(doc)
                
                if documentos_validos:
                    logger.info(f"‚úÖ {len(documentos_validos)} documentos v√°lidos j√° processados")
                    return {
                        'success': True,
                        'message': 'Documentos j√° processados',
                        'documentos_count': len(documentos_validos),
                        'action': 'documents_already_exist'
                    }
                else:
                    logger.warning(f"‚ö†Ô∏è Documentos existem mas s√£o inv√°lidos, reprocessando...")
                    # Limpar documentos inv√°lidos
                    self.unified_processor._limpar_documentos_licitacao(licitacao_id)
            
            # 2. Processar documentos usando nossa l√≥gica recursiva aprimorada
            logger.info("üì• Iniciando processamento recursivo de documentos...")
            
            # Primeiro, validar se a licita√ß√£o existe
            licitacao_info = self.unified_processor.extrair_info_licitacao(licitacao_id)
            if not licitacao_info:
                return {
                    'success': False,
                    'error': f'Licita√ß√£o {licitacao_id} n√£o encontrada no banco de dados',
                    'action': 'licitacao_not_found'
                }
            
            # Tentar processar documentos (m√©todo ass√≠ncrono)
            import asyncio
            result = asyncio.run(self.unified_processor.processar_documentos_licitacao(licitacao_id))
            
            if result['success']:
                logger.info(f"üéâ Processamento recursivo conclu√≠do: {result.get('documentos_processados', 0)} documentos")
                return {
                    'success': True,
                    'message': 'Documentos processados com sucesso usando extra√ß√£o recursiva',
                    'documentos_processados': result.get('documentos_processados', 0),
                    'storage_provider': result.get('storage_provider', 'supabase'),
                    'pasta_nuvem': result.get('pasta_nuvem'),
                    'action': 'documents_processed_recursive'
                }
            else:
                # Se falhou, tentar diagnosticar o problema
                error_detail = result.get('error', 'Erro desconhecido')
                logger.error(f"‚ùå Falha no processamento: {error_detail}")
                
                # Verificar se √© problema da API PNCP
                if 'API PNCP' in error_detail or 'HTTP' in error_detail:
                    return {
                        'success': False,
                        'error': f'Erro na API PNCP: {error_detail}',
                        'suggestion': 'Verifique se a licita√ß√£o possui documentos dispon√≠veis na API',
                        'action': 'api_error'
                    }
                
                # Outros erros
                return {
                    'success': False,
                    'error': f'Erro no processamento de documentos: {error_detail}',
                    'licitacao_info': {
                        'objeto': licitacao_info.get('objeto_compra', 'N/A')[:100] + '...',
                        'orgao': licitacao_info.get('orgao_entidade', 'N/A'),
                        'uf': licitacao_info.get('uf', 'N/A')
                    },
                    'action': 'processing_error'
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico ao garantir documentos processados: {e}")
            return {
                'success': False,
                'error': f'Erro cr√≠tico no processamento: {str(e)}',
                'action': 'critical_error'
            }
    
    def _update_document_status(self, documento_id: str, status: str):
        """Atualiza status de processamento do documento"""
        try:
            with self.db_manager.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        UPDATE documentos_licitacao 
                        SET status_processamento = %s, updated_at = NOW()
                        WHERE id = %s
                    """, (status, documento_id))
                    conn.commit()
        except Exception as e:
            logger.error(f"‚ùå Erro ao atualizar status: {e}")
    
    def _save_extracted_text(self, documento_id: str, texto: str):
        """Salva texto extra√≠do no banco"""
        try:
            with self.db_manager.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        UPDATE documentos_licitacao 
                        SET texto_extraido = %s, updated_at = NOW()
                        WHERE id = %s
                    """, (texto, documento_id))
                    conn.commit()
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar texto: {e}")
    
    def get_licitacao_stats(self, licitacao_id: str) -> Dict[str, Any]:
        """Retorna estat√≠sticas de uma licita√ß√£o"""
        try:
            status = self.vector_store.check_vectorization_status(licitacao_id)
            cache_stats = self.cache_manager.get_cache_stats()
            
            return {
                'licitacao_id': licitacao_id,
                'vectorization_status': status,
                'cache_stats': cache_stats,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao obter stats: {e}")
            return {'error': str(e)}
