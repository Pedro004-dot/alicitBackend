import logging
from typing import Dict, Any, Optional, List
import time
from datetime import datetime
import re

# Importar componentes RAG
from rag.document_processor import DocumentProcessor
from rag.embedding_service import EmbeddingService
from rag.vector_store import VectorStore
from rag.cache_manager import CacheManager
from rag.retrieval_engine import RetrievalEngine

# üÜï NOVO: Importar servi√ßos de cache e deduplica√ß√£o
from services.embedding_cache_service import EmbeddingCacheService
from services.deduplication_service import DeduplicationService

logger = logging.getLogger(__name__)

class RAGService:
    """Servi√ßo principal de RAG com cache inteligente"""
    
    def __init__(self, db_manager, unified_processor, openai_api_key: str, 
                 **kwargs):
        self.db_manager = db_manager
        self.unified_processor = unified_processor
        
        # üÜï NOVO: Inicializar cache e deduplica√ß√£o (Railway ready)
        self.cache_service = EmbeddingCacheService(db_manager)
        self.dedup_service = DeduplicationService(db_manager, self.cache_service)
        
        # Componentes RAG existentes
        self.document_processor = DocumentProcessor()
        self.embedding_service = EmbeddingService()  # Manter VoyageAI como fallback
        self.vector_store = VectorStore(db_manager)
        self.cache_manager = CacheManager()  # Usa configura√ß√£o unificada
        self.retrieval_engine = RetrievalEngine(openai_api_key)
        
        # üîß CORRE√á√ÉO: Inicializar SentenceTransformerService uma √∫nica vez
        try:
            from services.sentence_transformer_service import SentenceTransformerService
            self.sentence_transformer_service = SentenceTransformerService()
            logger.info("‚úÖ SentenceTransformerService inicializado (singleton)")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è SentenceTransformerService n√£o dispon√≠vel: {e}")
            self.sentence_transformer_service = None
        
        logger.info("‚úÖ RAGService inicializado com cache inteligente")
    
    def process_or_query(self, licitacao_id: str, query: str) -> Dict[str, Any]:
        """Fun√ß√£o principal: processa documentos se necess√°rio e responde query"""
        try:
            # üÜï Validar licitacao_id antes de qualquer processamento
            if not licitacao_id or licitacao_id.lower() in ['undefined', 'null', 'none', '']:
                logger.error(f"‚ùå licitacao_id inv√°lido recebido: '{licitacao_id}'")
                return {
                    'success': False,
                    'error': 'ID da licita√ß√£o √© obrigat√≥rio e deve ser um UUID v√°lido',
                    'invalid_input': {
                        'received_id': licitacao_id,
                        'expected': 'UUID v√°lido (ex: 123e4567-e89b-12d3-a456-426614174000)'
                    }
                }
            
            # üÜï Verificar se parece com UUID (formato b√°sico)
            uuid_pattern = re.compile(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', re.IGNORECASE)
            if not uuid_pattern.match(licitacao_id):
                logger.error(f"‚ùå licitacao_id n√£o parece ser um UUID: '{licitacao_id}'")
                return {
                    'success': False,
                    'error': 'Formato de UUID inv√°lido para licitacao_id',
                    'invalid_input': {
                        'received_id': licitacao_id,
                        'expected_format': 'UUID (ex: 123e4567-e89b-12d3-a456-426614174000)'
                    }
                }
            
            logger.info(f"üöÄ Iniciando RAG para licita√ß√£o: {licitacao_id}")
            
            # 1. Verificar cache primeiro
            cached_result = self.cache_manager.get_cached_query_result(query, licitacao_id)
            if cached_result:
                logger.info("‚ö° Resultado encontrado no cache")
                return {
                    'success': True,
                    'cached': True,
                    **cached_result
                }
            
            # 2. Verificar se documentos est√£o vetorizados
            status = self.vector_store.check_vectorization_status(licitacao_id)
                         
            if not status.get('vetorizado_completo', False):
                # 3. Processar documentos se necess√°rio (inclui extra√ß√£o recursiva de ZIPs)
                logger.info("üìù Documentos n√£o vetorizados, iniciando processamento completo...")
                print("üìù Documentos n√£o vetorizados, iniciando processamento completo...")
                vectorization_result = self._vectorize_licitacao(licitacao_id)
                print(f"vectorization_result: {vectorization_result}")
                
                if not vectorization_result['success']:
                    # Incluir detalhes de diagn√≥stico no resultado
                    error_response = {
                        'success': False,
                        'error': vectorization_result.get('error'),
                        'processing_details': {
                            'action': vectorization_result.get('action', 'unknown'),
                            'suggestion': vectorization_result.get('suggestion'),
                            'licitacao_info': vectorization_result.get('licitacao_info')
                        }
                    }
                    return error_response
            
            # 4. Responder query
            response_result = self._answer_query(query, licitacao_id)
            
            # 5. Cachear resultado
            if response_result['success']:
                self.cache_manager.cache_query_result(
                    query, licitacao_id, response_result, ttl=3600
                )
                
                # Adicionar informa√ß√µes de processamento se documentos foram processados nesta sess√£o
                if not status.get('vetorizado_completo', False):
                    response_result['processing_info'] = {
                        'documents_processed_this_session': True,
                        'processing_method': 'recursive_zip_extraction',
                        'vectorization_completed': True
                    }
            
            return response_result
            
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento RAG: {e}")
            return {
                'success': False,
                'error': f'Erro interno: {str(e)}'
            }
    
    def _vectorize_licitacao(self, licitacao_id: str) -> Dict[str, Any]:
        """VERS√ÉO ATUALIZADA: Vetoriza documentos evitando reprocessamento"""
        try:
            # 1. Verificar e processar documentos automaticamente se necess√°rio
            documentos_result = self._ensure_documents_processed(licitacao_id)
            if not documentos_result['success']:
                return documentos_result
            
            # 2. Obter documentos do banco
            documentos = self.unified_processor.obter_documentos_licitacao(licitacao_id)
            
            if not documentos:
                return {
                    'success': False,
                    'error': 'Nenhum documento encontrado para vetoriza√ß√£o'
                }
            
            # 3. üÜï NOVO: Verificar quais documentos j√° foram processados
            documentos_para_processar = []
            documentos_ja_processados = 0
            
            for documento in documentos:
                documento_data = {
                    'arquivo_url': documento.get('arquivo_nuvem_url', ''),
                    'tamanho_arquivo': documento.get('tamanho_arquivo', 0),
                    'hash_arquivo': documento.get('hash_arquivo', '')
                }
                
                # Verificar se j√° tem chunks vetorizados OU se j√° foi processado antes
                existing_chunks = self.vector_store.count_document_chunks(documento['id'])
                already_processed = self.dedup_service.should_process_rag_document(documento['id'], documento_data)
                
                if existing_chunks > 0 or not already_processed:
                    documentos_ja_processados += 1
                    logger.info(f"‚è≠Ô∏è Documento j√° processado: {documento['titulo']} ({existing_chunks} chunks)")
                else:
                    documentos_para_processar.append(documento)
            
            if documentos_ja_processados > 0:
                logger.info(f"üìä Documentos: {documentos_ja_processados} j√° processados, {len(documentos_para_processar)} para processar")
            
            # Se todos j√° processados, retornar sucesso
            if not documentos_para_processar:
                total_chunks = sum(self.vector_store.count_document_chunks(doc['id']) for doc in documentos)
                return {
                    'success': True,
                    'message': 'Todos os documentos j√° estavam vetorizados',
                    'processed_documents': len(documentos),
                    'total_chunks': total_chunks,
                    'already_vectorized': True
                }
            
            # 4. üÜï NOVO: Processar apenas documentos necess√°rios
            total_chunks = 0
            processed_docs = 0
            embedding_cache_hits = 0
            
            logger.info(f"üìã Processando {len(documentos_para_processar)} documentos para vetoriza√ß√£o...")
            
            for idx, documento in enumerate(documentos_para_processar, 1):
                logger.info(f"üìÑ Processando documento {idx}/{len(documentos_para_processar)}: {documento['titulo']}")
                
                # Marcar como processando
                self._update_document_status(documento['id'], 'processando')
                
                try:
                    # Extrair texto (SEM MUDAN√áA)
                    texto_completo = self.document_processor.extract_text_from_url(
                        documento['arquivo_nuvem_url']
                    )
                    
                    if not texto_completo:
                        self._update_document_status(documento['id'], 'erro')
                        continue
                    
                    # Salvar texto extra√≠do
                    self._save_extracted_text(documento['id'], texto_completo)
                    
                    # Criar chunks inteligentes (SEM MUDAN√áA)
                    chunks = self.document_processor.create_intelligent_chunks(
                        texto_completo, documento['id']
                    )
                    
                    if not chunks:
                        self._update_document_status(documento['id'], 'erro')
                        continue
                    
                    # üîß OTIMIZA√á√ÉO: Gerar embeddings em lotes para m√°xima efici√™ncia
                    chunk_texts = [chunk.text for chunk in chunks]
                    embeddings = []
                    texts_to_process = []
                    indices_to_process = []
                    
                    # üîß OTIMIZA√á√ÉO: Verificar cache em lote (uma √∫nica query DB)
                    cached_embeddings_map = self._batch_get_embeddings_from_cache(chunk_texts)
                    
                    for i, chunk_text in enumerate(chunk_texts):
                        if chunk_text in cached_embeddings_map:
                            embeddings.append((i, cached_embeddings_map[chunk_text]))
                            embedding_cache_hits += 1
                        else:
                            texts_to_process.append(chunk_text)
                            indices_to_process.append(i)
                    
                    # üöÄ OTIMIZA√á√ÉO: Usar VoyageAI como prim√°rio (API paga) para Railway
                    if texts_to_process:
                        logger.info(f"üîÑ Gerando embeddings VoyageAI para {len(texts_to_process)} textos em lote")
                        
                        # 1. Tentar VoyageAI primeiro (API paga - n√£o consome servidor)
                        batch_embeddings = self.embedding_service.generate_embeddings(texts_to_process)
                        
                        if batch_embeddings and len(batch_embeddings) == len(texts_to_process):
                            # üîß OTIMIZA√á√ÉO: Cachear embeddings em lote (uma √∫nica transa√ß√£o DB)
                            texts_and_embeddings_to_cache = []
                            for j, (text, embedding, original_index) in enumerate(
                                zip(texts_to_process, batch_embeddings, indices_to_process)
                            ):
                                embeddings.append((original_index, embedding))
                                texts_and_embeddings_to_cache.append((text, embedding))
                            
                            # Salvar todos os embeddings em uma √∫nica transa√ß√£o
                            self._batch_save_embeddings_to_cache(texts_and_embeddings_to_cache)
                            logger.info(f"‚úÖ VoyageAI batch processing conclu√≠do")
                            
                        else:
                            logger.warning(f"‚ö†Ô∏è VoyageAI batch falhou, tentando fallback individual...")
                            # Fallback para processamento individual com APIs pagas
                            for text, original_index in zip(texts_to_process, indices_to_process):
                                new_embedding = self._generate_embedding_with_fallback(text)
                                embeddings.append((original_index, new_embedding))
                                if new_embedding:
                                    self.cache_service.save_embedding_to_cache(
                                        text, new_embedding, "voyage-ai"
                                    )
                    
                    # Ordenar embeddings pela ordem original dos chunks
                    embeddings.sort(key=lambda x: x[0])
                    final_embeddings = [emb for _, emb in embeddings]
                    
                    # Verificar se todos embeddings foram gerados
                    valid_embeddings = [emb for emb in final_embeddings if emb]
                    
                    if len(valid_embeddings) != len(chunks):
                        logger.error(f"‚ùå Embeddings incompletos para {documento['titulo']}")
                        logger.error(f"üìä Esperado: {len(chunks)} embeddings, Gerado: {len(valid_embeddings)}")
                        self._update_document_status(documento['id'], 'erro_embedding')
                        continue
                    
                    # Salvar no vector store (SEM MUDAN√áA)
                    chunk_dicts = [self._chunk_to_dict(chunk) for chunk in chunks]
                    success = self.vector_store.save_chunks_with_embeddings(
                        documento['id'], licitacao_id, chunk_dicts, valid_embeddings
                    )
                    
                    if success:
                        total_chunks += len(chunks)
                        processed_docs += 1
                        
                        # üÜï NOVO: Marcar documento como processado
                        documento_data = {
                            'arquivo_url': documento.get('arquivo_nuvem_url', ''),
                            'tamanho_arquivo': documento.get('tamanho_arquivo', 0),
                            'hash_arquivo': documento.get('hash_arquivo', '')
                        }
                        self.dedup_service.mark_rag_document_processed(documento['id'], documento_data)
                        
                        logger.info(f"‚úÖ Documento vetorizado: {documento['titulo']} ({len(chunks)} chunks)")
                    else:
                        self._update_document_status(documento['id'], 'erro')
                
                except Exception as e:
                    logger.error(f"‚ùå Erro ao vetorizar {documento['t√≠tulo']}: {e}")
                    self._update_document_status(documento['id'], 'erro')
                    continue
            
            logger.info(f"üìä Cache hits de embeddings: {embedding_cache_hits}")
            
            if processed_docs > 0 or documentos_ja_processados > 0:
                total_existing_chunks = sum(self.vector_store.count_document_chunks(doc['id']) for doc in documentos)
                return {
                    'success': True,
                    'message': 'Vetoriza√ß√£o conclu√≠da com cache',
                    'processed_documents': processed_docs,
                    'already_processed_documents': documentos_ja_processados,
                    'total_chunks': total_chunks + total_existing_chunks,
                    'embedding_cache_hits': embedding_cache_hits
                }
            else:
                return {
                    'success': False,
                    'error': 'Nenhum documento foi vetorizado com sucesso'
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erro na vetoriza√ß√£o: {e}")
            return {
                'success': False,
                'error': f'Erro na vetoriza√ß√£o: {str(e)}'
            }
    
    def _generate_embedding_with_fallback(self, text: str) -> List[float]:
        """üÜï OTIMIZADO: Gera embedding priorizando APIs pagas: VoyageAI -> OpenAI small -> SentenceTransformers (√∫ltimo recurso)"""
        
        # 1. PRIM√ÅRIO: VoyageAI (API paga - ideal para Railway)
        try:
            embedding = self.embedding_service.generate_single_embedding(text)
            if embedding:
                logger.debug("‚úÖ VoyageAI embedding gerado")
                return embedding
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è VoyageAI falhou: {e}")
        
        # 2. FALLBACK: OpenAI small (API paga - mais barato que large)
        try:
            import openai
            import os
            
            openai_api_key = os.getenv('OPENAI_API_KEY')
            if openai_api_key:
                client = openai.OpenAI(api_key=openai_api_key)
                
                response = client.embeddings.create(
                    model="text-embedding-3-small",  # Mais barato: $0.02/1M tokens
                    input=text,
                    encoding_format="float"
                )
                
                embedding = response.data[0].embedding
                if embedding:
                    logger.debug("‚úÖ OpenAI small embedding gerado")
                    return embedding
            else:
                logger.warning("‚ö†Ô∏è OPENAI_API_KEY n√£o encontrada para fallback")
                
        except ImportError:
            logger.warning("‚ö†Ô∏è Biblioteca openai n√£o instalada")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è OpenAI fallback falhou: {e}")
        
        # 3. √öLTIMO RECURSO: SentenceTransformers (local - consome servidor)
        if self.sentence_transformer_service:
            try:
                embedding = self.sentence_transformer_service.generate_single_embedding(text)
                if embedding:
                    logger.debug("‚úÖ SentenceTransformers (local) embedding gerado - √öLTIMO RECURSO")
                    logger.warning("‚ö†Ô∏è Considere configurar VoyageAI ou OpenAI para melhor performance")
                    return embedding
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è SentenceTransformers falhou: {e}")
        
        logger.error("‚ùå TODOS os m√©todos de embedding falharam")
        return []
    
    def _batch_get_embeddings_from_cache(self, texts: List[str]) -> Dict[str, List[float]]:
        """üîß OTIMIZA√á√ÉO: Busca m√∫ltiplos embeddings em uma √∫nica query DB"""
        try:
            import hashlib
            text_hash_map = {}
            hash_to_text_map = {}
            
            # Criar mapa de hash para texto
            for text in texts:
                text_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()
                text_hash_map[text] = text_hash
                hash_to_text_map[text_hash] = text
            
            # Buscar todos os embeddings em uma √∫nica query
            with self.db_manager.get_connection() as conn:
                with conn.cursor() as cursor:
                    hashes = list(hash_to_text_map.keys())
                    if not hashes:
                        return {}
                    
                    # Usar ANY para buscar m√∫ltiplos hashes
                    cursor.execute("""
                        SELECT text_hash, embedding FROM embedding_cache 
                        WHERE text_hash = ANY(%s) AND model_name = %s
                    """, (hashes, "sentence-transformers"))
                    
                    results = cursor.fetchall()
                    
                    # Construir mapa de resultado
                    cached_embeddings = {}
                    for text_hash, embedding in results:
                        if text_hash in hash_to_text_map:
                            original_text = hash_to_text_map[text_hash]
                            cached_embeddings[original_text] = embedding
                    
                    if cached_embeddings:
                        logger.info(f"‚ö° {len(cached_embeddings)} embeddings encontrados no cache em lote")
                    
                    return cached_embeddings
                    
        except Exception as e:
            logger.error(f"‚ùå Erro ao buscar embeddings em lote: {e}")
            return {}

    def _batch_save_embeddings_to_cache(self, texts_and_embeddings: List[tuple]) -> bool:
        """üîß OTIMIZA√á√ÉO: Salva m√∫ltiplos embeddings em uma √∫nica transa√ß√£o DB"""
        try:
            import hashlib
            batch_data = []
            seen_hashes = set()  # Para deduplificar
            
            for text, embedding in texts_and_embeddings:
                text_hash = hashlib.sha256(text.encode('utf-8')).hexdigest()
                
                # üîß CORRE√á√ÉO: Pular duplicatas no mesmo batch
                if text_hash in seen_hashes:
                    logger.debug(f"‚ö†Ô∏è Hash duplicado no batch, pulando: {text_hash[:8]}...")
                    continue
                
                seen_hashes.add(text_hash)
                text_preview = text[:100] + "..." if len(text) > 100 else text
                batch_data.append((text_hash, text_preview, embedding, "sentence-transformers"))
            
            if not batch_data:
                logger.warning("‚ö†Ô∏è Nenhum dado √∫nico para salvar no batch")
                return True
            
            # Salvar todos em uma √∫nica transa√ß√£o
            with self.db_manager.get_connection() as conn:
                with conn.cursor() as cursor:
                    # Usar execute_values para inser√ß√£o em lote (muito mais r√°pido)
                    from psycopg2.extras import execute_values
                    execute_values(
                        cursor,
                        """
                        INSERT INTO embedding_cache (text_hash, text_preview, embedding, model_name)
                        VALUES %s
                        ON CONFLICT (text_hash) DO UPDATE SET
                            accessed_at = NOW(),
                            access_count = embedding_cache.access_count + 1
                        """,
                        batch_data,
                        template=None,
                        page_size=100
                    )
                    conn.commit()
            
            logger.info(f"‚úÖ {len(batch_data)} embeddings √∫nicos salvos em lote (de {len(texts_and_embeddings)} originais)")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar embeddings em lote: {e}")
            # Fallback para salvamento individual
            for text, embedding in texts_and_embeddings:
                self.cache_service.save_embedding_to_cache(text, embedding, "sentence-transformers")
            return False

    def _chunk_to_dict(self, chunk) -> Dict[str, Any]:
        """Converte chunk para dicion√°rio"""
        return {
            'text': chunk.text,
            'chunk_type': chunk.chunk_type,
            'page_number': chunk.page_number,
            'section_title': chunk.section_title,
            'token_count': chunk.token_count,
            'char_count': chunk.char_count,
            'metadata': chunk.metadata or {}
        }
    
    def _answer_query(self, query: str, licitacao_id: str) -> Dict[str, Any]:
        """Responde uma query usando RAG"""
        try:
            start_time = time.time()
            
            # 1. Gerar embedding da query
            query_embedding = self.embedding_service.generate_single_embedding(query)
            
            if not query_embedding:
                return {
                    'success': False,
                    'error': 'Erro ao gerar embedding da consulta'
                }
            
            # 2. Buscar chunks relevantes (h√≠brida)
            chunks = self.vector_store.hybrid_search(
                query, query_embedding, licitacao_id, limit=12  # Buscar mais para reranking
            )
            
            if not chunks:
                return {
                    'success': False,
                    'error': 'Nenhum conte√∫do relevante encontrado nos documentos'
                }
            
            # 3. Aplicar reranking
            top_chunks = self.retrieval_engine.rerank_chunks(query, chunks, top_k=8)
            
            # 4. Obter informa√ß√µes da licita√ß√£o
            licitacao_info = self.unified_processor.extrair_info_licitacao(licitacao_id)
            
            # 5. Gerar resposta
            response_result = self.retrieval_engine.generate_response(
                query, top_chunks, licitacao_info
            )
            
            if response_result.get('error'):
                return {
                    'success': False,
                    'error': response_result['answer']
                }
            
            processing_time = time.time() - start_time
            
            # 6. Montar resultado final
            result = {
                'success': True,
                'answer': response_result['answer'],
                'query': query,
                'licitacao_id': licitacao_id,
                'chunks_used': response_result['chunks_used'],
                'sources': response_result['sources'],
                'processing_time': round(processing_time, 2),
                'model_response_time': response_result.get('response_time'),
                'cost_usd': response_result.get('cost_usd'),
                'model': response_result.get('model'),
                'cached': False
            }
            
            logger.info(f"‚úÖ Query respondida em {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao responder query: {e}")
            return {
                'success': False,
                'error': f'Erro ao processar consulta: {str(e)}'
            }
    
    def _ensure_documents_processed(self, licitacao_id: str) -> Dict[str, Any]:
        """
        üöÄ NOVA FUN√á√ÉO: Garante que documentos est√£o processados usando UnifiedDocumentProcessor recursivo
        Integra√ß√£o completa do processamento recursivo de ZIPs no fluxo RAG
        """
        try:
            logger.info(f"üîç Verificando documentos para licita√ß√£o: {licitacao_id}")
            
            # 1. Verificar se documentos j√° existem
            documentos_existem = self.unified_processor.verificar_documentos_existem(licitacao_id)
            
            if documentos_existem:
                # Verificar se documentos s√£o v√°lidos (n√£o corrompidos/vazios)
                documentos = self.unified_processor.obter_documentos_licitacao(licitacao_id)
                
                # Validar se temos documentos √∫teis
                documentos_validos = []
                for doc in documentos:
                    # Verificar se √© arquivo √∫til (PDF, DOC, etc) e n√£o est√° corrompido
                    if (doc.get('tipo_arquivo') in ['application/pdf', 'application/msword'] and 
                        doc.get('tamanho_arquivo', 0) > 1000):  # Arquivos > 1KB
                        documentos_validos.append(doc)
                
                if documentos_validos:
                    logger.info(f"‚úÖ {len(documentos_validos)} documentos v√°lidos j√° processados")
                    return {
                        'success': True,
                        'message': 'Documentos j√° processados',
                        'documentos_count': len(documentos_validos),
                        'action': 'documents_already_exist'
                    }
                else:
                    logger.warning(f"‚ö†Ô∏è Documentos existem mas s√£o inv√°lidos, reprocessando...")
                    # Limpar documentos inv√°lidos
                    self.unified_processor._limpar_documentos_licitacao(licitacao_id)
            
            # 2. Processar documentos usando nossa l√≥gica recursiva aprimorada
            logger.info("üì• Iniciando processamento recursivo de documentos...")
            
            # Primeiro, validar se a licita√ß√£o existe
            licitacao_info = self.unified_processor.extrair_info_licitacao(licitacao_id)
            if not licitacao_info:
                return {
                    'success': False,
                    'error': f'Licita√ß√£o {licitacao_id} n√£o encontrada no banco de dados',
                    'action': 'licitacao_not_found'
                }
            
            # Tentar processar documentos (m√©todo ass√≠ncrono)
            import asyncio
            result = asyncio.run(self.unified_processor.processar_documentos_licitacao(licitacao_id))
            
            if result['success']:
                logger.info(f"üéâ Processamento recursivo conclu√≠do: {result.get('documentos_processados', 0)} documentos")
                return {
                    'success': True,
                    'message': 'Documentos processados com sucesso usando extra√ß√£o recursiva',
                    'documentos_processados': result.get('documentos_processados', 0),
                    'storage_provider': result.get('storage_provider', 'supabase'),
                    'pasta_nuvem': result.get('pasta_nuvem'),
                    'action': 'documents_processed_recursive'
                }
            else:
                # Se falhou, tentar diagnosticar o problema
                error_detail = result.get('error', 'Erro desconhecido')
                logger.error(f"‚ùå Falha no processamento: {error_detail}")
                
                # Verificar se √© problema da API PNCP
                if 'API PNCP' in error_detail or 'HTTP' in error_detail:
                    return {
                        'success': False,
                        'error': f'Erro na API PNCP: {error_detail}',
                        'suggestion': 'Verifique se a licita√ß√£o possui documentos dispon√≠veis na API',
                        'action': 'api_error'
                    }
                
                # Outros erros
                return {
                    'success': False,
                    'error': f'Erro no processamento de documentos: {error_detail}',
                    'licitacao_info': {
                        'objeto': licitacao_info.get('objeto_compra', 'N/A')[:100] + '...',
                        'orgao': licitacao_info.get('orgao_entidade', 'N/A'),
                        'uf': licitacao_info.get('uf', 'N/A')
                    },
                    'action': 'processing_error'
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico ao garantir documentos processados: {e}")
            return {
                'success': False,
                'error': f'Erro cr√≠tico no processamento: {str(e)}',
                'action': 'critical_error'
            }
    
    def _update_document_status(self, documento_id: str, status: str):
        """Atualiza status de processamento do documento"""
        try:
            with self.db_manager.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        UPDATE documentos_licitacao 
                        SET status_processamento = %s, updated_at = NOW()
                        WHERE id = %s
                    """, (status, documento_id))
                    conn.commit()
        except Exception as e:
            logger.error(f"‚ùå Erro ao atualizar status: {e}")
    
    def _save_extracted_text(self, documento_id: str, texto: str):
        """Salva texto extra√≠do no banco"""
        try:
            with self.db_manager.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        UPDATE documentos_licitacao 
                        SET texto_extraido = %s, updated_at = NOW()
                        WHERE id = %s
                    """, (texto, documento_id))
                    conn.commit()
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar texto: {e}")
    
    def get_licitacao_stats(self, licitacao_id: str) -> Dict[str, Any]:
        """Retorna estat√≠sticas de uma licita√ß√£o"""
        try:
            status = self.vector_store.check_vectorization_status(licitacao_id)
            cache_stats = self.cache_manager.get_cache_stats()
            
            return {
                'licitacao_id': licitacao_id,
                'vectorization_status': status,
                'cache_stats': cache_stats,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao obter stats: {e}")
            return {'error': str(e)}
