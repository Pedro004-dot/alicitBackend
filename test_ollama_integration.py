#!/usr/bin/env python3
"""
ğŸ§ª Teste de IntegraÃ§Ã£o Ollama
Verifica se o qwen2.5:7b estÃ¡ funcionando corretamente com nosso sistema
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

import requests
import time
from config.llm_config import LLMConfig
from matching.ollama_match_validator import OllamaMatchValidator

def test_ollama_direct():
    """Teste direto do Ollama"""
    print("ğŸ” TESTE DIRETO DO OLLAMA")
    print("-" * 40)
    
    # 1. Verificar se estÃ¡ rodando
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [m['name'] for m in models]
            print(f"âœ… Ollama conectado - Modelos: {model_names}")
            
            if 'qwen2.5:7b' in model_names:
                print("âœ… qwen2.5:7b disponÃ­vel")
            else:
                print("âŒ qwen2.5:7b NÃƒO encontrado")
                return False
                
        else:
            print(f"âŒ Ollama erro: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ Erro conectando Ollama: {e}")
        return False
    
    return True

def test_ollama_validator():
    """Teste do nosso validador Ollama"""
    print("\nğŸ¦™ TESTE DO VALIDADOR OLLAMA")
    print("-" * 40)
    
    try:
        validator = OllamaMatchValidator()
        
        if not validator.ollama_available:
            print("âŒ Validador detectou Ollama indisponÃ­vel")
            return False
        
        # Teste com match Ã³bvio
        start_time = time.time()
        
        result = validator.validate_match(
            empresa_nome="TechSoft Desenvolvimento",
            empresa_descricao="Empresa especializada em desenvolvimento de software de gestÃ£o, sistemas web e aplicaÃ§Ãµes mÃ³veis",
            licitacao_objeto="Desenvolvimento de sistema de gestÃ£o administrativa para Ã³rgÃ£o pÃºblico",
            pncp_id="TEST123",
            similarity_score=0.89
        )
        
        processing_time = time.time() - start_time
        
        print(f"ğŸ“Š RESULTADO DO TESTE:")
        print(f"   ğŸ¤– Modelo: {result.get('model', 'N/A')}")
        print(f"   âœ… VÃ¡lido: {result['is_valid']}")
        print(f"   ğŸ“ˆ ConfianÃ§a: {result['confidence']:.1%}")
        print(f"   ğŸ§  Justificativa: {result['reasoning'][:100]}...")
        print(f"   âš¡ Tempo: {processing_time:.2f}s")
        
        # Verificar se resultado faz sentido
        if result['is_valid'] and result['confidence'] > 0.6:
            print("âœ… TESTE PASSOU - Match Ã³bvio foi aprovado!")
            return True
        else:
            print("âŒ TESTE FALHOU - Match Ã³bvio foi rejeitado!")
            return False
            
    except Exception as e:
        print(f"âŒ Erro no validador: {e}")
        return False

def test_llm_config():
    """Teste da configuraÃ§Ã£o LLM"""
    print("\nâš™ï¸ TESTE DA CONFIGURAÃ‡ÃƒO LLM")
    print("-" * 40)
    
    try:
        # Testar detecÃ§Ã£o de provider
        provider = LLMConfig.get_provider()
        print(f"ğŸ“‹ Provider detectado: {provider}")
        
        # Testar configuraÃ§Ãµes Ollama
        ollama_config = LLMConfig.get_ollama_config()
        print(f"ğŸ¦™ Config Ollama: {ollama_config}")
        
        # Testar disponibilidade
        is_available = LLMConfig.is_ollama_available()
        print(f"ğŸ” Ollama disponÃ­vel: {is_available}")
        
        # Testar factory
        validator = LLMConfig.create_validator()
        validator_type = type(validator).__name__
        print(f"ğŸ­ Validador criado: {validator_type}")
        
        if validator_type == "OllamaMatchValidator":
            print("âœ… TESTE PASSOU - Factory estÃ¡ usando Ollama!")
            return True
        else:
            print(f"âš ï¸ AVISO - Factory estÃ¡ usando: {validator_type}")
            return True  # Pode ser fallback
            
    except Exception as e:
        print(f"âŒ Erro na configuraÃ§Ã£o: {e}")
        return False

def test_fallback_scenario():
    """Teste do cenÃ¡rio de fallback"""
    print("\nğŸ”„ TESTE DE FALLBACK")
    print("-" * 40)
    
    # Simular Ollama indisponÃ­vel temporariamente
    original_url = os.getenv('OLLAMA_URL')
    os.environ['OLLAMA_URL'] = 'http://localhost:99999'  # URL invÃ¡lida
    
    try:
        validator = LLMConfig.create_validator()
        validator_type = type(validator).__name__
        
        print(f"ğŸ”„ Com Ollama indisponÃ­vel, usando: {validator_type}")
        
        if validator_type == "LLMMatchValidator":
            print("âœ… TESTE PASSOU - Fallback para OpenAI funcionou!")
            result = True
        else:
            print(f"âš ï¸ Fallback inesperado: {validator_type}")
            result = True
            
    except Exception as e:
        print(f"âŒ Erro no fallback: {e}")
        result = False
    finally:
        # Restaurar configuraÃ§Ã£o original
        if original_url:
            os.environ['OLLAMA_URL'] = original_url
        else:
            os.environ.pop('OLLAMA_URL', None)
    
    return result

def main():
    """Executar todos os testes"""
    print("ğŸ§ª INICIANDO TESTES DE INTEGRAÃ‡ÃƒO OLLAMA")
    print("=" * 50)
    
    tests = [
        ("ConexÃ£o Ollama Direta", test_ollama_direct),
        ("Validador Ollama", test_ollama_validator),
        ("ConfiguraÃ§Ã£o LLM", test_llm_config),
        ("CenÃ¡rio Fallback", test_fallback_scenario),
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\nğŸ” Executando: {test_name}")
        try:
            result = test_func()
            results.append((test_name, result))
            status = "âœ… PASSOU" if result else "âŒ FALHOU"
            print(f"ğŸ“Š {test_name}: {status}")
        except Exception as e:
            print(f"ğŸ’¥ ERRO em {test_name}: {e}")
            results.append((test_name, False))
    
    # RelatÃ³rio final
    print("\n" + "=" * 50)
    print("ğŸ“Š RELATÃ“RIO FINAL")
    print("=" * 50)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "âœ…" if result else "âŒ"
        print(f"{status} {test_name}")
    
    print(f"\nğŸ¯ RESUMO: {passed}/{total} testes passaram")
    
    if passed == total:
        print("ğŸ‰ TODOS OS TESTES PASSARAM! Ollama integrado com sucesso!")
    else:
        print("âš ï¸ Alguns testes falharam. Verifique a configuraÃ§Ã£o.")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 